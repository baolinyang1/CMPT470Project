PR ID,Title,Description,State,Created At,Closed At,Merged At,Labels,Base Branch,Head Branch,Files Changed,Lines Added,Lines Deleted,Commits,Comments,Review Comments,Reviewers,Review States,Draft Status,Linked Issues
146764,Fix standalone runner for CUTLASS auto-tuning backend,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #146764
* #146755



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov",open,2025-02-08T17:23:45Z,,,"open source, topic: not user facing, module: inductor, ciflow/inductor",gh/alexsamardzic/26/base,gh/alexsamardzic/26/head,1,64,8,2,2,0,,,False,https://api.github.com/repos/pytorch/pytorch/issues/146764
146763,[Break XPU] Align meta calculation for fft_r2c with _fft_r2c_mkl,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #146763
* #145248
* #146762

Fix #146761",open,2025-02-08T16:20:39Z,,,"open source, ciflow/trunk, topic: not user facing, ciflow/inductor, ciflow/xpu",gh/etaf/98/base,gh/etaf/98/head,1,3,1,1,1,0,,,False,https://api.github.com/repos/pytorch/pytorch/issues/146763
146762,[Break XPU][Inductor UT] Fix XPU Inductor UT introduced from community.,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #146763
* #145248
* __->__ #146762



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov",open,2025-02-08T16:20:35Z,,,"open source, topic: not user facing, module: inductor, ciflow/inductor",gh/etaf/97/base,gh/etaf/97/head,4,10,2,1,1,0,,,False,https://api.github.com/repos/pytorch/pytorch/issues/146762
146756,[Inductor][CPU] Add GEMM tamplates for _weight_int4pack_mm_for_cpu with AVX512,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #146756
* #145250
* #145245

**Summary**
It's part of the task to enable max-autotune with GEMM template for WoQ INT4 GEMM on CPU. Early work: #133846

This PR adds GEMM templates for `torch.ops.aten_weight_int4pack_mm_for_cpu`. The micro kernel used for the templates is based on AVX512 and it's a copy of the ATen implementation of `torch.ops.aten_weight_int4pack_mm_for_cpu` with minor changes.

Due to better blocking and loop schedule, the GEMM template based implementation outperforms the ATen implementation in all cases we tested.

**Test plan**
```
python test/inductor/test_cpu_select_algorithm.py -k test_int4_woq_mm_avx512
```

cc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @voznesenskym @penguinwu @EikanWang @Guobing-Chen @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov",open,2025-02-08T13:51:09Z,,,"open source, ciflow/trunk, topic: not user facing, intel, module: inductor, ciflow/inductor",gh/Xia-Weiwen/30/base,gh/Xia-Weiwen/30/head,8,468,21,2,1,0,,,True,https://api.github.com/repos/pytorch/pytorch/issues/146756
146755,Fix CUTLASS 2.x kernels for auto-tuning,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #146764
* __->__ #146755



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov",open,2025-02-08T12:01:14Z,,,"open source, topic: not user facing, module: inductor, ciflow/inductor",gh/alexsamardzic/25/base,gh/alexsamardzic/25/head,3,12,30,1,1,0,,,False,https://api.github.com/repos/pytorch/pytorch/issues/146755
146754,[MPS] fix inverse bug for N>1024,"Fixes #138200 
",open,2025-02-08T10:24:29Z,,,"open source, release notes: mps",main,mps-inverse-bugfix,4,26,27,1,1,0,,,False,https://api.github.com/repos/pytorch/pytorch/issues/146754
146753,[MPS] fix lu factor for large tensors with bs>1,"Try this:
```
import torch

batch_size = 2
A = torch.eye(256, device=""mps"")[None, :, :].expand(batch_size, -1, -1) + 0.1 * torch.randn((batch_size, 256, 256), device=""mps"")
A_cpu = A.cpu()
LU_cpu, pivots_cpu = torch.linalg.lu_factor(A_cpu)
LU, pivots = torch.linalg.lu_factor(A)
torch.testing.assert_close(LU.cpu(), LU_cpu)
```
You'll get huge difference in LU tensors
<img width=""706"" alt=""Screenshot 2025-02-08 at 12 14 39"" src=""https://github.com/user-attachments/assets/b45f2b3c-e0a5-49c8-aa07-42792150b781"" />
",open,2025-02-08T08:15:29Z,,,"open source, release notes: mps",main,lu-factor-fix-batches,2,7,5,1,1,0,Isalia20,COMMENTED,False,https://api.github.com/repos/pytorch/pytorch/issues/146753
146752,realize stride symbols in estimate_runtime,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #146752

Unfortuanlty could not create a local repo, or unit test.
fix https://github.com/pytorch/pytorch/issues/146686

",open,2025-02-08T07:30:03Z,,,"topic: not user facing, ciflow/inductor",gh/laithsakka/107/base,gh/laithsakka/107/head,1,2,1,1,1,0,,,False,https://api.github.com/repos/pytorch/pytorch/issues/146752
146751,[MTIA] (4/n) Implement PyTorch APIs to query/reset device peak memory usage,"Summary: Public summary (shared with Github): This diff updates the unit test for the PyTorch API ""reset_peak_memory_stats"".

Test Plan:
```
buck2 test //mtia/host_runtime/torch_mtia/tests:test_torch_mtia_api -- -r test_reset_peak_memory_stats
```

https://www.internalfb.com/intern/testinfra/testrun/9007199321947161

Reviewed By: yuhc

Differential Revision: D68989900


",open,2025-02-08T07:16:36Z,,,fb-exported,main,export-D68989900,2,17,0,1,3,0,,,False,https://api.github.com/repos/pytorch/pytorch/issues/146751
146750,Update instructions about faster linker,"This PR adds instructions to specify linker via cmake env `CMAKE_LINKER_TYPE` and also adds `mold` as a linker alternative.

Since 3.29, cmake introduced [`CMAKE_LINKER_TYPE`](https://cmake.org/cmake/help/latest/variable/CMAKE_LINKER_TYPE.html) that can specify linker without overwriting `ld` file or changing build script.

`mold` is already stable and **the fastest** (afaict) linker out there, and also easier to install compared with `lld`. So I added it here. After switching to `mold`, the time of linking `libtorch_cuda.so` has been reduced from ~7s to ~0.6s locally.

Also note `gold` has been marked deprecated recently[1].

[1] https://lwn.net/Articles/1007541/",open,2025-02-08T06:30:17Z,,,"open source, topic: not user facing",main,update-linker,1,6,8,2,1,0,,,False,https://api.github.com/repos/pytorch/pytorch/issues/146750
146748,Update strided test to float32,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #146748

Fixes #146377

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov",open,2025-02-08T04:48:51Z,,,"ciflow/trunk, topic: not user facing, module: inductor, ciflow/inductor",gh/drisspg/122/base,gh/drisspg/122/head,1,3,3,1,6,1,"BoyuanFeng, leijurv","APPROVED, APPROVED",False,https://api.github.com/repos/pytorch/pytorch/issues/146748
146747,Add hint message for `pack_padded_sequence`,"Fixes #144207

Add truncate hint message in docs [torch.nn.utils.rnn.pack_padded_sequence](https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pack_padded_sequence.html)

## Test Result

![image](https://github.com/user-attachments/assets/46258f36-f6c7-4f11-9213-8513e52a9001)

",open,2025-02-08T04:28:15Z,,,"open source, topic: not user facing",main,opt/nn/rnn,1,5,1,1,2,0,,,False,https://api.github.com/repos/pytorch/pytorch/issues/146747
146746,[Inductor] Fix the lowering of squeeze when input is not contiguous,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #146746

**Summary**
Fix issue https://github.com/pytorch/pytorch/issues/143498. The issue happens when we lowering `select = torch.ops.aten.select.int(cat, 1, 0)`. 

For example, when `cat` is contiguous with size[2, 2] stride[2,1]

- for eager, it returns a view of size[2,] stride[2,]
- for Inductor lowering, it returns wrong stride 1 instead of 2
```
TensorBox(
  ReinterpretView(
    StorageBox(
      ConcatKernel(name='buf10', layout=FixedLayout('cpu', torch.int64, size=[u0, 2], stride=[2, 1]), inputs=[ComputedBuffer(name='buf8', layout=NonOwningLayout('cpu', torch.int64, size=[u0, 1], stride=[2, 1]), data=Pointwise(device=device(type='cpu'), dtype=torch.int64, inner_fn=<function ReinterpretView.make_loader.<locals>.loader at 0x7f6b856449d0>, ranges=[u0, 1])), ComputedBuffer(name='buf9', layout=NonOwningLayout('cpu', torch.int64, size=[u0, 1], stride=[2, 1]), data=Pointwise(device=device(type='cpu'), dtype=torch.int64, inner_fn=<function ReinterpretView.make_loader.<locals>.loader at 0x7f6b85644790>, ranges=[u0, 1]))])
    ),
    FixedLayout('cpu', torch.int64, size=[u0], stride=[**1**]),
    origins=OrderedSet([select])
  )
)
```

To fix this issue, we give the right stride when lowering of `squeeze`.

**Test Plan**
```
python -u -m pytest -s -v test/inductor/test_unbacked_symints.py -k test_issue_143498
```

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov",open,2025-02-08T03:50:25Z,,,"open source, ciflow/trunk, topic: not user facing, module: inductor, ciflow/inductor",gh/leslie-fang-intel/180/base,gh/leslie-fang-intel/180/head,2,16,4,2,1,0,,,True,https://api.github.com/repos/pytorch/pytorch/issues/146746
146743,[cutlass backend] refactor tests to remove duplicate logic,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #146743
* #146356



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov",open,2025-02-08T01:36:44Z,,,"topic: not user facing, module: inductor, ciflow/inductor",gh/henrylhtsang/3/base,gh/henrylhtsang/3/head,1,57,81,1,1,0,,,False,https://api.github.com/repos/pytorch/pytorch/issues/146743
146742,[Dynamo][autograd.Function] Relax backward speculation strict mode: support .requires_grad,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #146742
* #146741
* #146571



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",open,2025-02-08T01:30:53Z,,,"topic: not user facing, module: dynamo, ciflow/inductor",gh/yanboliang/65/base,gh/yanboliang/65/head,2,27,1,1,1,0,,,False,https://api.github.com/repos/pytorch/pytorch/issues/146742
146741,[Dynamo][autograd.Function] Relax backward speculation strict mode: support .data,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #146742
* __->__ #146741
* #146571



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",open,2025-02-08T01:30:49Z,,,"topic: not user facing, module: dynamo, ciflow/inductor",gh/yanboliang/64/base,gh/yanboliang/64/head,2,29,1,1,1,0,,,False,https://api.github.com/repos/pytorch/pytorch/issues/146741
146739,Testing,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #146739
* #145748

This reverts commit 5cd5b4d2d54c0220b92ee488dd36d789c9b60af3.",open,2025-02-08T00:30:57Z,,,"release notes: releng, ciflow/binaries_wheel",gh/mikaylagawarecki/312/base,gh/mikaylagawarecki/312/head,8,25,80,2,1,0,,,True,https://api.github.com/repos/pytorch/pytorch/issues/146739
146738,[audio hash update] update the pinned audio hash,"This PR is auto-generated nightly by [this action](https://github.com/pytorch/pytorch/blob/main/.github/workflows/nightly.yml).
Update the pinned audio hash.",open,2025-02-08T00:23:09Z,,,"open source, ciflow/trunk, topic: not user facing, ciflow/inductor, merging",main,update-audio-commit-hash/13210264744-1454-1,1,1,1,1,4,0,pytorchbot,APPROVED,False,https://api.github.com/repos/pytorch/pytorch/issues/146738
146737,[dynamo][user-defined] Unify standard and non-standard __new__ codebase,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #146678
* __->__ #146737
* #146677



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",open,2025-02-08T00:21:25Z,,,"ciflow/trunk, topic: not user facing, module: dynamo, ciflow/inductor, keep-going",gh/anijain2305/676/base,gh/anijain2305/676/head,1,17,28,2,1,0,,,False,https://api.github.com/repos/pytorch/pytorch/issues/146737
146736,Document dynamo,"Many files in dynamo are currently lacking file/module-level documentation, which makes it hard to know what they do at a glance and without digging into the code. This fixes that.

Note: documentation was AI-generated and could be incorrect, please review carefully.

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @xmfan @svekars @brycebortree @sekyondaMeta @AlannaBurke",open,2025-02-08T00:15:41Z,,,"better-engineering, ciflow/trunk, topic: not user facing, module: inductor, module: dynamo, ciflow/inductor, module: compiled autograd",main,gh/raymo/document-dynamo,30,601,45,1,4,0,,,False,https://api.github.com/repos/pytorch/pytorch/issues/146736
146735,[ca] log graph before reodering passes,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #146735
* #146720



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @yf225",open,2025-02-07T23:39:50Z,,,"module: dynamo, ciflow/inductor, module: compiled autograd",gh/xmfan/177/base,gh/xmfan/177/head,1,12,0,1,2,0,,,False,https://api.github.com/repos/pytorch/pytorch/issues/146735
146734,[CUDA][CUDNN][SDPA] Pass dropout seed and offset to cuDNN in `int64`,"Workaround for limitation in cuDNN that does not accept dropout seed/offset in `int32` for SM 10.0 kernels.

cc @csarofeen @ptrblck @xwang233 @msaroufim",open,2025-02-07T23:30:33Z,,,"module: cudnn, module: cuda, triaged, open source, topic: not user facing, module: sdpa",main,cudnnsm100dropoutworkaround,1,31,12,1,1,0,,,False,https://api.github.com/repos/pytorch/pytorch/issues/146734
146733,[CUDA][SDPA] Don't dispatch to mem eff attn for batch_size >= 65536,"#146704

cc @ptrblck @msaroufim",open,2025-02-07T23:29:27Z,,,"module: cuda, open source, topic: not user facing, module: sdpa",main,memeff65536,2,23,0,1,2,0,"drisspg, drisspg, nikitaved","COMMENTED, APPROVED, COMMENTED",False,https://api.github.com/repos/pytorch/pytorch/issues/146733
146731,dont specialize symints when testing truthiness,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #133044
* #146729
* #146642
* __->__ #146731



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",open,2025-02-07T22:48:49Z,,,"module: dynamo, ciflow/inductor",gh/bdhirsh/641/base,gh/bdhirsh/641/head,2,20,1,1,3,1,bobrenjc93,APPROVED,False,https://api.github.com/repos/pytorch/pytorch/issues/146731
146730,"[BaseHOP] change hop(subgraph, operands) to hop(subgraph, *operands)","Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #146730
* #146727

Our three main users are OK with this, with two of them (foreach_map,
invoke_quant) prefering it like this.

I was originally worried about BC issues (this now means you cannot add
any positional args) but I think that's not a concern -- one can always
add kwonly args.

Test Plan
- tests

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov",open,2025-02-07T22:42:43Z,,,"release notes: foreach_frontend, module: inductor, module: dynamo, ciflow/inductor",gh/zou3519/1130/base,gh/zou3519/1130/head,6,35,48,1,1,0,,,False,https://api.github.com/repos/pytorch/pytorch/issues/146730
146729,support meta_tensor.to(device='cpu') under fake_mode,"Fixing this is actually a bit annoying:

(1) FakeTensorMode sees a function where all of its inputs are real tensors, so it tries to run the real compute before converting the output to a FakeTensor

(2) we don't actually want this, because the ""real compute"" is support to error normally, when you do `meta_tensor.to(device='cpu')`. Instead, we want FakeTensor to actually skip constant prop and run the normal FakeTensor implementation, which will not error

Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #133044
* __->__ #146729
* #146642
* #146731

",open,2025-02-07T22:19:32Z,,,ciflow/inductor,gh/bdhirsh/640/base,gh/bdhirsh/640/head,2,12,0,2,2,0,"zou3519, zou3519, bdhirsh","COMMENTED, APPROVED, COMMENTED",False,https://api.github.com/repos/pytorch/pytorch/issues/146729
146728,[StaticRuntime] Fix a bug that memory planner ignores subblocks,"Summary: When Static Runtime graph node has sub-blocks, the memory planner does not consider sub-blocks' inputs as a node's input in memory planner. As the result, such nodes' inputs' lifetime is incorrect and corresponding tensor memory is released earlier than required and causes errors.

Differential Revision: D69195886




cc @EikanWang @jgong5 @wenzhe-nrv @sanchitintel",open,2025-02-07T22:13:28Z,,,"oncall: jit, fb-exported, release notes: jit",main,export-D69195886,3,75,8,1,7,0,,,False,https://api.github.com/repos/pytorch/pytorch/issues/146728
146727,Rename PrimHOPBase to BaseHOP + minor changes,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #146730
* __->__ #146727

This PR:
- renames PrimHOPBase to BaseHOP
- changes the backward pass to always return a tuple (to match the
  forward pass).

Test Plan:
- tests

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",open,2025-02-07T22:00:22Z,,,"release notes: foreach_frontend, module: dynamo, ciflow/inductor",gh/zou3519/1129/base,gh/zou3519/1129/head,6,18,18,1,1,0,,,False,https://api.github.com/repos/pytorch/pytorch/issues/146727
146726,[ez][BE] get rid of the extra printf('\n'),"Summary: as title

Test Plan:
```
AOT_INDUCTOR_DEBUG_INTERMEDIATE_VALUE_PRINTER=3  TORCHINDUCTOR_FORCE_DISABLE_CACHES=1 TORCHINDUCTOR_ABI_COMPATIBLE=1 TORCH_COMPILE_DEBUG=1 TORCH_LOGS=""+graph, inductor, +schedule, output_code"" buck2 run -c fbcode.enable_gpu_sections=true -c fbcode.nvcc_arch=h100a @//mode/opt fbcode//caffe2/test/inductor:test_aot_inductor -- -r test_addmm_cuda
```

Differential Revision: D69328701




cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov",open,2025-02-07T21:57:49Z,,,"fb-exported, ciflow/trunk, topic: not user facing, module: inductor, ciflow/inductor, merging",main,export-D69328701,1,1,2,1,5,0,ColinPeppler,APPROVED,False,https://api.github.com/repos/pytorch/pytorch/issues/146726
146723,torch: Log a unified waitcounter for torch.compile and triton.autotune,"Summary: Add a second more generic waitcounter to torch.compile. We'll keep expanding this as new generic pytorch compilation sites show up.

Test Plan: Waitcounter only change, relying on existing tests.

Differential Revision: D69215401




cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov",open,2025-02-07T20:59:32Z,,,"fb-exported, ciflow/trunk, topic: not user facing, module: inductor, ciflow/inductor",main,export-D69215401,1,3,1,1,3,0,davidberard98,APPROVED,False,https://api.github.com/repos/pytorch/pytorch/issues/146723
146722,Test on in-graph constructed NJTs,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #146722
* #146721

A recent set of bugs has been cropping up related to NJTs that constructed in-graph within a compiled function. This exercises different paths related to symbolic nested ints, etc. Some examples:
* #145874
* #146644

To get ahead of these, we should do NJT testing for this case as well.

This PR parametrizes the OpInfo tests for compile + forward to cover both in-graph constructed NJT and normal input cases. TBD what fails..

TODO:
* Do this for compile + backward tests also (?)",open,2025-02-07T20:09:15Z,,,topic: not user facing,gh/jbschlosser/229/base,gh/jbschlosser/229/head,2,44,6,2,1,0,"jbschlosser, cpuhrsch, soulitzer","COMMENTED, APPROVED, APPROVED",True,https://api.github.com/repos/pytorch/pytorch/issues/146722
146721,Use inductor backend for NJT compile tests,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #146722
* __->__ #146721

We've been using `backend=""aot_eager_decomp_partition""` for NJT compile testing, but this can let inductor bugs slip through. This PR switches the compile tests to use `backend=""inductor""`; let's see if test runtime is an issue after this.",open,2025-02-07T20:09:11Z,,,topic: not user facing,gh/jbschlosser/228/base,gh/jbschlosser/228/head,1,3,7,2,1,0,soulitzer,APPROVED,False,https://api.github.com/repos/pytorch/pytorch/issues/146721
146720,[ca] remove private API: _compiled_autograd_should_lift,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #146735
* __->__ #146720

Since the functional autograd + compiled autograd migration, we don't trace into nodes anymore, and everything is lifted. We can't support this flag which tries to inline make_fx style in CA initial pass. There's no more usage internally.",open,2025-02-07T20:06:08Z,,,"ciflow/trunk, ciflow/inductor, release notes: dynamo",gh/xmfan/176/base,gh/xmfan/176/head,4,0,15,1,7,0,zou3519,APPROVED,False,https://api.github.com/repos/pytorch/pytorch/issues/146720
146718,[CUDAGraph] add skip message for unbacked symint,"Add explicit skip message for unbacked symint in cudagraph, as suggested by @bdhirsh.

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov",open,2025-02-07T19:29:37Z,,,"ciflow/trunk, topic: not user facing, module: inductor, module: dynamo, ciflow/inductor",main,bf/cg-skip-unbacked-symint-msg,4,31,14,1,1,0,,,False,https://api.github.com/repos/pytorch/pytorch/issues/146718
146717,[BE][cuDNN] cuDNN to 9.7.1.26 for CUDA 12.8,"cuDNN 9.7.1 is out now and is expected to be the longer-lived branch with more potential backports vs. 9.7.0

CC @nWEIdia @tinglvv 

cc @malfet @seemethere @csarofeen @ptrblck @xwang233",open,2025-02-07T19:25:00Z,,,"module: build, module: cudnn, triaged, open source, topic: not user facing, topic: build",main,cudnn971,6,14,12,1,1,0,drisspg,APPROVED,False,https://api.github.com/repos/pytorch/pytorch/issues/146717
146716,[BE] Remove outdated RPC benchmark,"We have lots of outdated unused + uncalled code in our codebase, namely in our benchmarks and examples folders among others. The last change to this directory was 4 years ago and this code looks dead. cc @albanD @H-Huang for feedback

Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #146716

",open,2025-02-07T18:52:53Z,,,"release notes: distributed (rpc), skip-pr-sanity-checks",gh/janeyx99/223/base,gh/janeyx99/223/head,29,0,2535,1,1,0,"Skylion007, H-Huang","APPROVED, APPROVED",False,https://api.github.com/repos/pytorch/pytorch/issues/146716
146715,[export][ez] Allow math.trunc for serialization.,"Summary: as title.

Test Plan: CI

Differential Revision: D69317084


",open,2025-02-07T18:24:55Z,,,"fb-exported, ciflow/trunk, ciflow/inductor, release notes: export",main,export-D69317084,1,1,0,1,2,0,angelayi,APPROVED,False,https://api.github.com/repos/pytorch/pytorch/issues/146715
146714,[hop] Support more output types for `flat_apply`,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #146367
* __->__ #146714
* #146713

This patch enables `flat_apply` to support certain non-Tensor output
types like containers and graphable types. This will in turn enable the
upcoming `mark_traceable` to support more output types.

The patch also exposes a `func_to_graphable` rather than having the
users calling the lower level `pytree.flatten(ConstantFunction(...))`.

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",open,2025-02-07T17:43:34Z,,,"module: dynamo, ciflow/inductor",gh/StrongerXi/83/base,gh/StrongerXi/83/head,2,54,21,1,2,0,zou3519,APPROVED,False,https://api.github.com/repos/pytorch/pytorch/issues/146714
146713,[dynamo][fx] Support dataclass whose fields have `init=False`,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* (to be filled)

Previously Dynamo and FX have code paths that reconstruct a dataclass
instance based on its type and fields; however they weren't taking
`init=False` into account (which is supposed to exclude the field from
constructor).

This patch fixes that, and also updates `pytree.LeafSpec` so that its
`__init__` conforms with the `init` attribute of its fields. Without
this change, the aforementioned reconstruction logic would fail.

cc @ezyang @SherlockNoMad @EikanWang @jgong5 @wenzhe-nrv @voznesenskym @penguinwu @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @jiayisunx @chenyang78 @kadeng @chauhang @amjames",open,2025-02-07T17:43:29Z,,,"release notes: fx, fx, module: dynamo, ciflow/inductor",gh/StrongerXi/82/base,gh/StrongerXi/82/head,4,61,9,1,1,0,Skylion007,COMMENTED,False,https://api.github.com/repos/pytorch/pytorch/issues/146713
146710,[MTIA] (3/n) Implement PyTorch APIs to query/reset device peak memory usage,"Summary: Public summary (shared with Github): This diff implements a C++-Python binding to enable `reset_peak_memory_stats`.

Test Plan: The test is implemented in the following diff.

Reviewed By: yuhc

Differential Revision: D68988673


",open,2025-02-07T16:52:00Z,,,"fb-exported, ciflow/trunk, topic: not user facing, merging",main,export-D68988673,3,9,1,1,6,0,nautsimon,APPROVED,False,https://api.github.com/repos/pytorch/pytorch/issues/146710
146709,FSDP: avoid resetting version counter of all_gather_output in inference_mode,"Summary:
FSDP needs to hide VC bumps on its allgather buffer, but it does not need to do this is the allgather buffer was generated under inference mode.

more details here: https://www.internalfb.com/diff/D69115649?dst_version_fbid=1316814572779281&transaction_fbid=849120230625711

Test Plan: CI

Differential Revision: D69311496




cc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o",open,2025-02-07T16:39:54Z,,,"oncall: distributed, fb-exported, ciflow/trunk, release notes: distributed (fsdp), ciflow/inductor, merging",main,export-D69311496,1,9,1,1,6,0,awgu,APPROVED,False,https://api.github.com/repos/pytorch/pytorch/issues/146709
146706,cpp_wrapper: persist autotune example tensors until last use,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #146452
* __->__ #146706
* #146424
* #146109
* #146449
* #144349
* #144293
* #144002

Patches over an issue where randomly generated example tensors can cause kernel autotuning to fail, when those tensors would not be possible outputs from previous kernels in the sequence. This fixes a failure in `test_torchinductor_opinfo.py` when run with compile-time autotuning, `test_comprehensive_nanquantile_cuda_float64`.

For clarity, the situation triggering this PR looks like kernels `A -> BCDE -> F` (`BCDE` is fused), where one of the outputs from `A` is a boolean tensor describing some of the input data. Previously, we randomly regenerated that boolean tensor and the input data before passing them to `BCDE`, so that they no longer matched. This caused a `tl.device_assert` call in `BCDE` to fail. With this PR, we reuse the random data input to `A` and the output Boolean tensor, such that they match and pass the device assertion in `BCDE`.

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @desertfire @chauhang @aakhundov",open,2025-02-07T16:08:21Z,,,"open source, topic: not user facing, module: inductor, ciflow/inductor",gh/benjaminglass1/67/base,gh/benjaminglass1/67/head,1,28,8,2,2,0,benjaminglass1,COMMENTED,False,https://api.github.com/repos/pytorch/pytorch/issues/146706
146705,Remove NO_MULTIPROCESSING_SPAWN checks,"py 3.9 has spawn.

cc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o",open,2025-02-07T15:19:32Z,,,"oncall: distributed, triaged, open source, release notes: distributed (torchelastic)",main,NO_MULTIPROCESSING_SPAWN,11,23,156,1,1,0,,,False,https://api.github.com/repos/pytorch/pytorch/issues/146705
146696,Add full_like.default to list of ops with kwargs,"The _maybe_insert_input_observers_for_node function expects ops, except a few exceptions, to have zero kwargs. full_like.default seems to be one of these cases and should therefore be added to the list.

Addresses https://github.com/pytorch/pytorch/issues/146621

Fixes #146621 
",open,2025-02-07T11:34:31Z,,,"triaged, open source, release notes: quantization, release notes: AO frontend",main,main,2,3,2,1,2,1,Xia-Weiwen,COMMENTED,False,https://api.github.com/repos/pytorch/pytorch/issues/146696
146695,Enable Windows tests,"Fixes #ISSUE_NUMBER


cc @peterjc123 @mszhanyi @skyline75489 @nbcsm @iremyux @Blackhex",open,2025-02-07T11:34:26Z,,,"module: windows, triaged, open source, topic: not user facing",main,win_test294,3,1,23,2,1,0,,,False,https://api.github.com/repos/pytorch/pytorch/issues/146695
146692,[clang-tidy] Add suppression clang-diagnostic-shadow,"Summary:
Reviewed By: varun2784

Differential Revision: D69182465


",open,2025-02-07T10:06:04Z,,,"fb-exported, topic: not user facing",main,export-D69182465,1,2,1,1,8,0,,,False,https://api.github.com/repos/pytorch/pytorch/issues/146692
146690,Enable pt2e quantization path for arm,"**Title**: Enable PyTorch 2 Export Quantization path for ARM CPUs.

**Description:**
 - This PR extends the PyTorch 2 Export Quantization (PT2E Quantization) workflow—originally available only on x86 CPUs—to support ARM platforms. PT2E Quantization is an automated, full-graph quantization solution in PyTorch that improves on Eager Mode Quantization by adding support for functionals and automating the overall process. It is part of the torch.ao module and fully supports quantization when using the compile mode.

**Key Changes:**

 - Introduces ARM-specific support by leveraging oneDNN kernels for matmuls and convolution.

 - Integrates pre-defined configuration selection to automatically choose the best quantization settings based on the selected quantization method.

**Provides customization options via two flags:**

 - **qat_state:** Indicates whether to use Quantization Aware Training (if set to True) or Post Training Quantization (if set to False). The default remains False.
 - **dynamic_state:** Selects between dynamic quantization (if True) and static quantization (if False). The default is also set to False.
![Screenshot 2025-01-22 105543](https://github.com/user-attachments/assets/c611a1ce-9274-4b70-9c58-cae96000d06d)

These options allow users to tailor the quantization process for their specific workload requirements (e.g., using QAT for fine-tuning or PTQ for calibration-based quantization).

Testing and Validation:

The new ARM flow has been thoroughly tested across a range of models with all combinations:
**NLP**: Models such as BERT and T5.
**Vision**: Models like ResNet and ViT.
**Custom Models**: user defined models with various operators.

example script:
```
import torch
import torchvision.models as models
from torch.ao.quantization.quantize_pt2e import prepare_pt2e, convert_pt2e
import torch.ao.quantization.quantizer.arm_inductor_quantizer as armiq
from torch.ao.quantization.quantizer.arm_inductor_quantizer import ArmInductorQuantizer
from torch.profiler import profile, record_function, ProfilerActivity

model_name = ""resnet50""
model = models.__dict__[model_name](pretrained=True)

# Set the model to eval mode
model = model.eval()

# Create the data, using the dummy data here as an example
traced_bs = 500
x = torch.randn(traced_bs, 3, 224, 224).contiguous(memory_format=torch.channels_last)
example_inputs = (x,)

with torch.no_grad():
    exported_model = torch.export.export_for_training(model, example_inputs).module()
    quantizer = armiq.ArmInductorQuantizer()
    quantizer.set_global(armiq.get_default_arm_inductor_quantization_config(is_dynamic=False))
    prepared_model = prepare_pt2e(exported_model, quantizer)
    converted_model = convert_pt2e(prepared_model)

    with torch.set_grad_enabled(False):
        for _ in range(50):
            converted_model(*example_inputs) #Warmup
        print(""Warmup over"")
        with profile(activities=[ProfilerActivity.CPU], record_shapes=True) as prof:
            with record_function(""model_inference""):
                for _ in range(100):
                    converted_model(*example_inputs)

    print(prof.key_averages(group_by_input_shape=True).table(sort_by=""self_cpu_time_total""))

```

cc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @malfet @snadampal @milpuz01",open,2025-02-07T09:23:48Z,,,"module: cpu, triaged, open source, module: arm, release notes: quantization, release notes: AO frontend",main,devang/pt2e_quantization_arm,2,1592,0,1,7,1,jerryzh168,APPROVED,False,https://api.github.com/repos/pytorch/pytorch/issues/146690
146689,"Update addbmm, addmm, addmv and baddbmm description","Fixes #146611, following #146482

## Test Result

![image](https://github.com/user-attachments/assets/5c1749be-1f10-4e80-a284-b1929ca340eb)
",open,2025-02-07T08:57:51Z,,,"triaged, open source, release notes: python_frontend",main,opt/docs/add,1,4,4,1,7,0,mikaylagawarecki,APPROVED,False,https://api.github.com/repos/pytorch/pytorch/issues/146689
146687,Make GetCPUAllocatorMaybePinned to be Device-Agnostic,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #146687

----

- Keep cuda first to perserve BC
- Remove cuda first if it is possible to have only one accelerator at a time in the future",open,2025-02-07T08:54:31Z,,,"open source, topic: not user facing",gh/fffrog/38/base,gh/fffrog/38/head,2,18,15,1,2,0,,,False,https://api.github.com/repos/pytorch/pytorch/issues/146687
146684,Optimize LRScheduler docs,"Fixes #120735

Add more description about [`LRScheduler`](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.LRScheduler.html#torch.optim.lr_scheduler.LRScheduler)

## Test Result

### Before

![image](https://github.com/user-attachments/assets/619c3ea8-5652-4e61-936f-0cb5aa5a326b)

### After

![image](https://github.com/user-attachments/assets/174a6ffc-5da2-4837-bf49-2f09f6c7b6ee)

![image](https://github.com/user-attachments/assets/ae1bc984-49cc-4d5b-8d81-08f460b71361)

cc @janeyx99
",open,2025-02-07T08:42:57Z,,,"triaged, open source, release notes: optim",main,opt/docs/LRScheduler,1,21,2,1,1,0,,,False,https://api.github.com/repos/pytorch/pytorch/issues/146684
146678,[dynamo][not ready] polyfill infra for classes,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #146678
* #146737
* #146677



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",open,2025-02-07T06:41:35Z,,,"module: dynamo, ciflow/inductor",gh/anijain2305/675/base,gh/anijain2305/675/head,7,247,41,3,2,0,XuehaiPan,COMMENTED,False,https://api.github.com/repos/pytorch/pytorch/issues/146678
146677,[dynamo][user-defined] User class.__new__ instead of special casing,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #146678
* #146737
* __->__ #146677



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",open,2025-02-07T06:41:32Z,,,"ciflow/trunk, topic: not user facing, module: dynamo, ciflow/inductor, keep-going",gh/anijain2305/674/base,gh/anijain2305/674/head,6,168,104,3,1,0,,,False,https://api.github.com/repos/pytorch/pytorch/issues/146677
146675,[ROCm] Move ROCm unstable MI300 jobs back to stable,"Fixes #145790

This PR moves rocm unstable MI300 back to stable. The change to unstable was introduced through this [PR](https://github.com/pytorch/pytorch/pull/145790). This was because the MI300s were failing with a [docker daemon](https://github.com/pytorch/pytorch/actions/runs/13015957622/job/36306779536) issue which has been resolved.

cc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd @ZainRizvi 

cc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd",open,2025-02-07T05:22:51Z,,,"module: rocm, open source, topic: not user facing, ciflow/inductor, ciflow/rocm, ciflow/inductor-rocm",main,patch-8,3,80,70,8,1,0,"jithunnair-amd, jithunnair-amd","COMMENTED, COMMENTED",True,https://api.github.com/repos/pytorch/pytorch/issues/146675
146671,Update torch-xpu-ops commit pin,"Update the torch-xpu-ops commit to [662837e722cbbc0701fbcf6ea9ad6383158cc44e](https://github.com/intel/torch-xpu-ops/commit/662837e722cbbc0701fbcf6ea9ad6383158cc44e), includes:

- Aten operator coverage improvement
- SYCL kernel optimization
- Nested Tensor OPs support
",open,2025-02-07T03:28:45Z,,,"triaged, open source, topic: not user facing, keep-going, ciflow/xpu",main,xyt/xpu_pin_662837e722cbbc0701fbcf6ea9ad6383158cc44e,1,1,1,1,2,0,,,False,https://api.github.com/repos/pytorch/pytorch/issues/146671
146669,Optimize inductor `Self` typing,"Replace method return type with `Self` typing

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @desertfire @chauhang @aakhundov",open,2025-02-07T01:59:49Z,,,"triaged, open source, topic: not user facing, module: inductor",main,opt/inductor/typing,3,8,5,1,3,0,jansel,APPROVED,False,https://api.github.com/repos/pytorch/pytorch/issues/146669
146668,Make sure cutlass kernel .cu file has configuration name and nvcc compile command,"I think its good to have everything in the .cu file. Especially the nvcc compile command.

Technically, the configuration name can be found in the template already. So let me know if you think its not needed. 

Differential Revision: D69281295




cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @desertfire @chauhang @aakhundov",open,2025-02-07T01:54:45Z,,,"fb-exported, ciflow/trunk, topic: not user facing, module: inductor, ciflow/inductor, merging",main,export-D69281295,2,6,0,1,8,0,chenyang78,APPROVED,False,https://api.github.com/repos/pytorch/pytorch/issues/146668
146664,[Docs] Fix description of `input` in `torch.addbmm()`,"Fixes #146613
",open,2025-02-07T01:27:52Z,,,"open source, ciflow/trunk, release notes: python_frontend, topic: docs, merging",main,docs/addbmm,1,1,1,1,10,1,mikaylagawarecki,APPROVED,False,https://api.github.com/repos/pytorch/pytorch/issues/146664
146662,[Optimus] Bug fix in the select cat aten pass,"Summary: Thanks to Shuai for reporting the bug in the pattern. We found there's a typo in the pass, where we should make sure all the selects will go to the cat node.

Test Plan:
buck2 test 'fbcode//mode/dev-nosan' fbcode//caffe2/test/inductor:split_cat_fx_aten_passes -- test_select_cat_post_grad


Buck UI: https://www.internalfb.com/buck2/2cd0888e-d803-43a8-8530-d97e6bc281b3
Test UI: https://www.internalfb.com/intern/testinfra/testrun/6192449699305108
Network: Up: 110KiB  Down: 35KiB  (reSessionID-687be0fa-031a-47a0-8780-5ab4cf4bbd94)
Executing actions. Remaining     0/4                                                                              6.6s exec time total
Command: test.     Finished 2 local
Time elapsed: 2:12.0s
Tests finished: Pass 2. Fail 0. Fatal 0. Skip 0. Build failure 0

Differential Revision: D69278487




cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @desertfire @chauhang @aakhundov",open,2025-02-07T00:50:59Z,,,"fb-exported, module: inductor, ciflow/inductor, release notes: inductor, inductor_pattern_match",main,export-D69278487,2,53,35,1,4,0,,,False,https://api.github.com/repos/pytorch/pytorch/issues/146662
146661,[cond] Refactor cond_op's signature to take *operands.,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #146661
* #146660


This is a BC-breaking change for hop's IR schema. Previously, 
```python
torch.cond(pred, true_fn, false_fn, (a, b))
# Old representation
torch.ops.higher_order.cond(pred, true_gm, false_gm, (a, b))
# New representation:
torch.ops.higher_order.cond(pred, true_gm, false_gm, a, b)
```
The benefits of this change is that it's much easier to construct the schema since the tuple is flattened. What's particularly troublesome about previous node is that it's hard to represent the mutation and alias information inside the tuple: we have to change the legacy schema parser and verify (maybe re-purpose) the aliasInfo to supports nested aliasInfo inside tuple/list.

We'll also refactor other control flow operators.

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @desertfire @chauhang @aakhundov

Differential Revision: [D69279033](https://our.internmc.facebook.com/intern/diff/D69279033)",open,2025-02-07T00:44:09Z,,,"ciflow/trunk, topic: not user facing, module: inductor, module: dynamo, ciflow/inductor, keep-going, release notes: export",gh/ydwu4/208/base,gh/ydwu4/208/head,13,119,181,3,3,0,zou3519,APPROVED,False,https://api.github.com/repos/pytorch/pytorch/issues/146661
146660,[hop][inductor] don't promote arg type for cond and while_loop,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #146661
* __->__ #146660

Hop subgraph codegen assumes arguments's type are not promoted. Otherwise, we might generate wrong kernel.


cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @desertfire @chauhang @aakhundov

Differential Revision: [D69279031](https://our.internmc.facebook.com/intern/diff/D69279031)",open,2025-02-07T00:44:03Z,,,"ciflow/trunk, topic: not user facing, module: inductor, ciflow/inductor, merging",gh/ydwu4/207/base,gh/ydwu4/207/head,1,2,2,2,6,1,"zou3519, zou3519, ydwu4","COMMENTED, APPROVED, COMMENTED",False,https://api.github.com/repos/pytorch/pytorch/issues/146660
146658,[HOP] Mutation and alias rework,"This PR reworks the way the input mutations and various aliases are checked

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @ydwu4 
",open,2025-02-07T00:28:18Z,,,"open source, topic: not user facing, module: dynamo",main,mutation_alias_rework,12,116,100,5,2,0,,,True,https://api.github.com/repos/pytorch/pytorch/issues/146658
146656,Optimize isclose() for CPU and GPU by adding specific implementations,"`isclose()` is currently quite slow, so this PR adds specific implementations for both CPU and cuda.

CUDA implementation seeing ~4.9x improvement at 100m elements and ~18.7x improvement at 400m elements

CPU implementation seeing ~5.7x improvements at 100m elements and ~5.9x improvements at 400m elements 

simple benchmark used(adapt with CPU as needed):
```python
import time
import numpy as np
import torch

def benchmark_isclose(shape1, shape2, num_runs=5):
    tensor1 = torch.randn(shape1, device=""cuda"")
    tensor2 = tensor1.clone()
    tensor2 += torch.randn_like(tensor2) * 0.001

    # warn up
    _ = torch.isclose(tensor1, tensor2)
    torch.cuda.synchronize()

    times = []
    for _ in range(num_runs):
        start_time = time.perf_counter()

        _ = torch.isclose(tensor1, tensor2)
        torch.cuda.synchronize()
        end_time = time.perf_counter()
        times.append(end_time - start_time)

    mean_time = np.mean(times)
    std_time = np.std(times)

    return mean_time, std_time


test_shapes = [
    (10000, 10000),  # 100M elements
    (20000, 20000),  # 400M elements
]

print(""\nBenchmarking torch.isclose():"")
print(""-"" * 50)

for shape in test_shapes:
    total_elements = np.prod(shape)
    print(f""\nTensor shape: {shape} ({total_elements:,} elements)"")

    mean_time, std_time = benchmark_isclose(shape, shape)
    print(f""Mean time: {mean_time*1000:.2f} ms +/- {std_time*1000:.2f} ms"")
    print(f""Elements per second: {total_elements/mean_time:,.0f}"")
```

```
(optimized)
Tensor shape: (10000, 10000) (100,000,000 elements)
Mean time: 2.73 ms ± 0.26 ms
Elements per second: 36,611,905,024

Tensor shape: (20000, 20000) (400,000,000 elements)
Mean time: 8.98 ms ± 0.28 ms
Elements per second: 44,546,604,660

(unoptimized)
Tensor shape: (10000, 10000) (100,000,000 elements)
Mean time: 13.48 ms ± 0.28 ms
Elements per second: 7,420,814,236

Tensor shape: (20000, 20000) (400,000,000 elements)
Mean time: 166.90 ms ± 4.71 ms
Elements per second: 2,396,711,992
```
cc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @albanD 

",open,2025-02-07T00:09:02Z,,,"module: cpu, triaged, open source",main,feature/isclose-kernels,4,171,57,3,2,1,albanD,COMMENTED,False,https://api.github.com/repos/pytorch/pytorch/issues/146656
146655,torch._scaled_mm with MX dtypes,"making https://github.com/pytorch/pytorch/pull/145562 work with in-core dtypes

not ready for review yet",open,2025-02-07T00:02:24Z,,,ciflow/inductor,gh/vkuzo/2/head,gh/vkuzo/3/head,8,291,79,3,2,0,,,False,https://api.github.com/repos/pytorch/pytorch/issues/146655
146654,[DCP] Introduce modules metadata in the storage_meta,"Summary: Introduce the list of modules in the storage_meta which is shared between the planner and the storage writer. We will use it to let the storage writer know about the modules in the state dict and create module directories in the checkpoint.

Test Plan: UTs

Differential Revision: D69154628




cc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o @LucasLLC @MeetVadakkanchery @mhorowitz @pradeepfn @ekr0",open,2025-02-06T23:43:44Z,,,"oncall: distributed, fb-exported, module: distributed_checkpoint",main,export-D69154628,1,2,1,1,3,0,mhorowitz,COMMENTED,False,https://api.github.com/repos/pytorch/pytorch/issues/146654
146653,windows Magma build for cu128,"https://github.com/pytorch/pytorch/issues/145570

removing `.ci/pytorch/windows/internal/cuda_install.bat` as it is a duplicate with` .github/scripts/windows/cuda_install.bat`. The later one is the one in use - https://github.com/pytorch/pytorch/pull/146653/files#diff-613791f266f2f7b81148ca8f447b0cd6c6544f824f5f46a78a2794006c78957bR8

cc @atalman @ptrblck @nWEIdia ",open,2025-02-06T23:33:34Z,,,"open source, Merged, Reverted, release notes: releng, ciflow/binaries_wheel, ci-no-td",main,cu128-win-magma,5,95,223,4,6,1,atalman,APPROVED,False,https://api.github.com/repos/pytorch/pytorch/issues/146653
146642,[poc] force UntypedStorage.from_buffer(buf) to return meta storage under FakeTensorMode,"context here: https://fb.workplace.com/groups/326136610199609/permalink/495389539940981/

This PR is an attempt to make it such that if you create a tensor from an external buffer (using `UntypedStorage.from_buffer(buf)`, we can generate a proper fake tensor for you out of the box.

The annoying bit is that there are not any dispatcher ops to interpose on and change behavior. So instead, I took the manual C binding and tweaked the storage device to be ""meta' if we see an active fake mode.

Put ""poc"" in the title since I... think this is hopefully reasonable, but I can be convinced that it's not :)

```
from torch._subclasses.fake_tensor import FakeTensorMode
import pickle
import io
import torch
from contextlib import nullcontext


use_fake_tensor = True
with FakeTensorMode() if use_fake_tensor else nullcontext():
    obj = [1, 2]
    f = io.BytesIO()
    pickle.Pickler(f).dump(obj)
    byte_storage = torch.ByteStorage._from_buffer(f.getvalue())  # type: ignore[attr-defined]
    
    t = torch.ByteTensor(byte_storage)
```

Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #133044
* #146729
* __->__ #146642
* #146731

",open,2025-02-06T21:50:27Z,,,release notes: composability,gh/bdhirsh/639/base,gh/bdhirsh/639/head,2,27,8,4,2,1,zou3519,APPROVED,False,https://api.github.com/repos/pytorch/pytorch/issues/146642
146641,[dim order]  solve broken doc,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #146641

Differential Revision: [D69265340](https://our.internmc.facebook.com/intern/diff/D69265340/)",open,2025-02-06T21:49:03Z,,,"fb-exported, ciflow/trunk, topic: not user facing, merging",gh/gasoonjia/2/base,gh/gasoonjia/2/head,1,3,0,3,8,0,"svekars, svekars, Jack-Khuu","COMMENTED, APPROVED, APPROVED",False,https://api.github.com/repos/pytorch/pytorch/issues/146641
146640,POC for mixed prec optim frontend,"This PR is a prototype for what a frontend for asking for mixed precision can look like torch.optim through set_dtype_policy in optimizer.py.

This is not meant to be landable but to start some discussions on what people want/would like to see and to ask if there are things I haven't considered yet.

This currently only works with Adam(W)!

A toy script for how to use:
```
import torch

model = torch.nn.Sequential(
    torch.nn.Linear(2, 3),
    torch.nn.Sigmoid(),
    torch.nn.Linear(3, 1),
    torch.nn.Sigmoid(),
)
model.to(""cuda"")

optim = torch.optim.AdamW(model.named_parameters(), foreach=False)
mp_policy = {
    ""exp_avg"": lambda _: torch.bfloat16,
    ""exp_avg_sq"": lambda _: torch.bfloat16,
    ""max_exp_avg_sq"": lambda _: torch.bfloat16,
}
optim.set_dtype_policy(mp_policy)

i = torch.tensor([0.1, 0.2, 0.3, 0.4, 0.5, 0.6], device=""cuda"").reshape(3, 2)
l = model(i).sum()
l.backward()

optim.step()
```

Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #146640

",open,2025-02-06T21:31:50Z,,,release notes: optim,gh/janeyx99/222/base,gh/janeyx99/222/head,3,113,11,2,1,0,janeyx99,COMMENTED,True,https://api.github.com/repos/pytorch/pytorch/issues/146640
146638,use None to slice when list has one element only,"When autotune_num_choices_displayed is None and the list of choices has length 1, slicing with `[:-1]` means getting all elements except the last one, which resulted in an empty list.

Slicing with `[:None]` works. 

Differential Revision: D69265168




cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @desertfire @chauhang @aakhundov",open,2025-02-06T21:08:40Z,,,"fb-exported, ciflow/trunk, topic: not user facing, module: inductor, ciflow/inductor, merging",main,export-D69265168,1,2,5,1,8,0,drisspg,APPROVED,False,https://api.github.com/repos/pytorch/pytorch/issues/146638
146637,gloo: fix building system gloo with CUDA/HIP,"Fix incorrect linking of Gloo's libraries when building with system Gloo. Previously, either Gloo's native library or Gloo's CUDA library were linked. However, Gloo had changed such that all users of Gloo must link the native library, and can optionally link the CUDA or HIP library for Gloo + CUDA/HIP support.
This had been updated when building/linking with vendored Gloo, but not when using system Gloo.

Fixes: #146239

Reported-by: Adam J Stewart <ajstewart426@gmail.com>



cc @malfet @seemethere @ptrblck @msaroufim @eqy",open,2025-02-06T21:04:17Z,,,"module: build, module: cuda, triaged, open source, topic: not user facing",main,gloo_cuda,2,25,25,1,2,0,,,False,https://api.github.com/repos/pytorch/pytorch/issues/146637
146636,example repro failure,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #146636

Summary:

Test Plan:

Reviewers:

Subscribers:

Tasks:

Tags:

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",open,2025-02-06T21:00:47Z,,,"module: dynamo, ciflow/inductor",gh/c00w/37/base,gh/c00w/37/head,2,5,0,1,2,0,,,True,https://api.github.com/repos/pytorch/pytorch/issues/146636
146634,Add Structured Tracing for Traced Graph Edge Details for AC Debugging,"Summary:
Updating the structured trace infrastructure so that we are able to output to Zoomer and have an E2E solution.

Context Doc: https://docs.google.com/document/d/1T6omIBEWVhbOiwDLSLffgQwjxiT2rQv8QvvQwXkw4fY/edit?usp=sharing

Test Plan:
### Testing Structured Log + tlparse locally

Command:
```
TORCH_TRACE=/data/users/basilwong/fbsource/fbcode/log_torch_trace buck2 run mode/opt //aps_models/ads/icvr:icvr_launcher -- mode=local_fb_fm_v4 launcher.num_workers=2
```

Torch Trace Logs (local then sent to paste): P1686419449
```
cat log_torch_trace/dedicated_log_torch_trace_rank_0_2lg012xo.log | pastry
P1686419449
```

tlparse output: https://manifold.edge.x2p.facebook.net/v0/read/tree/logs/.tmpyiv5wj/rank_1/index.html?bucketName=tlparse_reports&apiKey=tlparse_reports-key&withPayload=1&timeoutMsec=100

tlparse graph edge details output: https://manifold.edge.x2p.facebook.net/v0/read/tree/logs/.tmpyiv5wj/rank_1/9_0_0/joint_graph_information_397.txt?bucketName=tlparse_reports&apiKey=tlparse_reports-key&withPayload=1&timeoutMsec=100

Differential Revision: D61557220


",open,2025-02-06T20:29:48Z,,,"fb-exported, topic: not user facing, ciflow/inductor",main,export-D61557220,2,137,38,1,4,1,jansel,CHANGES_REQUESTED,False,https://api.github.com/repos/pytorch/pytorch/issues/146634
146633,[NJT] Fix inference mode for composite implicit ops without nested-specific kernel,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #146633

",open,2025-02-06T20:02:09Z,,,"ciflow/trunk, topic: bug fixes, release notes: nested tensor, merging",gh/soulitzer/352/base,gh/soulitzer/352/head,2,27,4,3,10,1,jbschlosser,APPROVED,False,https://api.github.com/repos/pytorch/pytorch/issues/146633
146632,[ROCm] OCP FP8 Support for new GPUs,"TLDR: Follow up/ Build on top of https://github.com/pytorch/pytorch/pull/144476. add OCP FP8 support for gfx950
refer to https://github.com/pytorch/ao/pull/1677

This pull request includes several changes to improve compatibility and support for new GPU architectures and data types, particularly for ROCm. The key updates involve adding support for new ROCm versions and GPU architectures, updating data type handling, and removing outdated checks.

### Improvements to GPU Architecture and ROCm Version Support:
* [`aten/src/ATen/Context.cpp`](diffhunk://#diff-33de472d304acbe57d693c8567370c638068bedc1aa0ce8e9dc115dad05a7810L323-R326): Added support for new GPU architectures `gfx1200`, `gfx1201`, and `gfx950` based on ROCm version checks.
* [`aten/src/ATen/native/cuda/Blas.cpp`](diffhunk://#diff-e8a569efee1e650172f120a0fdcda024fe3e4703a4ee3336425c8f685af6b3abL196-R199): Updated architecture support in multiple functions to include `gfx1200`, `gfx1201`, and `gfx950` based on ROCm version checks. [[1]](diffhunk://#diff-e8a569efee1e650172f120a0fdcda024fe3e4703a4ee3336425c8f685af6b3abL196-R199) [[2]](diffhunk://#diff-e8a569efee1e650172f120a0fdcda024fe3e4703a4ee3336425c8f685af6b3abL865-R876)

### Updates to Data Type Handling:
* [`aten/src/ATen/cuda/CUDADataType.h`](diffhunk://#diff-9188bb13b1a49f459141f5f9b875593d1c5ce2beb5ad711fdbaf5bc7089ec015L81-L98): Enhanced data type conversion to include new float8 types for both CUDA and ROCm environments.
* [`aten/src/ATen/cuda/tunable/GemmHipblaslt.h`](diffhunk://#diff-bfa1a3b5d4bef1892bf50338775f3b0fd8cd31fc1868148f3968b98aefb68e3fL29-R80): Updated `HipDataTypeFor` template to handle new float8 types and added hard-coded enum values for ROCm versions prior to 6.3.

### Removal of Outdated Checks:
* [`cmake/public/LoadHIP.cmake`](diffhunk://#diff-b98e27b9a5f196a6965a99ee5a7bb15b3fc633d6375b767635b1b04ccb2fd3d5L169-L197): Removed the check for `HIP_NEW_TYPE_ENUMS` as it is no longer necessary with the updated ROCm versions. [[1]](diffhunk://#diff-b98e27b9a5f196a6965a99ee5a7bb15b3fc633d6375b767635b1b04ccb2fd3d5L169-L197) [[2]](diffhunk://#diff-b98e27b9a5f196a6965a99ee5a7bb15b3fc633d6375b767635b1b04ccb2fd3d5L211-R182)

These changes ensure better compatibility and performance on newer hardware and software environments, particularly for users leveraging ROCm and CUDA for deep learning and scientific computing tasks.


cc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd",open,2025-02-06T18:57:21Z,,,"module: rocm, open source, release notes: linalg_frontend",main,ocp_gfx950,11,98,25,8,1,0,jeffdaily,APPROVED,False,https://api.github.com/repos/pytorch/pytorch/issues/146632
146631,Support ignoring parameters in FSDP2,"Differential Revision: D69153051




cc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o",open,2025-02-06T18:46:40Z,,,"oncall: distributed, fb-exported, release notes: distributed (fsdp), ciflow/inductor",main,export-D69153051,3,392,5,1,17,2,"awgu, weifengpy, weifengpy, weifengpy","COMMENTED, COMMENTED, COMMENTED, COMMENTED",False,https://api.github.com/repos/pytorch/pytorch/issues/146631
146626, [inductor] Improve type annotations in _inductor/pattern_matcher.py,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #146626
* #146248



cc @ezyang @SherlockNoMad @EikanWang @jgong5 @wenzhe-nrv @voznesenskym @penguinwu @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @desertfire @chauhang @aakhundov",open,2025-02-06T17:59:04Z,,,"open source, release notes: fx, topic: not user facing, fx, module: inductor, ciflow/inductor, suppress-api-compatibility-check, suppress-bc-linter",gh/rec/132/base,gh/rec/132/head,2,41,33,6,2,0,,,True,https://api.github.com/repos/pytorch/pytorch/issues/146626
146625,Move capture_provenance to make_node_impl,"Previously we were only logging `make_user_impl` implementations, which only gets triggered for operations done on python SymInts, not cpp SymInts. Instead `make_node_impl` will get triggered for both python and cpp SymInt operations.


cc @ezyang @SherlockNoMad @EikanWang @jgong5 @wenzhe-nrv",open,2025-02-06T17:53:35Z,,,"ciflow/trunk, release notes: fx, fx, ciflow/inductor, merging",main,angelayi/test_expression_created,1,89,90,1,13,2,"bobrenjc93, bobrenjc93","APPROVED, APPROVED",False,https://api.github.com/repos/pytorch/pytorch/issues/146625
146623,bug fix: ensure 4d input in _scaled_dot_product_attention_math_mps,"This pr addresses the issue in the MPS backend for `_scaled_dot_product_attention_math_mps` where a 3d input like (num_heads, seq_len, query_dim) cannot be automatically treated as (1, num_heads, seq_len, query_dim), which can be inferred on cpu or cuda, which can be circumvented by adding a util function to ensure a 4d shape.

The issue was found in https://github.com/hiyouga/LLaMA-Factory/issues/6835, in [transformers qwen2_vl](https://github.com/huggingface/transformers/blob/1590c664306766f32ba68c50e67f14d61b16925d/src/transformers/models/qwen2_vl/modeling_qwen2_vl.py#L373C14-L373C93), 3d q/k/v were passed into sdpa function, which lead to an error.

Considering consistency, since this pattern might pop up elsewhere in the transformers codebase, I think it makes more sense to maintain the same intuition across all platforms.

---
reproduce code:
```
import torch
import torch.nn.functional as F

head_num, seq_len, embed_dim = 16, 16, 80
bsz = 1

q = torch.randn(head_num, seq_len, embed_dim)
k = torch.randn(head_num, seq_len, embed_dim)
v = torch.randn(head_num, seq_len, embed_dim)
attention_mask = torch.ones(1, seq_len, seq_len)

oo_cpu = F.scaled_dot_product_attention(
    q.to(""cpu""),
    k.to(""cpu""),
    v.to(""cpu""),
    attention_mask.to(""cpu""),
    dropout_p=0.0
)

if torch.backends.mps.is_available():
    oo_mps = F.scaled_dot_product_attention(
        q.to(""mps""),
        k.to(""mps""),
        v.to(""mps""),
        attention_mask.to(""mps""),
        dropout_p=0.0
    )
    assert torch.allclose(oo_cpu, oo_mps.to(""cpu""), atol=1e-5)
```

error outputs:
```
Traceback (most recent call last):
  File ""/opt/homebrew/Caskroom/miniconda/base/envs/torch-dev/lib/python3.10/site-packages/IPython/core/interactiveshell.py"", line 3577, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-2-5169b8d2c5dd>"", line 21, in <module>
    oo_mps = F.scaled_dot_product_attention(
IndexError: Dimension out of range (expected to be in range of [-3, 2], but got 3)
```

hardware and envs:
```
torch               2.6.0
apple m3 max
```

---

",open,2025-02-06T17:15:59Z,,,"triaged, open source, topic: bug fixes, release notes: mps, ciflow/mps",main,main,2,66,14,6,5,0,"Skylion007, Skylion007, malfet, malfet","COMMENTED, COMMENTED, COMMENTED, COMMENTED",False,https://api.github.com/repos/pytorch/pytorch/issues/146623
146622,Fix inductor non-stable argsort/sort test,"- Prevent the inductor test for argsort/sort from wrongly failing when the argsort/sort output with stable=False differs from pytorch but is still a valid argsort output.
- Add functionality to allow alternative assert_equal functions in inductor tests for future cases.


cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @desertfire @chauhang @aakhundov",open,2025-02-06T17:10:12Z,,,"triaged, open source, topic: not user facing, module: inductor",main,PYT-466,2,169,19,1,1,0,,,False,https://api.github.com/repos/pytorch/pytorch/issues/146622
146620,Enable qint8 and quint8 add for AArch64 using ACL directly,"This enables qint8 and quint8 add for AArch64 through Arm Compute Library (ACL) directly.
It’s based on changes in PR #145942 which enables the use of ACL directly in ATen.
Relative performance improvement using OMP_NUM_THREADS=1 is ~15x, using OMP_NUM_THREADS=32 it’s ~5.4x.

Script to benchmark quantised add performance:
```
import torch
import torch.profiler as profiler

a_f32 = torch.rand((400, 3456),dtype=torch.float)
b_f32 = torch.rand((400, 3456),dtype=torch.float)
a_q = torch.quantize_per_tensor(a_f32, 1.2, 0, torch.qint8)
b_q = torch.quantize_per_tensor(b_f32, 1.7, 5, torch.qint8)

with profiler.profile(with_stack=True, profile_memory=False, record_shapes=True) as prof:
    for i in range(1000):     
        _ = torch.ops.quantized.add(a_q, b_q, 1.3, 2)
print(prof.key_averages(group_by_input_shape=True).table(sort_by='self_cpu_time_total', row_limit=50))
```

cc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10",open,2025-02-06T16:54:08Z,,,"module: cpu, triaged, open source, release notes: quantization, release notes: releng",main,acl_qadd,10,577,15,3,2,1,"malfet, malfet","COMMENTED, COMMENTED",False,https://api.github.com/repos/pytorch/pytorch/issues/146620
146617,Generate test reports for pytest when option is given,"The argument needs to be appended when test reports should be generated. `IS_CI` is not necessarily set, so rather check `TEST_SAVE_XML` instead as in other places where test reports are conditionally enabled.

See also https://github.com/pytorch/pytorch/issues/126523",open,2025-02-06T16:12:06Z,,,"triaged, open source, topic: not user facing",main,Flamefire-patch-1,1,2,1,1,1,0,,,False,https://api.github.com/repos/pytorch/pytorch/issues/146617
146616,[don't merge] test baseline,"Fixes #ISSUE_NUMBER
",open,2025-02-06T16:04:22Z,,,"open source, topic: not user facing, ciflow/binaries_wheel, ciflow/xpu",main,test_main,1,1,0,1,7,0,,,True,https://api.github.com/repos/pytorch/pytorch/issues/146616
146615,[BE][Ez]: Enable specific ruff rules to prevent antipatterns and bugs,"Enables a few ruff rules
* Ban print statements within asserts (likely bugs)
* ~Use string for Decimal literal to prevent loss of precision~ 
* ~Do not use default args for __post__init__ in dataclasses, they likely were meant to go into the factory method, the __init__, or somewhere else. The default values are useless here.~

Wait until ruff upgrade for the last 2

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",open,2025-02-06T16:04:20Z,,,"triaged, open source, better-engineering, topic: not user facing, module: dynamo, ciflow/inductor",main,skylion007/enable-RUF-2025-02-06,2,4,3,3,1,0,,,False,https://api.github.com/repos/pytorch/pytorch/issues/146615
146614,[CD] Add python 3.13t build for xpu,"Fixes #146451
",open,2025-02-06T15:55:52Z,,,"open source, ciflow/trunk, topic: not user facing, ciflow/binaries_wheel",main,xpu_py_3_13t,3,339,2,1,5,1,atalman,APPROVED,False,https://api.github.com/repos/pytorch/pytorch/issues/146614
146612,[WIP] BaseSubclass,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #146612

",open,2025-02-06T15:33:23Z,,,,gh/IvanKobzarev/100/base,gh/IvanKobzarev/100/head,3,98,17,1,2,0,,,False,https://api.github.com/repos/pytorch/pytorch/issues/146612
146604,[Profiler] Enable CUPTI teardown to reduce profiler overhead,"The problem is that the profiler slowed down
training by roughly 10-20% even after completion
because cuptiFinalize was not called in Kineto due to TEARDOWN_CUPTI=0. Disabling CUPTI teardown was a workaround for crashes which occured when CUDA graphs were used. This issue was fixed in CUDA 12.6. Also there is no point in disabling CUPTI teardown if CUDA Graphs are not used.

Fixes #144455 


cc @robieta @chaekit @guotuofeng @guyang3532 @dzhulgakov @davidberard98 @briancoutinho @sraikund16 @sanrise",open,2025-02-06T13:43:37Z,,,"triaged, open source, oncall: profiler, topic: not user facing",main,fix/144455_teardown_cupti,2,22,4,4,8,1,"sraikund16, sraikund16, davidberard98, mgmtea, mgmtea, mgmtea, mgmtea","APPROVED, APPROVED, CHANGES_REQUESTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED",False,https://api.github.com/repos/pytorch/pytorch/issues/146604
146597,Fix bug in _ref_test_helper in test_ops and fix failing test on Aarch64,"We have a failing unit test on Aarch64

```
Exception: Caused by reference input at index 34: SampleInput(input=Tensor[size=(5, 5, 4), device=""cpu"", dtype=torch.complex64, contiguous=False], args=(), kwargs={}, broadcasts_input=False, name='')

To execute this test, run the following from the base repo dir:
    PYTORCH_OPINFO_SAMPLE_INPUT_INDEX=34 python test/test_ops.py TestCommonCPU.test_python_ref__refs_square_cpu_complex64

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
```

After debugging it I found that `ex` variable is not being reset to None on each loop inside _ref_test_helper. Which after fixing, highlighted another expectedFailure to reenable - `nn.functional.hinge_embedding_loss` which was incorrectly being skipped due to the same problem.

https://github.com/pytorch/pytorch/blob/4a545eb85d6ba06079787a83f8ab1a8c8f67c76f/test/test_ops.py#L546
ex variable is not reset after this for next loop iteration",open,2025-02-06T11:07:07Z,,,"triaged, open source, topic: not user facing",main,test_fix,3,3,11,1,3,0,,,False,https://api.github.com/repos/pytorch/pytorch/issues/146597
146596,separate f16 vectorized class from bf16,"This refactoring is required as part of https://github.com/pytorch/pytorch/pull/143666

cc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @malfet @snadampal @milpuz01",open,2025-02-06T10:58:15Z,,,"module: cpu, open source, module: arm, topic: not user facing",main,refactor,4,1031,404,1,1,0,,,True,https://api.github.com/repos/pytorch/pytorch/issues/146596
146595,skip test_torch_dynamo_codegen_pow if CPU backend is not cpp,"The test asserts that `aten.pow` is not present in the generated kernel code. When using a CPU backend other than cpp, the kernel contains comments referencing the aten ops that produced the kernel in this case `aten.pow`. 

This PR skips that test case if the CPU backend is not cpp.


cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",open,2025-02-06T10:46:32Z,,,"triaged, open source, topic: not user facing, module: dynamo",main,skip-if-cpu-backend-not-cpp,1,4,0,1,1,0,,,False,https://api.github.com/repos/pytorch/pytorch/issues/146595
146593,[NOT FOR LANDING] experimental NVSHMEM integration,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #146593
* #146592



cc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o",open,2025-02-06T10:26:18Z,,,"oncall: distributed, release notes: distributed (c10d)",gh/yifuwang/195/base,gh/yifuwang/195/head,7,471,4,2,1,0,,,True,https://api.github.com/repos/pytorch/pytorch/issues/146593
146592,clang-format CUDASymmetricMemory.cu,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #146593
* __->__ #146592



cc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o",open,2025-02-06T10:26:14Z,,,"oncall: distributed, release notes: distributed (c10d)",gh/yifuwang/194/base,gh/yifuwang/194/head,1,20,10,1,2,0,,,True,https://api.github.com/repos/pytorch/pytorch/issues/146592
146589,[DDP] Use NCCL allocated memory for gradient bucket,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #146589

So that NVLink SHARP comes with zero-copy on H100+ platforms, for DDP applications.
Less SM usage, less memory contention between NCCL kernel and compute kernels.

Added env `DDP_DISABLE_COMM_MEM` as a back-out option:
```
An environment variable to disable comm-optimized memory pool.
Default is 0, which means comm-optimized memory pool is enabled.
Users can set it to 1 in case of seeing regression or OOM (because this
comm MemPool may not share space with regular compute MemPool).
```

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o

Differential Revision: [D69297766](https://our.internmc.facebook.com/intern/diff/D69297766)",open,2025-02-06T09:01:24Z,,,"oncall: distributed, ciflow/trunk, release notes: distributed (c10d), release notes: distributed (ddp), merging",gh/kwen2501/123/base,gh/kwen2501/123/head,7,127,13,6,14,2,"Skylion007, Skylion007, Skylion007, Skylion007, kwen2501, Skylion007, kwen2501, syed-ahmed, syed-ahmed, syed-ahmed, syed-ahmed, syed-ahmed, syed-ahmed, yifuwang, kwen2501, yifuwang, fegin, syed-ahmed, kwen2501, c-p-i-o, c-p-i-o, c-p-i-o, c-p-i-o, fduwjj, fduwjj","COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, CHANGES_REQUESTED, APPROVED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, APPROVED, COMMENTED, APPROVED",False,https://api.github.com/repos/pytorch/pytorch/issues/146589
146587,[Dynamo] Allow dynamo to handle `str.xxx()`,"
Fixes #146350


cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",open,2025-02-06T08:47:14Z,,,"open source, topic: not user facing, module: dynamo",main,fix/dynamo/str,2,13,0,2,10,1,"zou3519, shink, shink, zou3519","COMMENTED, COMMENTED, COMMENTED, APPROVED",False,https://api.github.com/repos/pytorch/pytorch/issues/146587
146583,[symbolic shapes] Log symnode id,"We want to log the symnode id which will help us with provenance tracking between expressions created.


cc @ezyang @SherlockNoMad @EikanWang @jgong5 @wenzhe-nrv",open,2025-02-06T07:45:13Z,,,"ciflow/trunk, release notes: fx, fx, ciflow/inductor, merging",main,angelayi/test,1,26,7,1,7,1,bobrenjc93,APPROVED,False,https://api.github.com/repos/pytorch/pytorch/issues/146583
146582,[Partitioner] Reduce time consuming of partitions merger,"This patch optimize maybe_merge_partition func through 3-ways:

Remove unnecessary copy https://github.com/pytorch/pytorch/blob/main/torch/fx/passes/infra/partitioner.py#L99. The number of copied nodes is large if we can merge all of the nodes of graph into one partition.
Record users of each partition to avoid duplicate iteration over nodes https://github.com/pytorch/pytorch/blob/main/torch/fx/passes/infra/partitioner.py#L133. The trip count of this loop maybe very large.
The nodes number of each partitions maybe not balance https://github.com/pytorch/pytorch/blob/main/torch/fx/passes/infra/partitioner.py#L145. We always encounter one issue: one partition has n nodes, but the other has one node. Merge the smaller partition into the larger can help to reduce time consuming.

cc @ezyang @SherlockNoMad @EikanWang @jgong5 @wenzhe-nrv",open,2025-02-06T07:35:55Z,,,"triaged, open source, release notes: fx, topic: not user facing, fx",main,lingzhiz/optimize_partition_merger,1,39,26,3,2,0,Skylion007,COMMENTED,False,https://api.github.com/repos/pytorch/pytorch/issues/146582
146581,Clarify that compile(module) only affects the forward method,"Fixes #141616

## Changes

- Add `Note` to Clarify how compile works with `nn.Module`
- Optimize plain url address with clickable description

## Test Result

### Before

![image](https://github.com/user-attachments/assets/15ff9985-7e91-4d71-be7d-cdd38eacd3f9)
![image](https://github.com/user-attachments/assets/26e27ba4-52da-4336-b72d-a0f9d0ebe839)


### After

![image](https://github.com/user-attachments/assets/5eaa8421-19e8-4186-af3d-dab4323d2c95)
![image](https://github.com/user-attachments/assets/a96a8a79-6320-4748-931f-33f4dbc640eb)

",open,2025-02-06T07:34:49Z,,,"triaged, open source, topic: not user facing",main,opt/docs/compile,1,8,2,1,2,0,,,False,https://api.github.com/repos/pytorch/pytorch/issues/146581
146580,[Partitioner] Remove unnecessary upstream nodes in dependency viewer,"We iterate upstream nodes to update partition map. But actually did nothing due to we iterate nodes with reversed topological order https://github.com/pytorch/pytorch/pull/136608/files#diff-f2f9dd3903fd99955732eb694941fea0cb7301a58d59554787f3311d417e5615L193 so that there exists no upstream nodes in assignment. Remove it to reduce for-loop overhead which up to O(N * N) complexity.


cc @ezyang @SherlockNoMad @EikanWang @jgong5 @wenzhe-nrv",open,2025-02-06T07:32:01Z,,,"triaged, open source, release notes: fx, topic: not user facing, fx",main,lingzhiz/remove_upstream_nodes,1,0,19,1,2,0,,,False,https://api.github.com/repos/pytorch/pytorch/issues/146580
146578,add `torch.float4_e2m1fn_x2` to PyTorch,"Summary:

Adds the `torch.float4_e2m1fn_x2` dtype to PyTorch, as detailed in
https://github.com/pytorch/pytorch/issues/146414 . Please see the issue for a detailed definition of the format.  

Note that I decided to keep the casts out of this to significantly simplify the code, as defining casting between packed and unpacked formats will be tricky using the existing casting machinery.  

Example of basic functionality:

```python
import torch

# creation with empty
x0 = torch.empty(4, 4, dtype=torch.float4_e2m1fn_x2)

# printing, prints the uint8 representation of the stored values
print(x0)

# view as other dtype
x0.view(torch.uint8).view(torch.float4_e2m1fn_x2)
```

Done in this PR:
* tensor creation and tensor printing works (no other ops defined)

For future PRs:
* torch._scaled_mm
* PT2
* various cleanups (detailed in comments with issue numbers)

Test Plan:

```
pytest test/quantization/core/experimental/test_floatx.py -s
```

Reviewers:

Subscribers:

Tasks:

Tags:

cc @yanbing-j @albanD @kadeng @penguinwu @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10",open,2025-02-06T07:08:42Z,,,release notes: quantization,gh/vkuzo/1/head,gh/vkuzo/2/head,7,70,5,3,2,0,,,False,https://api.github.com/repos/pytorch/pytorch/issues/146578
146574,[ROCm][TunableOp] Offline results are saved to file when offline tuning is disabled.,"This PR is to fix UT breakage that has been reported internally and is considered high priority. When `tunable.record_untuned_enable(False)` is invoked, we flush the results of the untuned gemm file.

Offline tuning I/O currently doesn't have a set untuned results filename member function or untuned results write to file member function. When performing back-to-back unit tests, the same ofstream ends up getting reused between UTs. Due to the way the UT are executed, this can lead to unexpected failures.

cc: @jfactory07 



cc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang",open,2025-02-06T06:04:28Z,,,"module: rocm, open source, topic: not user facing, ciflow/rocm",main,fix_tunableop_untuned_fileio,1,2,0,1,4,0,jeffdaily,APPROVED,False,https://api.github.com/repos/pytorch/pytorch/issues/146574
146573,add python root bin to windows load path.,"This PR is extend python root bin path to dll load list. 
It makes PyTorch robust and compatible to more dependency libraries, such as `intel-pti`.

cc @peterjc123 @mszhanyi @skyline75489 @nbcsm @iremyux @Blackhex @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10",open,2025-02-06T05:58:55Z,,,"module: windows, open source, ciflow/trunk, topic: not user facing, intel",main,xu_add_init_path,1,8,1,1,1,0,EikanWang,APPROVED,False,https://api.github.com/repos/pytorch/pytorch/issues/146573
146571,[Dynamo][autograd.Function] Relax backward speculation strict mode a bit,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #146742
* #146741
* __->__ #146571



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",open,2025-02-06T05:05:51Z,,,"topic: not user facing, module: dynamo, ciflow/inductor",gh/yanboliang/63/base,gh/yanboliang/63/head,7,228,44,5,1,0,"yanboliang, zou3519, zou3519, zou3519, yanboliang, zou3519, zou3519","COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED",False,https://api.github.com/repos/pytorch/pytorch/issues/146571
146562,WIP hacky reordering pass,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #146562
* #146561
* #146560
* #146559
* #146558



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @desertfire @chauhang @aakhundov",open,2025-02-06T01:41:51Z,,,"module: inductor, ciflow/inductor",gh/wconstab/396/base,gh/wconstab/396/head,3,325,24,1,2,0,,,False,https://api.github.com/repos/pytorch/pytorch/issues/146562
146561,Improve comms debug visualization,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #146562
* __->__ #146561
* #146560
* #146559
* #146558

",open,2025-02-06T01:41:46Z,,,"module: inductor, ciflow/inductor",gh/wconstab/395/base,gh/wconstab/395/head,1,2,3,1,2,0,,,False,https://api.github.com/repos/pytorch/pytorch/issues/146561
146560,enable reorder,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #146562
* #146561
* __->__ #146560
* #146559
* #146558

",open,2025-02-06T01:41:41Z,,,"module: inductor, ciflow/inductor",gh/wconstab/394/base,gh/wconstab/394/head,1,1,1,1,2,0,,,False,https://api.github.com/repos/pytorch/pytorch/issues/146560
146559,Apply changes from https://github.com/pytorch/pytorch/commit/211847de3c1c3d6cbd299e14a001b794eabf2a2d,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #146562
* #146561
* #146560
* __->__ #146559
* #146558

",open,2025-02-06T01:41:36Z,,,"oncall: distributed, ciflow/inductor",gh/wconstab/393/base,gh/wconstab/393/head,1,65,15,1,2,0,,,False,https://api.github.com/repos/pytorch/pytorch/issues/146559
146558,[dtensor] support mixed precision for redistribute (#20),"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #146562
* #146561
* #146560
* #146559
* __->__ #146558

",open,2025-02-06T01:41:31Z,,,"oncall: distributed, ciflow/inductor",gh/wconstab/392/base,gh/wconstab/392/head,2,52,15,1,3,0,,,False,https://api.github.com/repos/pytorch/pytorch/issues/146558
146557,Add fqn_modifier at loading_state_dict and unit test,"In Fusion model, users might change the state_dict keys by state_dict_hook
The load_state_dict APIs here won't call model.state_dict() so that the hooks won't be called to change the keys, causing the mismatch between fqn and state_dict keys.

The PR here suggests users to add how they would change the state_dict key prefix (they can name it, here we call ""fqn_modifiers"") by default
During loading state_dict, we have the prefix change during getting fqn so that they can be processed same as through state_dict hook.

For example:
There's a state_dict_hook:

```
def _state_dict_hook(self, destination, prefix, keep_vars):
    """"""Remove ""embedding"" from the original embedding in the state_dict
    name. This keeps the orginal state dict name for the embedding
    from before fusing with the FusionEmbedding.

    [!Note] This update changes the order of the OrderedDict
    """"""
    key = prefix + ""embedding.weight""
    new_key = prefix + ""weight""
    destination[new_key] = destination[key]
    del destination[key]
```

In the dsd after this PR, we would skip ""embedding."" before ""weight"" if find the ""fqn_modifiers"" attribute at that module
```
def fqn_modifiers(self) -> Dict[str, str]:
    return {
        ""weight"": ""embedding"",
    }
```

cc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o @LucasLLC @MeetVadakkanchery @mhorowitz @pradeepfn @ekr0",open,2025-02-06T01:16:54Z,,,"oncall: distributed, topic: not user facing, module: distributed_checkpoint",main,dsd_fqn_modifiers,3,97,7,4,1,0,,,False,https://api.github.com/repos/pytorch/pytorch/issues/146557
146545,Update test.sh to run a greater set of unit tests on aarch64,"expanded set of unit tests that run  on aarch64 to be the entire set of tests that can be run by run_test.py



cc @seemethere @malfet @pytorch/pytorch-dev-infra",open,2025-02-05T23:41:27Z,,,"module: ci, triaged, open source, topic: not user facing",main,expanded_unit_tests,1,1,22,2,3,0,,,False,https://api.github.com/repos/pytorch/pytorch/issues/146545
146543,Update local_timer.py to improve queue handling,"- Switched from `multiprocessing.Queue` to `torch.multiprocessing.Queue`
- Wrapped `qsize()` in `try-except` to prevent `NotImplementedError`

cc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o",open,2025-02-05T23:33:01Z,,,"oncall: distributed, triaged, open source, release notes: distributed (torchelastic)",main,update-local-timer,1,12,9,1,2,0,,,False,https://api.github.com/repos/pytorch/pytorch/issues/146543
146540,[export] Draft export custom streamer,"* Instead of using tlparse's StreamHandler, draft-export will use its own, which will capture the logs, filter them, and only output the relevant ones to the log file. 
* To do this, the CaptureStructuredTrace logger will use a `LogRecord` which is basically a dictionary with a custom hash function based on what is being logged. This allows us to deduplicate logs which represent the same thing, such as:
  * ""missing_fake_kernel"" logs with the same operator
  * ""mismatched_fake_kernel"" logs with the same operator and reasoning
  * ""propagate_real_tensor"", ""create_unbacked_symbol"", and ""guard_added"" logs occurring on lines with the same stacktrace",open,2025-02-05T23:24:25Z,,,release notes: export,main,angelayi/draft_logger,2,89,48,1,1,0,,,True,https://api.github.com/repos/pytorch/pytorch/issues/146540
146537,[WIP] Log graph breaks,"Graph breaks currently aren't logged. We want to log them. Need to test before merging.

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @desertfire @chauhang @aakhundov",open,2025-02-05T23:08:22Z,,,"topic: not user facing, module: inductor, module: dynamo, ciflow/inductor, dynamo-logging",main,gh/raymo/log-graph-breaks,4,64,7,1,1,0,,,False,https://api.github.com/repos/pytorch/pytorch/issues/146537
146535,[wip] disable decorator for ca,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #146535



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @yf225",open,2025-02-05T22:52:10Z,,,"module: dynamo, ciflow/inductor, module: compiled autograd",gh/xmfan/174/base,gh/xmfan/174/head,9,48,0,2,2,0,,,True,https://api.github.com/repos/pytorch/pytorch/issues/146535
146532,[symbolic shapes] Log SymNode id for provenance,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* (to be filled)



cc @ezyang @SherlockNoMad @EikanWang @jgong5 @wenzhe-nrv",open,2025-02-05T22:47:15Z,,,"release notes: fx, fx, ciflow/inductor",gh/angelayi/66/base,gh/angelayi/66/head,3,148,92,1,1,0,,,False,https://api.github.com/repos/pytorch/pytorch/issues/146532
146530,[aoti] Fix FakeTensorMode not detected in aot_compile when there's no input,"Summary:
Fixes https://github.com/pytorch/pytorch/issues/118304

When we don't have inputs, we should still try to get a FakeTensorMode because there can be unbacked symints in the graph.

So we get the FakeTensorMode once when entering compile_fx, and then using the that FakeTensorMode for the rest of the lowering.

Test Plan:
```
buck run fbcode//mode/dev-nosan //caffe2/test/inductor:test_aot_inductor -- -r unbacked_arg
```

Differential Revision: D69158049




cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @desertfire @chauhang @aakhundov",open,2025-02-05T22:25:56Z,,,"fb-exported, ciflow/trunk, module: inductor, ciflow/inductor, release notes: AO frontend",main,export-D69158049,3,60,12,1,8,0,,,True,https://api.github.com/repos/pytorch/pytorch/issues/146530
146526,[inductor] add units to estimated runtime log,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #146526
* #146513



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @desertfire @chauhang @aakhundov",open,2025-02-05T21:57:04Z,,,"module: inductor, ciflow/inductor",gh/xmfan/173/base,gh/xmfan/173/head,1,2,2,1,2,0,,,False,https://api.github.com/repos/pytorch/pytorch/issues/146526
146525,[dynamo] improved graph break messages for some common graph break sites,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #146525



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",open,2025-02-05T21:47:17Z,,,"module: dynamo, ciflow/inductor, release notes: dynamo, module: compile ux",gh/williamwen42/205/base,gh/williamwen42/205/head,14,517,76,2,1,0,"zou3519, zou3519, zou3519, zou3519, zou3519, zou3519, jansel, bobrenjc93","COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED",False,https://api.github.com/repos/pytorch/pytorch/issues/146525
146520,CUDA CachingHostAllocator tracks registrations to call correct free,"Users may change the allocator config at will. torch unit tests do this. However, allocations using cudaHostRegister should use corresonding cudaHostUnregister and similarly for cudaHostAlloc / cudaFreeHost.",open,2025-02-05T21:19:31Z,,,"triaged, open source, topic: not user facing",main,cuda_host_allocator_free,1,15,6,1,2,0,,,False,https://api.github.com/repos/pytorch/pytorch/issues/146520
146514,[DTensor][Test] Create a simple unit test for tensordot,"Fixes #ISSUE_NUMBER

The dims and shape of the tensors are from a specific Shampoo use case. We want to create a unit test for it to make sure there are no regressions for this.

cc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wconstab @d4l3k @c-p-i-o",open,2025-02-05T19:35:29Z,,,"oncall: distributed, Merged, Reverted, ciflow/trunk, topic: not user facing, merging, ci-no-td",main,tensordot,1,23,0,2,13,1,tianyu-l,APPROVED,False,https://api.github.com/repos/pytorch/pytorch/issues/146514
146513,[dynamo] check for incompatible configs,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #146513

internal: https://fb.workplace.com/groups/1075192433118967/permalink/1599802033991335/

Assuming flags don't change during compilation, we shouldn't allow incompatible configs to be set at torch.compile wrap time.

Not in this PR: For flags that need to change during compilation, we'd have to be strict about where they can be used in the compile lifecycle

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @desertfire @chauhang @aakhundov",open,2025-02-05T19:09:40Z,,,"Merged, Reverted, ciflow/trunk, module: inductor, module: dynamo, ciflow/inductor, release notes: dynamo, merging, ci-no-td",gh/xmfan/172/base,gh/xmfan/172/head,3,31,0,4,13,0,williamwen42,APPROVED,False,https://api.github.com/repos/pytorch/pytorch/issues/146513
146512,Fix `DispatchStub.cpp` compilation for gcc 14,"Otherwise I get the following error:

```bash

.../intel-xpu-backend-for-triton/pytorch/aten/src/ATen/native/DispatchStub.cpp:152:18: error: no matching function for call to ‘find(std::array<c10::DeviceType, 7>::const_iterator, std::array<c10::DeviceType, 7>::const_iterator, const c10::DeviceType&)’
  152 |     if (std::find(supported_devices.begin(), supported_devices.end(), device_type) == supported_devices.end()) {
      |         ~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
In file included from /usr/include/c++/14/bits/locale_facets.h:48,
                 from /usr/include/c++/14/bits/basic_ios.h:37,
                 from /usr/include/c++/14/ios:46,
                 from /usr/include/c++/14/ostream:40,
                 from .../intel-xpu-backend-for-triton/pytorch/c10/core/DeviceType.h:13,
                 from .../intel-xpu-backend-for-triton/pytorch/aten/src/ATen/native/DispatchStub.h:3,
                 from .../intel-xpu-backend-for-triton/pytorch/aten/src/ATen/native/DispatchStub.cpp:2:
/usr/include/c++/14/bits/streambuf_iterator.h:435:5: note: candidate: ‘template<class _CharT2> typename __gnu_cxx::__enable_if<std::__is_char<_CharT2>::__value, std::istreambuf_iterator<_CharT, std::char_traits<_CharT> > >::__type std::find(istreambuf_iterator<_CharT, char_traits<_CharT> >, istreambuf_iterator<_CharT, char_traits<_CharT> >, const _CharT2&)’
  435 |     find(istreambuf_iterator<_CharT> __first,
      |     ^~~~
/usr/include/c++/14/bits/streambuf_iterator.h:435:5: note:   template argument deduction/substitution failed:
.../intel-xpu-backend-for-triton/pytorch/aten/src/ATen/native/DispatchStub.cpp:152:18: note:   mismatched types ‘std::istreambuf_iterator<_CharT, std::char_traits<_CharT> >’ and ‘const std::array<c10::DeviceType, 7>::value_type*’ {aka ‘const c10::DeviceType*’}
  152 |     if (std::find(supported_devices.begin(), supported_devices.end(), device_type) == supported_devices.end()) {
      |         ~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
```
",open,2025-02-05T18:51:17Z,,,"triaged, open source, module: dispatch, topic: not user facing",main,patch-2,1,1,0,1,3,0,,,False,https://api.github.com/repos/pytorch/pytorch/issues/146512
146510,[ONNX] Bump torchlib opset to 22,"Fixes #ISSUE_NUMBER
",open,2025-02-05T18:47:25Z,,,"open source, release notes: onnx",main,justinchu/torchlib-opset22,30,20652,436,30,1,0,,,True,https://api.github.com/repos/pytorch/pytorch/issues/146510
146509,[BE][CI][Easy] bump `ruff` to 0.9.0: long statements in docstrings,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #145606
* #144546
* #144569
* #145148
* __->__ #146509



cc @ezyang @SherlockNoMad @EikanWang @jgong5 @wenzhe-nrv",open,2025-02-05T18:43:13Z,,,"open source, ciflow/trunk, release notes: onnx, topic: not user facing, fx",gh/XuehaiPan/240/base,gh/XuehaiPan/240/head,4,26,6,4,4,0,justinchuby,APPROVED,False,https://api.github.com/repos/pytorch/pytorch/issues/146509
146506,Support contextlib.ExitStack,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #146506
* #146505
* #146503
* #146502
* #146501
* #146500
* #146504
* #146499
* #146498
* #146497
* #146496
* #146493
* #146492
* #146491
* #146495



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",open,2025-02-05T17:22:12Z,,,"open source, module: dynamo, ciflow/inductor",gh/guilhermeleobas/107/base,gh/guilhermeleobas/107/head,2,621,9,5,2,0,guilhermeleobas,COMMENTED,True,https://api.github.com/repos/pytorch/pytorch/issues/146506
146505,Allow setting attribute to NestedUserFunctionVariable,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #146506
* __->__ #146505
* #146503
* #146502
* #146501
* #146500
* #146504
* #146499
* #146498
* #146497
* #146496
* #146493
* #146492
* #146491
* #146495



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",open,2025-02-05T17:22:05Z,,,"open source, module: dynamo, ciflow/inductor",gh/guilhermeleobas/106/base,gh/guilhermeleobas/106/head,2,21,1,5,2,0,anijain2305,COMMENTED,True,https://api.github.com/repos/pytorch/pytorch/issues/146505
146504,Introduce `UserDefinedExceptionClassVariable`,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #146506
* #146505
* #146503
* #146502
* #146501
* #146500
* __->__ #146504
* #146499
* #146498
* #146497
* #146496
* #146493
* #146492
* #146491
* #146495



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",open,2025-02-05T17:21:58Z,,,"open source, module: dynamo, ciflow/inductor",gh/guilhermeleobas/105/base,gh/guilhermeleobas/105/head,5,45,5,5,2,0,,,True,https://api.github.com/repos/pytorch/pytorch/issues/146504
146503,Create new dynamo ObservedExceptions at runtime,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #146506
* #146505
* __->__ #146503
* #146502
* #146501
* #146500
* #146504
* #146499
* #146498
* #146497
* #146496
* #146493
* #146492
* #146491
* #146495



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",open,2025-02-05T17:21:52Z,,,"open source, module: dynamo, ciflow/inductor",gh/guilhermeleobas/104/base,gh/guilhermeleobas/104/head,3,18,1,5,2,0,,,True,https://api.github.com/repos/pytorch/pytorch/issues/146503
146502,Correctly propagate exception to parent tx,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #146506
* #146505
* #146503
* __->__ #146502
* #146501
* #146500
* #146504
* #146499
* #146498
* #146497
* #146496
* #146493
* #146492
* #146491
* #146495



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",open,2025-02-05T17:21:46Z,,,"open source, module: dynamo, ciflow/inductor",gh/guilhermeleobas/103/base,gh/guilhermeleobas/103/head,2,92,3,5,2,0,anijain2305,APPROVED,True,https://api.github.com/repos/pytorch/pytorch/issues/146502
146501,Update CPython tests for ctx manager to use unittest,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #146506
* #146505
* #146503
* #146502
* __->__ #146501
* #146500
* #146504
* #146499
* #146498
* #146497
* #146496
* #146493
* #146492
* #146491
* #146495



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",open,2025-02-05T17:21:40Z,,,"open source, topic: not user facing, module: dynamo, ciflow/inductor",gh/guilhermeleobas/102/base,gh/guilhermeleobas/102/head,1,206,211,5,1,0,,,True,https://api.github.com/repos/pytorch/pytorch/issues/146501
146500,Allow trace through unittest,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #146506
* #146505
* #146503
* #146502
* #146501
* __->__ #146500
* #146504
* #146499
* #146498
* #146497
* #146496
* #146493
* #146492
* #146491
* #146495



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",open,2025-02-05T17:21:34Z,,,"open source, module: dynamo, ciflow/inductor, release notes: dynamo",gh/guilhermeleobas/101/base,gh/guilhermeleobas/101/head,6,623,14,5,1,0,,,True,https://api.github.com/repos/pytorch/pytorch/issues/146500
146499,Add `__context/cause/suppress_context/traceback__` to Exception,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #146506
* #146505
* #146503
* #146502
* #146501
* #146500
* #146504
* __->__ #146499
* #146498
* #146497
* #146496
* #146493
* #146492
* #146491
* #146495



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",open,2025-02-05T17:21:27Z,,,"open source, module: dynamo, ciflow/inductor, release notes: dynamo",gh/guilhermeleobas/100/base,gh/guilhermeleobas/100/head,5,361,13,5,1,0,"guilhermeleobas, guilhermeleobas","COMMENTED, COMMENTED",False,https://api.github.com/repos/pytorch/pytorch/issues/146499
146498,Add `sys.exc_info` and `sys.exception`,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #146506
* #146505
* #146503
* #146502
* #146501
* #146500
* #146504
* #146499
* __->__ #146498
* #146497
* #146496
* #146493
* #146492
* #146491
* #146495



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",open,2025-02-05T17:21:20Z,,,"open source, module: dynamo, ciflow/inductor, release notes: dynamo",gh/guilhermeleobas/99/base,gh/guilhermeleobas/99/head,3,162,0,5,1,1,"guilhermeleobas, anijain2305","COMMENTED, APPROVED",False,https://api.github.com/repos/pytorch/pytorch/issues/146498
146497,Propagate `AttributeError` to user code in user_defined.py,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #146506
* #146505
* #146503
* #146502
* #146501
* #146500
* #146504
* #146499
* #146498
* __->__ #146497
* #146496
* #146493
* #146492
* #146491
* #146495



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",open,2025-02-05T17:21:13Z,,,"open source, topic: not user facing, module: dynamo, ciflow/inductor",gh/guilhermeleobas/98/base,gh/guilhermeleobas/98/head,3,38,1,5,1,1,"anijain2305, anijain2305","COMMENTED, CHANGES_REQUESTED",False,https://api.github.com/repos/pytorch/pytorch/issues/146497
146496,Handle `is`/`is not`,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #146506
* #146505
* #146503
* #146502
* #146501
* #146500
* #146504
* #146499
* #146498
* #146497
* __->__ #146496
* #146493
* #146492
* #146491
* #146495



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",open,2025-02-05T17:21:06Z,,,"open source, module: dynamo, ciflow/inductor, release notes: dynamo",gh/guilhermeleobas/97/base,gh/guilhermeleobas/97/head,2,26,0,4,1,1,"anijain2305, anijain2305","COMMENTED, CHANGES_REQUESTED",False,https://api.github.com/repos/pytorch/pytorch/issues/146496
146495,Fix round(...) with constants,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #146506
* #146505
* #146503
* #146502
* #146501
* #146500
* #146504
* #146499
* #146498
* #146497
* #146496
* #146493
* #146492
* #146491
* __->__ #146495



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",open,2025-02-05T17:20:59Z,,,"open source, ciflow/trunk, module: dynamo, ciflow/inductor, release notes: dynamo, merging",gh/guilhermeleobas/96/base,gh/guilhermeleobas/96/head,2,10,1,4,6,0,anijain2305,APPROVED,False,https://api.github.com/repos/pytorch/pytorch/issues/146495
146494,Fix STOPITERATION_ERROR opcode,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #146506
* #146505
* #146504
* #146503
* #146502
* #146501
* #146500
* #146499
* #146498
* #146497
* #146496
* #146495
* __->__ #146494
* #146493
* #146492
* #146491



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",open,2025-02-05T17:20:54Z,,,"open source, module: dynamo, ciflow/inductor, release notes: dynamo",gh/guilhermeleobas/95/base,gh/guilhermeleobas/95/head,1,5,4,2,1,1,"anijain2305, anijain2305","COMMENTED, CHANGES_REQUESTED",False,https://api.github.com/repos/pytorch/pytorch/issues/146494
146493,Add `RAISE_VARARGS 0`,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #146506
* #146505
* #146503
* #146502
* #146501
* #146500
* #146504
* #146499
* #146498
* #146497
* #146496
* __->__ #146493
* #146492
* #146491
* #146495



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",open,2025-02-05T17:20:47Z,,,"open source, module: dynamo, ciflow/inductor, release notes: dynamo",gh/guilhermeleobas/94/base,gh/guilhermeleobas/94/head,2,27,1,4,1,0,"zou3519, guilhermeleobas","COMMENTED, COMMENTED",False,https://api.github.com/repos/pytorch/pytorch/issues/146493
146492,Add `WITH_EXCEPT_START` opcode,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #146506
* #146505
* #146503
* #146502
* #146501
* #146500
* #146504
* #146499
* #146498
* #146497
* #146496
* #146493
* __->__ #146492
* #146491
* #146495



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",open,2025-02-05T17:20:40Z,,,"open source, module: dynamo, ciflow/inductor, release notes: dynamo",gh/guilhermeleobas/93/base,gh/guilhermeleobas/93/head,2,46,0,4,1,1,"zou3519, anijain2305","COMMENTED, APPROVED",False,https://api.github.com/repos/pytorch/pytorch/issues/146492
146491,Add `make_dynamo_test`,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #146506
* #146505
* #146503
* #146502
* #146501
* #146500
* #146504
* #146499
* #146498
* #146497
* #146496
* #146493
* #146492
* __->__ #146491
* #146495



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",open,2025-02-05T17:20:34Z,,,"open source, topic: not user facing, module: dynamo, ciflow/inductor",gh/guilhermeleobas/92/base,gh/guilhermeleobas/92/head,1,45,0,4,1,0,"zou3519, anijain2305, Skylion007, zou3519","APPROVED, APPROVED, COMMENTED, COMMENTED",False,https://api.github.com/repos/pytorch/pytorch/issues/146491
146490,[export] Serialize special values of float into strings for json.,"Summary: Currently inf is serialized as Infinity in JSON which is not standard compliant. Instead we will tweak all special floating points into strings and handle them at json layer.

Test Plan:
see D69060784
CI

Differential Revision: D69186425


",open,2025-02-05T16:36:50Z,,,"fb-exported, ciflow/trunk, ciflow/inductor, release notes: export",main,export-D69186425,6,133,53,3,2,0,yiming0416,APPROVED,False,https://api.github.com/repos/pytorch/pytorch/issues/146490
146489,Update code_template.py re.compile() is directly applied to the regex…,"… string inside the class variable

re.compile() is directly applied to the regex string inside the class variable

Regular expressions are very expensive computationally. So, this avoids any redundant compilation.
Fixes #ISSUE_NUMBER
",open,2025-02-05T16:23:56Z,,,"open source, topic: not user facing",main,patch-1,1,3,2,1,2,0,mikaylagawarecki,APPROVED,False,https://api.github.com/repos/pytorch/pytorch/issues/146489
146485,Update quantile doc,"Fixes #146156
",open,2025-02-05T15:33:15Z,,,"triaged, open source, release notes: python_frontend",main,patch-1,1,3,3,1,2,0,,,False,https://api.github.com/repos/pytorch/pytorch/issues/146485
146482,Update addr doc,"Fixes https://github.com/pytorch/pytorch/issues/146399
",open,2025-02-05T13:29:04Z,,,topic: not user facing,main,albanD-patch-1,1,1,1,1,1,0,,,False,https://api.github.com/repos/pytorch/pytorch/issues/146482
146481,[WIP][Windows][Inductor] Enable Inductor UT on XPU Windows.,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #146481



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @desertfire @chauhang @aakhundov",open,2025-02-05T13:22:21Z,,,"open source, topic: not user facing, module: inductor, module: dynamo, ciflow/inductor, keep-going, ciflow/xpu",gh/etaf/96/base,gh/etaf/96/head,9,21,17,1,1,0,,,True,https://api.github.com/repos/pytorch/pytorch/issues/146481
146480,[Submodule]: Update KleidiAI submodule to v1.3.0,"Change-Id: I687255982c72ee7daca438a15b718f07298963cc


cc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @malfet @snadampal @milpuz01",open,2025-02-05T12:54:37Z,,,"module: cpu, open source, module: arm, ciflow/trunk, topic: not user facing, merging",main,kleidiai_submodule_update,1,1,1,1,9,1,"digantdesai, malfet","APPROVED, APPROVED",False,https://api.github.com/repos/pytorch/pytorch/issues/146480
146478,[c10d] Add hccl distributed backend to c10d data structures," # MOTIVATION
Intel Gaudi is an out-of-tree PyTorch accelerator having its own device /dispatch key ```hpu``` .
With this change we add entries for Gaudi's distributed backend ```hccl``` to the c10d Backend data structures.
This is to ensure that there is no naming conflict in case a new in-tree accelerator is introduced with the same backend name.


The Out-of-tree backends are registered calling https://github.com/pytorch/pytorch/blob/fd0cd6a08f706b7bb1dedb296217b6441e4fb9ff/torch/distributed/distributed_c10d.py#L302

Successful registration adds the backend name to the list : 
https://github.com/pytorch/pytorch/blob/fd0cd6a08f706b7bb1dedb296217b6441e4fb9ff/torch/distributed/distributed_c10d.py#L265

We are binding the process group creator constructs at run-time so if there are other distributed backend with the same device name they can safely add the device type to the dictionary 

https://github.com/pytorch/pytorch/blob/fd0cd6a08f706b7bb1dedb296217b6441e4fb9ff/torch/distributed/distributed_c10d.py#L274

And add another entry to the dictionary with the same backend name ( but different device name )
https://github.com/pytorch/pytorch/blob/fd0cd6a08f706b7bb1dedb296217b6441e4fb9ff/torch/distributed/distributed_c10d.py#L268

In addition the out-of-tree devices can utilize the ```backend_list``` to check for successful backend registration  eg: APIs like ```is_hccl_available```
 


cc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o",open,2025-02-05T12:10:24Z,,,"oncall: distributed, triaged, open source, release notes: distributed (c10d)",main,c10d_add_hccl_to_backends,1,19,8,1,3,0,,,False,https://api.github.com/repos/pytorch/pytorch/issues/146478
146477,Improve error handling when checking CUDA version in case nvcc is not found,"Fixes:
- https://github.com/pytorch/pytorch/issues/101138

**Description**
The PR enhances error handling in `_check_cuda_version` by verifying the existence of the `nvcc` executable before invoking `subprocess.check_output`. If `nvcc` is missing, a `FileNotFoundError` is raised with a clear message, guiding users to check their CUDA installation and path configuration.

**Testing**
Manually tested with and without `nvcc` present in the expected path.


cc @peterjc123 @mszhanyi @skyline75489 @nbcsm @iremyux @Blackhex",open,2025-02-05T12:09:22Z,,,"module: windows, triaged, open source, release notes: fx",main,pytorch-101138,1,4,1,1,1,0,,,False,https://api.github.com/repos/pytorch/pytorch/issues/146477
146476,[Feat]: Improve KleidiAI 4 bit kernel performance,"Description:
1. New thread blocking accelerates GEMVs

Perf improvements:
12% speedup in LLM prefill phase and upto 16% speedup in autoregressive phase


Change-Id: Ie574ff8459fdb75701ae366158b4e118c70694e4


cc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @malfet @snadampal @milpuz01",open,2025-02-05T11:50:00Z,,,"module: cpu, triaged, open source, module: arm, ciflow/trunk, topic: performance, release notes: intel, merging",main,kleidiai_threading_improvement,1,171,312,1,10,0,malfet,APPROVED,False,https://api.github.com/repos/pytorch/pytorch/issues/146476
146475,fix: replace stderr with stdout for download messages in hub.py,"This PR addresses an issue where download logs in `hub.py` are sent to `stderr` instead of `stdout`. Hence, when running models with workers, these messages are incorrectly categorized as errors, leading to confusion. ",open,2025-02-05T10:29:29Z,,,"triaged, open source, ciflow/trunk, topic: not user facing, merging",main,main,1,2,2,1,12,0,mikaylagawarecki,APPROVED,False,https://api.github.com/repos/pytorch/pytorch/issues/146475
146474,Fix torch.take_along_dim param type and default description,"## Changes

- Change type description to `LongTensor`, consistent with [`torch.take`](https://pytorch.org/docs/stable/generated/torch.take.html)
- Add `dim` param default value description

## Test Result

**Before**
![image](https://github.com/user-attachments/assets/720ce158-2bc1-48b5-a188-56fcc7188d96)

**After**
![image](https://github.com/user-attachments/assets/05fe20bd-9476-4b97-ac2b-9b161d6532a1)

",open,2025-02-05T10:01:46Z,,,"triaged, open source, ciflow/trunk, release notes: python_frontend, merging",main,opt/docs/take_along_dim,1,2,2,1,7,0,mikaylagawarecki,APPROVED,False,https://api.github.com/repos/pytorch/pytorch/issues/146474
146473,[export] Fix logger handler,"Differential Revision: D69169179


",open,2025-02-05T08:28:27Z,,,"fb-exported, release notes: export",main,export-D69169179,1,2,0,1,2,0,,,False,https://api.github.com/repos/pytorch/pytorch/issues/146473
146472,Refactoring pipeline parallelism test cases to be device agnostic [1/n],"In this series of PR we intend to refactor pipeline parallelism test cases to enable to be completely device agnostic.

These changes will include the following approaches to do the same :


- Allowing for multiple device types using instantiate_device_type_test
- Replacing calls to cuda stream with torch.get_device_module(device) wherever it applies

This should result in improvement in usability for all devices


For this PR we have shown support for the following devices:

- CPU (wherever applicable)
- CUDA
- HPU
- XPU

To add other device new users can simply append their device to the device list 

cc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o",open,2025-02-05T07:53:31Z,,,"oncall: distributed, triaged, open source, ciflow/trunk, topic: not user facing, merging, module: pipelining",main,AnantGulati_pipeline_refactoring,4,57,41,5,5,1,H-Huang,APPROVED,False,https://api.github.com/repos/pytorch/pytorch/issues/146472
146467,Fix an issue where functional collectives don't force fx stride on inputs when compiled,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #146467

Fixes https://github.com/pytorch/pytorch/issues/146416

Also added contiguity checks in the C++ functional collective ops to prevent striding issues introduced during compilation manifest as silent correctness issues.

cc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @desertfire @chauhang @aakhundov",open,2025-02-05T03:54:36Z,,,"oncall: distributed, ciflow/trunk, release notes: distributed (c10d), module: inductor, ciflow/inductor, merging",gh/yifuwang/193/base,gh/yifuwang/193/head,4,105,27,5,8,1,"Chillee, lw, shunting314","APPROVED, APPROVED, APPROVED",False,https://api.github.com/repos/pytorch/pytorch/issues/146467
146466,Fix one_hot inconsistent errors after compile,"Fixes #146274

**Test Result**

```python
>>> import torch
>>> f = torch.nn.functional.one_hot
>>> a = torch.arange(0, 5) % 3  # [0,1,2,0,1]
>>> num_classes = 0
>>> torch.nn.functional.one_hot(a,num_classes)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
RuntimeError: Class values must be smaller than num_classes.

>>> torch.compile(torch.nn.functional.one_hot)(a,num_classes)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/zong/code/pytorch/torch/_dynamo/eval_frame.py"", line 570, in _fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File ""/home/zong/code/pytorch/torch/_dynamo/external_utils.py"", line 48, in inner
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
RuntimeError: Class values must be smaller than num_classes.

```

cc @bdhirsh",open,2025-02-05T02:47:27Z,,,"triaged, open source, topic: not user facing",main,fix/aten/one_hot,1,18,9,1,2,1,zou3519,COMMENTED,False,https://api.github.com/repos/pytorch/pytorch/issues/146466
146464,[symbolic shapes] Log id for each SymNode,"Fixes #ISSUE_NUMBER


cc @ezyang @SherlockNoMad @EikanWang @jgong5 @wenzhe-nrv",open,2025-02-05T01:32:37Z,,,"release notes: fx, fx, ciflow/inductor",main,angelayi/provenance_id,3,148,92,1,1,0,,,True,https://api.github.com/repos/pytorch/pytorch/issues/146464
146456,Fix workarea compute in lapackSyevd,"work-query APIs return floating point values, that could loose precision when converted back to int. Solve this by using `nextafter` and `ceil`
Add regression test 

Fixes #145801
",open,2025-02-05T00:24:37Z,,,"ciflow/trunk, release notes: linalg_frontend, merging",main,wdvr/iss_145801,2,14,2,5,4,0,malfet,APPROVED,False,https://api.github.com/repos/pytorch/pytorch/issues/146456
146455,[logging] Save compile state in CompiledFxGraph and make it available at runtime,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #146455

Summary: To support logging the correct compile_id for runtime timings (like Triton autotuning), save the compile_id in CompiledFxGraph make it available to logging utilities, i.e., dynamo_timed.

The previous attempt put the compile_id in the inductor_metadata with the Triton output code, but that broke Triton caching and we reverted. This version does the following:
* When creating or deserializing a CompiledFxGraph, save the compile-time compile_id.
* Implement a class `RuntimeCompileContext` that's analogous to `CompileContext` where we can look up the compile_id at runtime.
* Set this runtime compile context during `CompiledFxGraph.__call__`.
* Removes the compile_id as a param to dynamo_timed; dynamo_timed can figure it out instead.
* Removes separate dynamo_timed params for compile-time and runtime dynamo_compile column names. We can use one param have dynamo_timed figure out whether to treat as a runtime or compile-time event.

Test Plan:
* tlparse (`python benchmarks/dynamo/torchbench.py --performance --training --amp --backend inductor --device cuda --print-compilation-time --repeat 5 --cold-start-latency --only nanogpt`): https://fburl.com/bu5i8efk
* dynamo_compile: https://fburl.com/scuba/dynamo_compile/sandbox/3d74ps92
* pt2_compile_events: https://fburl.com/scuba/pt2_compile_events/ooqoe5tu

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @desertfire @chauhang @aakhundov",open,2025-02-05T00:23:56Z,,,"topic: not user facing, module: inductor, module: dynamo, ciflow/inductor",gh/masnesral/176/base,gh/masnesral/176/head,7,113,74,5,2,0,"masnesral, masnesral, masnesral, masnesral, masnesral, masnesral, masnesral, masnesral, masnesral, masnesral, xmfan, masnesral, masnesral, xmfan, xmfan, masnesral, xmfan, xmfan, masnesral","COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED",False,https://api.github.com/repos/pytorch/pytorch/issues/146455
146454,[dynamo][fullgraph] Raise NoGraphError if no graph with fullgraph=True,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #146454
* #146507



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",open,2025-02-04T23:50:47Z,,,"ciflow/trunk, topic: not user facing, module: dynamo, ciflow/inductor",gh/anijain2305/669/base,gh/anijain2305/669/head,3,77,0,4,1,0,,,True,https://api.github.com/repos/pytorch/pytorch/issues/146454
146452,cpp_wrapper: enable all CI inductor tests,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #146452
* #146706
* #146424
* #146109
* #146449
* #144349
* #144293
* #144002

With the speedups from precompiled headers, we can now enable all currently enabled CI tests for inductor in cpp_wrapper mode.",open,2025-02-04T22:58:10Z,,,"open source, topic: not user facing, ciflow/inductor, keep-going, ci-no-test-timeout",gh/benjaminglass1/66/base,gh/benjaminglass1/66/head,1,25,45,6,1,0,,,True,https://api.github.com/repos/pytorch/pytorch/issues/146452
146449,cpp_wrapper: handle mixed-device C-shim fallbacks,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #146452
* #146706
* #146424
* #146109
* __->__ #146449
* #144349
* #144293
* #144002

Fixes an error from test_torch, where a CUDA cpp_wrapper run called a CUDA native C-shim kernel with two CPU tensors.

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @desertfire @chauhang @aakhundov",open,2025-02-04T22:07:25Z,,,"open source, topic: not user facing, module: inductor, ciflow/inductor",gh/benjaminglass1/65/base,gh/benjaminglass1/65/head,5,90,42,4,1,0,,,False,https://api.github.com/repos/pytorch/pytorch/issues/146449
146448,[ROCm] Indexing perf optimization via Unroll/WideFetch/IdxReuse/OneDupOpt,"Fixes #ISSUE_NUMBER


cc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd",open,2025-02-04T21:56:59Z,,,"module: rocm, triaged, open source, release notes: cuda, ciflow/rocm",main,main,1,49,0,1,2,0,,,False,https://api.github.com/repos/pytorch/pytorch/issues/146448
146443,Stop poisoning fork on Dataloader creation when pin_memory is enabled,"Fixes https://github.com/pytorch/pytorch/issues/144687
Needs https://github.com/pytorch/pytorch/pull/146098 that already landed to fix the issue above

A longer-term fix would be to move cuda's non-poisoning is_available() check to c++. But that would be quite a bit of work.

This PR also updates the behavior of current_accelerator() in python to match getAccelerator() in C++ and update all docs to reflect that.",open,2025-02-04T20:54:13Z,,,"release notes: dataloader, topic: bug fixes",main,fix_dataloader,6,66,26,5,1,1,"ngimel, guangyey, guangyey, guangyey, guangyey, andrewkho","APPROVED, COMMENTED, APPROVED, COMMENTED, COMMENTED, APPROVED",False,https://api.github.com/repos/pytorch/pytorch/issues/146443
146440,[sigmoid] Implement a OSS only model runner.,"Summary: Implement an oss version of modelrunner with clean dependencies. The new oss model runner only removes thrift and only use json header to load the model.

Test Plan: Test will be added in the next diff separately. (D69060784)

Differential Revision: D68846877


",open,2025-02-04T19:38:20Z,,,"fb-exported, ciflow/trunk, ciflow/inductor, release notes: export",main,export-D68846877,1,5,4,1,5,0,SherlockNoMad,APPROVED,False,https://api.github.com/repos/pytorch/pytorch/issues/146440
146436,[Testing] Reduce `test_exp` flakiness,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #146436

By setting `reference_in_float` to false,  as `exp(a + b)` could yield significantly different results than `exp(a.half()+b.half())` as one can see in the following example (which is accidentally the random values generated by MacOS RNG for this test)

```
>>> import torch
>>> x=torch.tensor(2.5599, dtype=torch.half)
>>> y=torch.tensor(0.6970, dtype=torch.half)
>>> (x + y).exp()
tensor(26., dtype=torch.float16)
>>> (x.float() + y.float()).exp()
tensor(25.9799)
```

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @desertfire @chauhang @aakhundov",open,2025-02-04T18:54:11Z,,,"Merged, Reverted, topic: not user facing, ciflow/mps, module: inductor, ciflow/inductor, ci-no-td",gh/malfet/169/base,gh/malfet/169/head,2,9,2,2,6,0,"dcci, malfet, dcci, jansel","COMMENTED, COMMENTED, APPROVED, APPROVED",False,https://api.github.com/repos/pytorch/pytorch/issues/146436
146427,add the `torch.float8_e8m0fnu` dtype to PyTorch,"Summary:

Adds the `torch.float8_e8m0fnu` dtype to PyTorch, as detailed in
https://github.com/pytorch/pytorch/issues/146414 . Please see the issue for a detailed definition of the format.  Example of basic functionality:

```python
import torch

# round trip
x0 = torch.randn(4, 4, dtype=torch.float32)
x1 = x0.to(torch.float8_e8m0fnu)  # RNE rounding
x2 = x1.to(torch.float32)  # 2 ** exponent

# creation with empty
x0 = torch.empty(4, 4, dtype=torch.float8_e8m0fnu)

# printing
print(x0)
```

Done in this PR:
* numerical correctness
* op coverage (except for `torch._scaled_mm`): create tensor, cast to/from float32
* printing a tensor works

For future PRs:
* performance optimizations for casting
* torch._scaled_mm
* PT2
* various cleanups (detailed in comments with issue numbers)

Test Plan:

```
pytest test/quantization/core/experimental/test_float8.py -s
```

Reviewers:

Subscribers:

Tasks:

Tags:

cc @yanbing-j @albanD @kadeng @penguinwu @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10",open,2025-02-04T18:07:11Z,,,"module: cpu, release notes: quantization, module: float8",main,gh/vkuzo/1/head,23,508,43,8,2,0,"vkuzo, vkuzo, vkuzo, vkuzo, vkuzo, vkuzo, vkuzo, vkuzo, vkuzo, vkuzo, vkuzo, drisspg, drisspg, drisspg, drisspg, vkuzo, vkuzo, drisspg, drisspg, vkuzo, vkuzo, eqy, vkuzo, drisspg","COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED",False,https://api.github.com/repos/pytorch/pytorch/issues/146427
146426,Test typing of arithmetic operators on Tensor (see #145838),"See #145838

Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #146426

",open,2025-02-04T18:06:50Z,,,"open source, ciflow/trunk, topic: not user facing, merging",gh/rec/131/base,gh/rec/131/head,2,462,0,5,10,0,"Skylion007, Skylion007, rec, rec","COMMENTED, APPROVED, COMMENTED, COMMENTED",False,https://api.github.com/repos/pytorch/pytorch/issues/146426
146424,cpp_wrapper: fix test_torchinductor* tests,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #146452
* #146706
* __->__ #146424
* #146109
* #146449
* #144349
* #144293
* #144002



cc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @voznesenskym @penguinwu @EikanWang @Guobing-Chen @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @desertfire @chauhang @aakhundov",open,2025-02-04T18:04:13Z,,,"module: cpu, open source, topic: not user facing, module: inductor, ciflow/inductor",gh/benjaminglass1/64/base,gh/benjaminglass1/64/head,3,22,7,5,1,0,,,False,https://api.github.com/repos/pytorch/pytorch/issues/146424
146421,experimental specialization logging,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #146421



cc @ezyang @SherlockNoMad @EikanWang @jgong5 @wenzhe-nrv

Differential Revision: [D69163120](https://our.internmc.facebook.com/intern/diff/D69163120)",open,2025-02-04T17:34:46Z,,,"ciflow/trunk, release notes: fx, fx, ciflow/inductor",gh/bobrenjc93/270/base,gh/bobrenjc93/270/head,1,37,1,6,2,0,,,True,https://api.github.com/repos/pytorch/pytorch/issues/146421
146420,[ROCm] Enable tunable warp size for stride one indexing backwards kernel,"Enable tunable warp size for stride one indexing backwards kernel. This will allow for the indexing backward kernel with stride one to work on smaller warp sizes.

cc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd",open,2025-02-04T17:24:38Z,,,"module: rocm, triaged, open source, topic: not user facing, ciflow/periodic, rocm, ciflow/rocm",main,improve-backwards-indexing-with-stride-1,1,74,0,1,1,0,,,False,https://api.github.com/repos/pytorch/pytorch/issues/146420
146418,[BE]: Add TypeVarTuple to RNN Args for better type inference,"Fixes #ISSUE_NUMBER
",open,2025-02-04T16:46:07Z,,,open source,main,skylion007/typevartuple-nn-rnn-2025-02-04,1,5,4,1,2,0,,,True,https://api.github.com/repos/pytorch/pytorch/issues/146418
146417,"Only call triton in worker process, kick off worker processes earlier, during inductor codegen","Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #146417

### Big idea
This PR extends https://github.com/pytorch/pytorch/pull/144288 by combining calling triton in worker processes with the future cache: we kick off triton compilation in the worker processes earlier, during inductor codegen. Basically instead of calling async_compile.triton for the first time only after the entire code has been generated, we start compiling as soon as we know we'll need to compile the kernel. Then, when loading the generated inductor code, we can simply read from our in memory future cache, considerably increasing the parallelism.
### Implementation Overview
In total, the diff does the following:
- Converts TritonFuture to LambdaFuture, only calling triton.compile on worker processes
- Now that triton.compile() isn't called on the main process, we call TritonBundler on all compiled kernels when we get them back from workers
- Extend @eellison's future cache to a class, mostly as a refactor
- Finally, call async_compile.triton ahead of time in Scheduler.codegen if workers are warmed up. This causes the subsequent
async_compile.triton call that occurs after codegen to cache hit on cold start.
In the diffs after this, I will add more to CompiledTritonKernels so that TritonBundler, on a warm start, automatically populates the in memory cache on warm start with the existing triton kernels, avoiding calling triton altogether on warm starts.
Because LambdaFutures are much faster to kick off than TritonFutures, due to not needing to load from TritonCodeCache at all, the time spent kicking off these worker jobs is pretty minimal for inductor codegen.

Differential Revision: [D69123174](https://our.internmc.facebook.com/intern/diff/D69123174/)

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @desertfire @chauhang @aakhundov",open,2025-02-04T16:20:19Z,,,"fb-exported, ciflow/trunk, topic: not user facing, module: inductor, ciflow/inductor",gh/jamesjwu/106/base,gh/jamesjwu/106/head,9,250,77,16,17,1,"jamesjwu, jamesjwu, masnesral, jamesjwu, jansel","COMMENTED, COMMENTED, COMMENTED, COMMENTED, CHANGES_REQUESTED",False,https://api.github.com/repos/pytorch/pytorch/issues/146417
146415,"Only call triton in worker process, ahead of time compile","Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #146415

# Big idea
This PR extends https://github.com/pytorch/pytorch/pull/144288 by combining calling triton in worker processes with the future cache: we kick off triton compilation in the worker processes earlier, during inductor codegen. Basically instead of calling async_compile.triton for the first time only after the entire code has been generated, we start compiling as soon as we know we'll need to compile the kernel. Then, when loading the generated inductor code, we can simply read from our in memory future cache, considerably increasing the parallelism.
# Implementation Overview
In total, the diff does the following:
- Converts TritonFuture to LambdaFuture, only calling triton.compile on worker processes
- Now that triton.compile() isn't called on the main process, we call TritonBundler on all compiled kernels when we get them back from workers
- Extend @eellison's future cache to a class, mostly as a refactor
- Finally, call async_compile.triton ahead of time in Scheduler.codegen if workers are warmed up. This causes the subsequent
async_compile.triton call that occurs after codegen to cache hit on cold start.
In the diffs after this, I will add more to CompiledTritonKernels so that TritonBundler, on a warm start, automatically populates the in memory cache on warm start with the existing triton kernels, avoiding calling triton altogether on warm starts.
Because LambdaFutures are much faster to kick off than TritonFutures, due to not needing to load from TritonCodeCache at all, the time spent kicking off these worker jobs is pretty minimal for inductor codegen.

### Can we split the diff for easier review?
It's best if this diff lands atomically with all of these changes, as doing the ahead of time codegen compile is only performant if we replace TritonFuture with LambdaFuture(as we don't need to load the triton kernel on the main process). However, I've made a diff stack for easier reviewing here:
- D69070048 - Run async_compile.triton ahead of time in Scheduler.codegen
- D68633454 - Only call triton in worker process

Differential Revision: [D69070616](https://our.internmc.facebook.com/intern/diff/D69070616/)

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @desertfire @chauhang @aakhundov",open,2025-02-04T16:14:20Z,,,"fb-exported, module: inductor, ciflow/inductor",gh/jamesjwu/105/base,gh/jamesjwu/105/head,6,111,63,1,3,0,,,True,https://api.github.com/repos/pytorch/pytorch/issues/146415
146407,[ROCm] Unskip std:bad_alloc failures,"Flakey MI300 issue related to memory usage should now be resolved after https://github.com/pytorch/pytorch/actions/runs/13007160888?pr=145829.

cc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @hongxiayang @naromero77amd @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @desertfire @chauhang @aakhundov",open,2025-02-04T14:14:17Z,,,"module: rocm, triaged, open source, ciflow/trunk, topic: not user facing, module: inductor, ciflow/inductor, merging, ciflow/rocm",main,unskip-bad-alloc,1,0,7,1,4,0,jeffdaily,APPROVED,False,https://api.github.com/repos/pytorch/pytorch/issues/146407
146406,Only enable aotriton on x86_64 and aarch64,"Make `USE_FLASH_ATTENTION` and `USE_MEM_EFF_ATTENTION` depend on `CPU_INTEL OR CPU_AARCH64`.

[aotriton pre-built](https://github.com/ROCm/aotriton/releases) is only available on x86_64.

Although `AOTRITON_INSTALL_FROM_SOURCE` can be specified to build from source, building aotriton requires CUDA, so on architectures without CUDA support (like riscv64), it still needs to be disabled.",open,2025-02-04T13:33:47Z,,,"triaged, open source, topic: not user facing",main,patch-1,1,3,2,1,4,0,,,False,https://api.github.com/repos/pytorch/pytorch/issues/146406
146403,Use std::string_view,"Fixes #ISSUE_NUMBER
",open,2025-02-04T12:59:16Z,,,"open source, topic: not user facing",main,string_view_gen2,2,2,1,1,1,0,,,True,https://api.github.com/repos/pytorch/pytorch/issues/146403
146395,[dynamo][builtin-skipfile-cleanup] Remove random,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #146395
* #146339
* #146116
* #146322



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",open,2025-02-04T05:35:35Z,,,"module: dynamo, ciflow/inductor, keep-going",gh/anijain2305/668/base,gh/anijain2305/668/head,1,0,2,1,2,0,,,False,https://api.github.com/repos/pytorch/pytorch/issues/146395
146393,PEP585: More fixes 2,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #146393
* #146392
* #146391



cc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o @EikanWang @jgong5 @wenzhe-nrv @sanchitintel @ezyang @SherlockNoMad",open,2025-02-04T04:20:02Z,,,"oncall: distributed, oncall: jit, release notes: quantization, fx, ciflow/inductor, release notes: AO frontend",gh/aorenste/217/base,gh/aorenste/217/head,30,62,76,1,1,0,,,True,https://api.github.com/repos/pytorch/pytorch/issues/146393
146392,PEP585: More fixes,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #146393
* __->__ #146392
* #146391

",open,2025-02-04T04:19:56Z,,,"release notes: onnx, module: inductor, module: dynamo, ciflow/inductor",gh/aorenste/216/base,gh/aorenste/216/head,30,108,147,1,1,0,,,True,https://api.github.com/repos/pytorch/pytorch/issues/146392
146391,PEP585: Add noqa to necessary tests,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #146391

",open,2025-02-04T04:19:51Z,,,topic: not user facing,gh/aorenste/215/base,gh/aorenste/215/head,7,63,31,7,1,1,"justinchuby, justinchuby, albanD, aorenste","COMMENTED, APPROVED, COMMENTED, COMMENTED",False,https://api.github.com/repos/pytorch/pytorch/issues/146391
146388,[WIP][CUDA][cuDNN] Experimental `cudnn_rms_norm`,"opt-in for now behind two new native functions---the plan would be to eventually add it as the `CUDA:` backend to `rms_norm`

Initial experiments show forward ~4-5x speed, up fwd+bwd ~3x speedup

cc @csarofeen @ptrblck @xwang233 @msaroufim",open,2025-02-04T02:55:56Z,,,"module: cudnn, module: cuda, open source, module: norms and normalization, topic: not user facing",main,cudnnrmsforward,5,422,0,4,5,0,"albanD, eqy","COMMENTED, COMMENTED",False,https://api.github.com/repos/pytorch/pytorch/issues/146388
146387,AMD: reverting to default config for better performance.,TopK performance on ROCm performs better on the test suite with the default config.,open,2025-02-04T02:49:19Z,,,"open source, release notes: cuda",main,topk_rocm_tune,1,0,9,1,2,1,malfet,APPROVED,True,https://api.github.com/repos/pytorch/pytorch/issues/146387
146385,[WIP] Confirm XPU Regression,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #146385

",open,2025-02-04T02:32:40Z,,,"triaged, open source, topic: not user facing, ciflow/xpu",gh/EikanWang/74/base,gh/EikanWang/74/head,1,1,1,1,1,0,,,True,https://api.github.com/repos/pytorch/pytorch/issues/146385
146372,[Submodule] Turning flash-attention integration into 3rd party submod (#144120),"Summary:

# Summary

### Sticky points

Cuda-graph rng handling has changed / deviated from original implementation. We will be left with a dangling 'offset' val and confusing naming due to BC

## Dependencies
- Flash PR: https://github.com/Dao-AILab/flash-attention/pull/1419

### Other Points
- The BC linter is complaining about losing generate.py and its functions which is not real BC surface
cc albanD

imported-using-ghimport

Test Plan:
Imported from OSS

Building in dev
`buck build @//mode/dev-nosan -c fbcode.nvcc_arch=h100a  //caffe2:ATen-cu --show-full-output    `

I and Nming the .so I do see that the flash symbols are correctly named:
```
0000000001c3dfb0 t pytorch_flash::run_mha_bwd(pytorch_flash::Flash_bwd_params&, CUstream_st*)::$_0::operator()() const::{lambda()#1}::operator()() const::{lambda()#1}::operator()() const::{lambda()#7}::operator()() const
0000000001c36080 t pytorch_flash::run_mha_fwd(pytorch_flash::Flash_fwd_params&, CUstream_st*, bool)::$_0::operator()() const::{lambda()#2}::operator()() const::{lambda()#1}::operator()() const::{lambda()#6}::operator()() const
0000000001c360e0 t pytorch_flash::run_mha_fwd(pytorch_flash::Flash_fwd_params&, CUstream_st*, bool)::$_0::operator()() const::{lambda()#2}::operator()() const::{lambda()#1}::operator()() const::{lambda()#7}::operator()() const
0000000001c35fc0 t pytorch_flash::run_mha_fwd(pytorch_flash::Flash_fwd_params&, CUstream_st*, bool)::$_0::operator()() const::{lambda()#1}::operator()() const::{lambda()#1}::operator()() const::{lambda()#6}::operator()() const
0000000001c36020 t pytorch_flash::run_mha_fwd(pytorch_flash::Flash_fwd_params&, CUstream_st*, bool)::$_0::operator()() const::{lambda()#1}::operator()() const::{lambda()#1}::operator()() const::{lambda()#7}::operator()() const
```

Reviewed By: vkuzo

Differential Revision: D68502879

Pulled By: drisspg
",open,2025-02-04T00:34:54Z,,,"fb-exported, ciflow/trunk, topic: not user facing, ciflow/inductor, module: sdpa",main,export-D68502879,30,121,4214,1,9,0,,,False,https://api.github.com/repos/pytorch/pytorch/issues/146372
146367,[dynamo][EXPERIMENT] Prototype for `mark_traceable`,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #146367
* #146714
* #146713



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",open,2025-02-04T00:05:53Z,,,"module: dynamo, ciflow/inductor",gh/StrongerXi/81/base,gh/StrongerXi/81/head,8,366,30,2,2,1,"StrongerXi, zou3519, StrongerXi, StrongerXi, StrongerXi, StrongerXi, zou3519","COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED",False,https://api.github.com/repos/pytorch/pytorch/issues/146367
146364,[DeviceMesh] Add some documentation for `from_group` API and add a 2D test,"Fixes #ISSUE_NUMBER


cc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wconstab @d4l3k @c-p-i-o @tianyu-l @XilunWu",open,2025-02-03T22:54:40Z,,,"oncall: distributed, module: dtensor, release notes: distributed (dtensor)",main,add_from_group_doc_and_test,2,102,11,1,1,2,"fduwjj, fduwjj, wz337, fegin, wz337","COMMENTED, APPROVED, COMMENTED, COMMENTED, COMMENTED",False,https://api.github.com/repos/pytorch/pytorch/issues/146364
146356,[cutlass backend] fix bug for accuminator dtype,"Will add unit tests for accuracy. 

Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #146743
* __->__ #146356



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @desertfire @chauhang @aakhundov",open,2025-02-03T22:03:13Z,,,"Merged, Reverted, ciflow/trunk, topic: not user facing, module: inductor, ciflow/inductor, ci-no-td",gh/henrylhtsang/2/base,gh/henrylhtsang/2/head,2,7,74,5,10,0,Chillee,APPROVED,False,https://api.github.com/repos/pytorch/pytorch/issues/146356
146355,[dynamo] replace hardcoded eval frame control flags skip_code_recursive_flag/cache_limit_hit_flag,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #146355
* #145603

This PR and the previous:
- Moves parts of `eval_frame.c` to C++.
- Reduces code duplication in `dynamo__custom_eval_frame` and makes the control flow more clear.
- Enables `convert_frame` to signal to `eval_frame.cpp` in a general manner how to evaluate this frame, recursive frames, and future frames with the same code object (default/compile, skip, run-only). e.g. this will allow us to change skipping/cache limit hit eval_frame behavior directly from convert_frame without requiring changes to C/C++.

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",open,2025-02-03T22:01:46Z,,,"topic: not user facing, module: dynamo, ciflow/inductor",gh/williamwen42/204/base,gh/williamwen42/204/head,9,215,172,5,1,0,"williamwen42, jansel, anijain2305, williamwen42, jansel","COMMENTED, CHANGES_REQUESTED, COMMENTED, COMMENTED, COMMENTED",False,https://api.github.com/repos/pytorch/pytorch/issues/146355
146352,Build a storage reader/writer to write checkpoints in HF format,"Summary: Title - we want to write checkpoints in HF format with DCP, this diff allows this for the non-distributed use case.

Test Plan:
buck2 test 'fbcode//mode/dev-nosan' fbcode//caffe2/test/distributed/checkpoint:test_hf_torchtune_storage

N6476188 --> able to save and load tensor in hf format

Differential Revision: D68444967




cc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o @LucasLLC @MeetVadakkanchery @mhorowitz @pradeepfn @ekr0",open,2025-02-03T21:20:40Z,,,"oncall: distributed, fb-exported, ciflow/trunk, module: distributed_checkpoint",main,export-D68444967,4,316,5,1,6,0,"saumishr, ankitageorge, ankitageorge, ankitageorge, ankitageorge, ankitageorge, fegin, ankitageorge, ankitageorge","COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED",False,https://api.github.com/repos/pytorch/pytorch/issues/146352
146341,Only call triton in worker process; run async_compile.triton ahead of time in Scheduler.codegen,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* (to be filled)

### Big idea
This PR extends https://github.com/pytorch/pytorch/pull/144288 by combining calling triton in worker processes with the future cache: we kick off triton compilation in the worker processes earlier, during inductor codegen. Basically instead of calling async_compile.triton for the first time only after the entire code has been generated, we start compiling as soon as we know we'll need to compile the kernel. Then, when loading the generated inductor code, we can simply read from our in memory future cache, considerably increasing the parallelism.

### Implementation Overview
In total, the diff does the following:
- Converts TritonFuture to LambdaFuture, only calling triton.compile on worker processes
- Now that triton.compile() isn't called on the main process, we call TritonBundler on all compiled kernels when we get them back from workers
- Extend @eellison's future cache to a class, mostly as a refactor
- Finally, call async_compile.triton ahead of time in Scheduler.codegen if workers are warmed up. This causes the subsequent async_compile.triton call that occurs after codegen to cache hit on cold start.

In the diffs after this, I will add more to CompiledTritonKernels so that TritonBundler, on a warm start, automatically populates the in memory cache on warm start with the existing triton kernels, avoiding calling triton altogether on warm starts.

Because LambdaFutures are much faster to kick off than TritonFutures, due to not needing to load from TritonCodeCache at all, the time spent kicking off these worker jobs is pretty minimal for inductor codegen.

### Can we split the diff for easier review?
It's best if this diff lands atomically with all of these changes, as doing the ahead of time codegen compile is only performant if we replace TritonFuture with LambdaFuture(as we don't need to load the triton kernel on the main process). However, I've made a diff stack for easier reviewing here:
D69070048 - Run async_compile.triton ahead of time in Scheduler.codegen
D68633454 - Only call triton in worker process

Differential Revision: [D69013710](https://our.internmc.facebook.com/intern/diff/D69013710/)

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @desertfire @chauhang @aakhundov",open,2025-02-03T20:46:41Z,,,"module: inductor, ciflow/inductor",gh/jamesjwu/102/base,gh/jamesjwu/102/head,6,108,71,1,2,0,,,True,https://api.github.com/repos/pytorch/pytorch/issues/146341
146335,[dynamic shapes][WIP] mark backed size symbols as size-like,"experimental, to apply upper-bound / maxsize size-oblivious semantics to backed symbols


cc @ezyang @SherlockNoMad @EikanWang @jgong5 @wenzhe-nrv @voznesenskym @penguinwu @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @jiayisunx @chenyang78 @kadeng @chauhang @amjames",open,2025-02-03T19:56:25Z,,,"ciflow/trunk, release notes: fx, fx, module: dynamo, ciflow/inductor",main,pianpwk/treat_sizes_as_size_like,5,31,8,7,2,0,,,True,https://api.github.com/repos/pytorch/pytorch/issues/146335
146333,Add optional generator to distribution sampler/rsample methods.,"Fixes part of #45115 and #11340
Adds a generator parameter to all the sample/rsample methods of torch distribution classes

cc @fritzo @neerajprad @alicanb @nikitaved",open,2025-02-03T19:47:59Z,,,"module: distributions, triaged, open source, topic: not user facing",main,features/distribution_generator,30,160,138,1,4,0,,,False,https://api.github.com/repos/pytorch/pytorch/issues/146333
146324,[torch][amdsmi] Avoid ODR violation when loading amdsmi,"Summary:
amdsmi bundles its own copy of `libamd_smi.so`. When you're interacting with `amdsmi` from *only* python that's fine, but when you try to interact with `libamd_smi.so` from native code too this poses a problem, because from native code you'll be linking against the copy of `libamd_smi.so` from the SDK.

This means you'll end up with 2 copies of `libamd_smi.so` in your process, and potentially (Murphey's law says you will, as does our CI) violate ODR.

In order to avoid this issue from the PT side of the world we can hook the `dlopen(""path/to/bundled/libamd_smi.so"")` and try to use the already loaded/SDK version of `libamd_smi.so` first, before proceeding to use the `path/to/bundled/libamd_smi.so`.

Test Plan: CI, inspect process using libamd_smi.so from native + python and observe only a single copy loaded

Differential Revision: D69064038


",open,2025-02-03T18:59:18Z,,,"fb-exported, ciflow/trunk, topic: not user facing",main,export-D69064038,1,41,1,1,14,1,"malfet, danzimm, malfet, malfet","COMMENTED, COMMENTED, APPROVED, COMMENTED",False,https://api.github.com/repos/pytorch/pytorch/issues/146324
146321,[ONNX] Support custom axis name through dynamic_shapes,"Fixes #143443

This PR aims to support custom dynamic axis naming through dynamic_shapes. Currently, _Dim and _DimHint do not support dynamic axis naming (#144273).

1. **the original dynamic shapes guarantee**
The axis renaming is only applied when dynamic shapes include string instead of all _Dim and _DimHint. Thus, there will not be any inconsistent behavior to dynamic_shapes with torch.export.export if the given dynamic shapes follow torch.export.export format.
2. _DimHint.AUTO is applied to the axes that are specified with custom names to avoid exporter crash. (_DimHint.DYNAMIC crashes when the export fails.)
3.  There's no need to handle cases where kwargs are out of order with the model signature,
    as torch.export.export supports dynamism only when kwargs and dynamic_shapes are provided in order.
    https://github.com/pytorch/pytorch/blob/49082f9dba3b79a344cb03652972ddbe7c3729cc/torch/export/_trace.py#L2034
4. If `torch.onnx.ExportedProgram` finds the axes share the same constraints, they will have the same name (e.g. s0, s1, ...). Therefore, even if the ONNX users specify them with different custom names, they won't be respected.

Example model:
```python
        class NestedModel(torch.nn.Module):
            def forward(
                self,
                x: torch.Tensor,
                ys: list[torch.Tensor],
                zs: dict[str, torch.Tensor],
                c: torch.Tensor,
            ):
                y = ys[0] + ys[1] + zs[""a""] + zs[""b""]
                w = 5
                if x.shape[0] < 3 and c.shape[0] != 4:
                    return x + w, x + y, c
                else:
                    return x - w, x - y, c

        input = (
            torch.ones(5),
            [torch.zeros(5), torch.ones(5)],
            {""a"": torch.zeros(5), ""b"": torch.ones(5)},
            torch.ones(6),
        )

        dynamic_shapes = (
            {0: torch.export.Dim(""dim_x"", min=3)},  # _Dim
            [(""custom_name_axis_ys_0"",), (torch.export.Dim.AUTO,)],  # custom name
            {
                ""a"": {0: torch.export.Dim.AUTO},
                ""b"": (""custom_name_axis_zs_b_0"",),
            },  # _DimHint
            {0: ""custom_name_axis_c_0""},  # custom name
        )

```",open,2025-02-03T18:00:09Z,,,"open source, release notes: onnx, topic: new features",main,titaiwang/support_axis_name,6,722,247,9,1,0,"justinchuby, justinchuby, justinchuby, justinchuby, justinchuby, justinchuby, justinchuby, xadupre, xadupre, xadupre, xadupre, xadupre, justinchuby, titaiwangms, titaiwangms, titaiwangms, titaiwangms, titaiwangms, titaiwangms, titaiwangms, titaiwangms, titaiwangms, justinchuby, justinchuby","COMMENTED, APPROVED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED",False,https://api.github.com/repos/pytorch/pytorch/issues/146321
146319,[export][dynamic shapes] use size-oblivious upper bound reasoning for backed symbols,cc @ezyang @SherlockNoMad @EikanWang @jgong5 @wenzhe-nrv,open,2025-02-03T17:31:50Z,,,"release notes: fx, fx, ciflow/inductor",main,pianpwk/backed_symint_endofbounds,2,28,11,1,1,0,,,True,https://api.github.com/repos/pytorch/pytorch/issues/146319
146318,Hack AC to not clear recomputed activations,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #146633
* __->__ #146318
* #145399
* #145533
* #145531
* #145520

",open,2025-02-03T17:29:36Z,,,,gh/soulitzer/351/base,gh/soulitzer/351/head,1,2,2,1,2,0,,,False,https://api.github.com/repos/pytorch/pytorch/issues/146318
146313,[Dynamo] fix torch._dynamo.assume_constant_result when used on class method,"This PR fixes `torch._dynamo.assume_constant_result` when it is used on a class method, and the class was instantiated inside of the dynamo traced code.

The issue: Currently when an object is modified by storing attributes on it in dynamo, the side effects of those attribute stores are tracked using the `SideEffect` system, and the modifications to the class are not performed in the first place.  For example, in
```python
class A:
    def __init__(self):
        self.value = 123
```
The `self.value` field will not be set on the class `A` but rather it will only be tracked within the `SideEffect` system.

This causes a problem with `torch._dynamo.assume_constant_result` as it converts the value in dynamo back into a normal python value and invokes the function as normal python.  However, it currently does not find the `self.value` field (as it was never set on the underlying object).  This PR checks if the object passed to the `torch._dynamo.assume_constant_result` has any pending mutations from the `SideEffect` system and applies them before calling the `torch._dynamo.assume_constant_result` function.

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",open,2025-02-03T16:00:09Z,,,"triaged, open source, module: dynamo",main,dynamo-fixes-6,2,37,8,2,8,1,"anijain2305, StrongerXi, jansel, anijain2305","APPROVED, COMMENTED, CHANGES_REQUESTED, COMMENTED",False,https://api.github.com/repos/pytorch/pytorch/issues/146313
146310,Fix type stubs for SymmetricMemory,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #146310
* #146308

",open,2025-02-03T14:11:18Z,,,release notes: distributed (c10d),gh/lw/7/base,gh/lw/7/head,1,35,4,1,1,1,yifuwang,APPROVED,False,https://api.github.com/repos/pytorch/pytorch/issues/146310
146308,Support SymmetricMemory's signaling kernels on sm60 and sm70,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #146310
* __->__ #146308

By leveraging libcudacxx's utilities: https://nvidia.github.io/cccl/libcudacxx/extended_api/synchronization_primitives/atomic_ref.html

cc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o",open,2025-02-03T13:35:20Z,,,"oncall: distributed, release notes: distributed (c10d)",gh/lw/6/base,gh/lw/6/head,3,36,58,1,1,0,yifuwang,APPROVED,False,https://api.github.com/repos/pytorch/pytorch/issues/146308
146290,[ c10d ] modify API to get device string from device with torch.device,"Modify the ```get_default_backend_for_device()``` API to extract the device string using ```torch.device()```


cc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o",open,2025-02-03T03:24:49Z,,,"oncall: distributed, triaged, open source, release notes: distributed (c10d)",main,c10d_api_modification,1,1,1,1,2,0,guangyey,APPROVED,False,https://api.github.com/repos/pytorch/pytorch/issues/146290
146289,Use device agnostic APIs for device_count and backend in common_fsdp,"Replace device specific APIs with device abstracted API
",open,2025-02-03T03:10:30Z,,,"triaged, open source, topic: not user facing",main,fsdp_common_cleanup,1,8,8,1,6,0,guangyey,APPROVED,False,https://api.github.com/repos/pytorch/pytorch/issues/146289
146288,[Trace PyDispatcher] Capture Vmapped autograd function as graph,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #146288
* #146272
* #146271
* #146270



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",open,2025-02-03T02:14:22Z,,,"topic: not user facing, module: dynamo, ciflow/inductor",gh/yanboliang/62/base,gh/yanboliang/62/head,5,275,3,3,1,2,"yanboliang, zou3519, zou3519, zou3519, zou3519, zou3519, yanboliang, yanboliang, yanboliang, yanboliang, yanboliang, yanboliang, yanboliang, zou3519, zou3519, zou3519","COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED",False,https://api.github.com/repos/pytorch/pytorch/issues/146288
146285,[scan] Autograd with partial gradient support,"This PR introduces the Autograd feature for scan with partial gradient support. It is a combination of the already opened PRs: https://github.com/pytorch/pytorch/pull/135631 and https://github.com/bohnstingl/pytorch/pull/4

cc @ydwu4 ",open,2025-02-03T00:36:42Z,,,"triaged, open source, topic: not user facing",main,scan_autograd22,2,1324,212,4,2,1,ydwu4,COMMENTED,False,https://api.github.com/repos/pytorch/pytorch/issues/146285
146280,[Inductor-CPU] Avoid redundant compute of index in AVX512 FP32 acc GEMM micro-kernel,"`constexpr int idx` doesn't seem to help here since `idx` is equal to `i`:

```cpp
        constexpr int row = i / COLS;
        constexpr int col = i % COLS;

       // some other code

        constexpr int idx = row * COLS + col;
        vc[idx] = at::vec::fmadd(va, vb[col], vc[idx]);
```

TODO
- [ ] Although it's known at the time of compilation as to what various values of `i` would be due to forced-unrolling of `compute` lambda calls, check if the compiler really computes values of `row`, `col` and `idx` corresponding to each value of `i` at compile-time.
 

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @desertfire @chauhang @aakhundov",open,2025-02-02T20:19:37Z,,,"open source, topic: not user facing, module: inductor, ciflow/inductor",main,sanchitintel/modify_fp32_micro_gemm,1,1,2,2,1,0,,,True,https://api.github.com/repos/pytorch/pytorch/issues/146280
146275,Correctly handle duplicated arguments when merging input views.,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #146275

Fix: #135099

This PR changes how we map the original inputs into the new set of
inputs that take in the tensor input's base instead of their aliases.

**Problem:** in order to create this mapping, we had a dictionary that
mapped the hashed arguments into their respective indices. However, if
there's a group of equal arguments, we will have only one mapping for
such an argument. This breaks the assumption that there will be one
mapping for each argument.

**Solution:** map the hashed arguments into a list of indices. Then, we
will be able to correctly reconstruct the parameters for the new calling
convention.",open,2025-02-02T17:20:19Z,,,"open source, topic: not user facing, ciflow/inductor",gh/ysiraichi/82/base,gh/ysiraichi/82/head,2,28,4,2,2,0,,,False,https://api.github.com/repos/pytorch/pytorch/issues/146275
146273,[dcp] Minor improvements to filesystem writer,"- Apply same check to `_SerialCpuLoader ` from `_OverlappingCpuLoader`  for determining when to clone non-contiguous cpu tensors
- Add minor helper function to avoid iterating over `WriteItem`s twice to collect bytes and tensor write items
- Use the metadata filename constant instead of harcoding 

cc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o @LucasLLC @MeetVadakkanchery @mhorowitz @pradeepfn @ekr0",open,2025-02-02T07:47:24Z,,,"oncall: distributed, triaged, open source, topic: not user facing, module: distributed_checkpoint",main,dist-ckpt-clone-patch,1,16,6,4,2,0,"Skylion007, ananthsub, ananthsub","COMMENTED, COMMENTED, COMMENTED",False,https://api.github.com/repos/pytorch/pytorch/issues/146273
146267,Format tests by PYFMT,"Fixes #ISSUE_NUMBER


cc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o @LucasLLC @MeetVadakkanchery @mhorowitz @pradeepfn @ekr0",open,2025-02-02T04:02:51Z,,,"oncall: distributed, triaged, open source, topic: not user facing, ciflow/inductor, module: distributed_checkpoint",main,ruff_import,8,104,78,2,1,0,,,False,https://api.github.com/repos/pytorch/pytorch/issues/146267
146265,Add libtorch CUDA 12.8 ,"Trying removing sm50 and sm60 to get around the ld --relink error
Test will fail but testing the build.

https://github.com/pytorch/pytorch/issues/145570


cc @ptrblck @msaroufim @eqy",open,2025-02-02T02:42:46Z,,,"module: cuda, triaged, open source, ciflow/binaries, topic: not user facing",main,cu128-libtorch-build,3,66,4,1,4,0,,,False,https://api.github.com/repos/pytorch/pytorch/issues/146265
146264,[ROCm] opportunistic fastatomics for ReduceAdd operations for MI300 GPUs,"In this approach, we are catching any lane within a wave that is doing fastatomics to the same destination address and computing the sum on the CU. This is leading to 3x improvement in scatter_add performance and 2x improvement in index_select.

co-authored by: @amd-hhashemi

Fixes #ISSUE_NUMBER


cc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd",open,2025-02-02T00:23:33Z,,,"module: rocm, open source, topic: not user facing, ciflow/periodic, ciflow/unstable, ciflow/rocm",main,pg-scatter-add-dup-fix,5,63,1,6,3,0,,,True,https://api.github.com/repos/pytorch/pytorch/issues/146264
146248,Make fx.node.map_arg() and .map_aggregate() generic,"## What's the problem?

The popular `fx.node.map_arg()` and `fx.node.map_aggregate()` apply operations recursively on `dict`s, `tuples`, `list`s, etc, and return a new collection of the same type.

Unfortunately, their base input type is `Argument`, which is [very unspecific indeed](https://github.com/pytorch/pytorch/blob/5d55a6585d5806c2743e92118e663f5abb261895/torch/fx/node.py#L48-L58): most type information is just thrown away at the call site of either of these functions, as far as the type checker goes.

As `torch` moves to a more typed code base, this would force innocent, unsuspecting developers to add logically unnecessary casts or `# type: ignore` statements.

## What's the solution?

Making these two `node.map_*` functions generic on the first argument and return type means that type information is preserved for the type checker. (The signature of the other parameter, the function that visits the nodes and subnodes, has not changed, nor should it.)

## Won't it break everything?

It doesn't break the type checker - one place needed an extra hint.

There have been code breakages, resolved one, at least one new one... we'll see!

Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #146626
* __->__ #146248



cc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o @ezyang @malfet @xuzhao9 @gramster @SherlockNoMad @EikanWang @jgong5 @wenzhe-nrv @voznesenskym @penguinwu @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @jiayisunx @chenyang78 @kadeng @chauhang @amjames",open,2025-02-01T18:09:17Z,,,"oncall: distributed, module: typing, open source, better-engineering, ciflow/trunk, release notes: fx, topic: not user facing, fx, module: dynamo, ciflow/inductor, suppress-api-compatibility-check, merging, suppress-bc-linter",gh/rec/129/base,gh/rec/129/head,4,53,56,14,7,0,"Skylion007, Skylion007, rec, Skylion007, XuehaiPan, XuehaiPan, Skylion007, rec, Skylion007, rec, rec, Skylion007","COMMENTED, COMMENTED, COMMENTED, COMMENTED, CHANGES_REQUESTED, APPROVED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, APPROVED",False,https://api.github.com/repos/pytorch/pytorch/issues/146248
146247,torch/nn/modules/conv.py: docs: improvements,"Fix highlighting in generated documentation (`torch/nn/modules/conv.py`):

* attrs should be `:attrs:`,
* constants should be constants,
* text in math should be '\text{}`.

Reborn of #136218.

/cc @albanD, @jbschlosser, @mikaylagawarecki",open,2025-02-01T18:06:45Z,,,"triaged, open source, topic: not user facing",main,patch-2,1,106,106,1,2,0,,,False,https://api.github.com/repos/pytorch/pytorch/issues/146247
146244,Simplify CUDA version checking on tests,"Since we require CUDA >=11.0


cc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o",open,2025-02-01T14:37:48Z,,,"oncall: distributed, triaged, open source, release notes: distributed (c10d)",main,windows_tests2,5,7,17,1,1,0,,,False,https://api.github.com/repos/pytorch/pytorch/issues/146244
146237,[2/N] Fix cppcoreguidelines-init-variables suppression,"Fixes #ISSUE_NUMBER
",open,2025-02-01T09:07:08Z,,,"triaged, open source, topic: not user facing",main,fix_init_variable,9,10,19,2,2,0,"Skylion007, cyyever, cyyever","COMMENTED, COMMENTED, COMMENTED",False,https://api.github.com/repos/pytorch/pytorch/issues/146237
146234,[1/N] Fix F401 errors in tests,"Fixes #ISSUE_NUMBER
",open,2025-02-01T05:05:54Z,,,"triaged, open source, topic: not user facing",main,win_tests,5,29,48,2,2,0,"Skylion007, cyyever, soulitzer, cyyever, albanD, cyyever","COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED",False,https://api.github.com/repos/pytorch/pytorch/issues/146234
146228,[PT2][Inductor][reland] Add runtime numeric check for the post grad pass,"Summary: We observed compilation time regression with previous diff implementation D63438718. Here we fix the issue and reland the diff

Test Plan:
### numeric check enablement test

```
buck2 run mode/opt //scripts/jackiexu0313/pt2:local_model_with_pt2 -- --test_mode batch  --use_synthetic_data --flow_id 685229965 -n
```


### compilation time check

```
buck2 run mode/opt //caffe2/benchmarks/dynamo/fb:torchbench_run_nanogpt_training -- -m nanogpt -t training
```

```
torchbench_run
    duration_ms: 219528
    defaults-batch_size: 1
    defaults-speedup-x1000: 1408
    defaults-abs_latency-x1000: 29068
    defaults-compilation_latency-x1000: 93996
    defaults-compression_ratio-x1000: 924
    defaults-eager_peak_mem-x1000: 2473
    defaults-dynamo_peak_mem-x1000: 2675
    defaults-calls_captured: 1156
    defaults-unique_graphs: 3
    defaults-graph_breaks: 8
    defaults-unique_graph_breaks: 6
    defaults-autograd_captures: 0
    defaults-autograd_compiles: 0
    defaults-cudagraph_skips: 0
    cudagraphs-batch_size: 1
    cudagraphs-speedup-x1000: 5065
    cudagraphs-abs_latency-x1000: 7983
    cudagraphs-compilation_latency-x1000: 76961
    cudagraphs-compression_ratio-x1000: 1485
    cudagraphs-eager_peak_mem-x1000: 4473
    cudagraphs-dynamo_peak_mem-x1000: 3012
    cudagraphs-calls_captured: 1154
    cudagraphs-unique_graphs: 2
    cudagraphs-graph_breaks: 4
    cudagraphs-unique_graph_breaks: 4
    cudagraphs-autograd_captures: 0
    cudagraphs-autograd_compiles: 0
    cudagraphs-cudagraph_skips: 0
    cudagraphs_dynamic-batch_size: 1
    cudagraphs_dynamic-speedup-x1000: 5038
    cudagraphs_dynamic-abs_latency-x1000: 8334
    cudagraphs_dynamic-compilation_latency-x1000: 22521
    cudagraphs_dynamic-compression_ratio-x1000: 893
    cudagraphs_dynamic-eager_peak_mem-x1000: 4017
    cudagraphs_dynamic-dynamo_peak_mem-x1000: 4493
    cudagraphs_dynamic-calls_captured: 1154
    cudagraphs_dynamic-unique_graphs: 2
    cudagraphs_dynamic-graph_breaks: 4
    cudagraphs_dynamic-unique_graph_breaks: 4
    cudagraphs_dynamic-autograd_captures: 0
    cudagraphs_dynamic-autograd_compiles: 0
    cudagraphs_dynamic-cudagraph_skips: 0
```


```
servicelab create benchmark_torchbench_run_nanogpt_training -d D68979204
```

Successfully submitted experiment: https://www.internalfb.com/servicelab/experiment/4800587892/

Differential Revision: D68979204




cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @desertfire @chauhang @aakhundov",open,2025-02-01T01:13:15Z,,,"fb-exported, module: inductor, ciflow/inductor, release notes: inductor, ci-no-td",main,export-D68979204,5,92,25,1,5,0,,,False,https://api.github.com/repos/pytorch/pytorch/issues/146228
146227,[ROCm][TunableOp] Add bias data type to TunableOp signature.,"Fixes #ISSUE_NUMBER


cc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang",open,2025-02-01T01:09:32Z,,,"module: rocm, open source, topic: not user facing, ciflow/rocm",main,tunableop_fp8_bias,1,1,1,1,1,0,,,True,https://api.github.com/repos/pytorch/pytorch/issues/146227
146224,[ONNX] Migrate torchlib into PyTorch,"Migrate torchlib and fix relevant implementation.

1. Use the current ONNX IR based graph builder to run unit tests. Removed the eager evaluator mode tests because they are not relevant to pytorch
2. Updated the builder to handle `Split` nodes correctly by supporting the num_outputs argument
3. Simplified the torchlib registry to directly produce a list of OnnxDecompMeta, which can be consumed directly by the dispatcher
4. Remove handling of traceable functions because all traceable functions are now trace only
5. `torchvision` and `quantized_decomposed` ops are not included.

Fixes https://github.com/pytorch/pytorch/issues/139301

## Next steps
The follow up PRs will decouple the implementation from ONNX Script type system
",open,2025-02-01T00:54:16Z,,,"module: onnx, triaged, open source, ciflow/trunk, release notes: onnx, topic: new features, merging",main,justinchu/ghstack/torchlib,30,20652,435,30,6,0,"justinchuby, justinchuby, justinchuby, justinchuby, justinchuby, justinchuby, justinchuby, xadupre, xadupre, xadupre, xadupre, xadupre, xadupre, justinchuby, justinchuby, justinchuby, justinchuby, justinchuby, xadupre, justinchuby, justinchuby, justinchuby, justinchuby, justinchuby, justinchuby, titaiwangms, justinchuby, justinchuby, justinchuby, justinchuby","COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED",False,https://api.github.com/repos/pytorch/pytorch/issues/146224
146223,What will happen?,,open,2025-02-01T00:25:10Z,,,topic: not user facing,main,malfet-patch-10,1,4,14,2,1,0,"malfet, Skylion007","COMMENTED, COMMENTED",True,https://api.github.com/repos/pytorch/pytorch/issues/146223
146222,[while_loop][inductor] support sym expression as cond_fn output,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #146222


As titled. Previously, we only support tensor output of cond_fn, this PR changes to also allow a shape expr to be returned in cond_fn.

aoti generated output code looks like:
```
V0203 11:28:05.750000 2611693 torch/_inductor/compile_fx.py:1091] [1/0] [__output_code]     bool buf7_cond_result;
....
(while_loop_cond_graph_0_arg2_1_handle);
V0203 11:27:59.336000 2611693 torch/_inductor/compile_fx.py:1091] [1/0] [__output_code]         buf7_cond_result = u0 + u1 < 10L;
V0203 11:27:59.336000 2611693 torch/_inductor/compile_fx.py:1091] [1/0] [__output_code]         if (!buf7_cond_result) break;
```

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @desertfire @chauhang @aakhundov",open,2025-02-01T00:23:46Z,,,"Merged, Reverted, ciflow/trunk, topic: not user facing, module: inductor, ciflow/inductor, ci-no-td",gh/ydwu4/205/base,gh/ydwu4/205/head,6,81,13,2,9,0,desertfire,APPROVED,False,https://api.github.com/repos/pytorch/pytorch/issues/146222
146218,[FSDP2][DEBUG] enforcing ReduceOp.SUM to avoid bug in ReduceOp.AVG,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #146218

workaround for https://github.com/pytorch/pytorch/issues/144045 , but not sure if we should land


cc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o",open,2025-01-31T23:34:19Z,,,"oncall: distributed, release notes: distributed (fsdp), ciflow/inductor",gh/weifengpy/21/base,gh/weifengpy/21/head,1,4,4,1,3,0,,,False,https://api.github.com/repos/pytorch/pytorch/issues/146218
146215,Update Dependencies.cmake,"fix cmake if check error:
“Unknown arguments specified”

Fixes #ISSUE_NUMBER
",open,2025-01-31T23:14:29Z,,,"triaged, open source",main,patch-1,1,1,1,1,3,0,"Skylion007, longlene","COMMENTED, COMMENTED",False,https://api.github.com/repos/pytorch/pytorch/issues/146215
146202,"[ROCm] follow up to #138964, remove work-around","PR #138964 used #ifdef to skip non-contig tensor copies on ROCm due to failing tests.


cc @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd",open,2025-01-31T21:42:15Z,,,"module: rocm, open source, release notes: cuda, ciflow/rocm",main,rocm_followup_128964,1,0,4,1,2,0,,,True,https://api.github.com/repos/pytorch/pytorch/issues/146202
146199,docs: change log to ln in Softplus function and class,"Updated the math formula in the softplus function in torch.nn.functional.py and the Softplus class in torch.nn.modules.activation.py from log to ln for correctness and accuracy.

",open,2025-01-31T20:47:48Z,,,"triaged, open source",main,docs,2,2,2,1,3,0,,,False,https://api.github.com/repos/pytorch/pytorch/issues/146199
146192,torch.check distributions,"Fixes #ISSUE_NUMBER
",open,2025-01-31T19:46:52Z,,,,main,angelayi/distribution,1,6,3,1,2,0,,,True,https://api.github.com/repos/pytorch/pytorch/issues/146192
146191,[cuda] Speed up layernorm backward by ~13% by using warp shuffles for the 16x32 kernel invocation,"Before this PR we had 2 kernels:
1. For blocksize=32x32, this kernel *only* used warp shuffles to do the reduction
2. For blocksize=16x32, this kernel *only* used shared memory to do the reduction

This PR replaces those two kernels with a single generic kernel with template parameters for the block size.

1. Uses template parameters for blockDim.x and blockDim.y.
1. Uses those template parameters to do a partial final reduction in shared memory if needed (i.e. if blockDim.y > 32).
1. Then, for the final 32 rows, it uses warp shuffles when we need to reduce 32 rows down to a single row.
1. Uses slightly more shared memory to reduce bank conflicts when reading the transposed data in both cases

When compared to the baseline 16x32 kernel, ncu shows lower latency:

![image](https://github.com/user-attachments/assets/df4fe13a-31b8-42ef-bc5d-348b39ec21e5)

ncu shows much lower shared memory loads and stores:

![image](https://github.com/user-attachments/assets/c233c712-e2b2-4038-a5e9-9acc45c6e5b9)

ncu shows lower cycle count:

![image](https://github.com/user-attachments/assets/83e7e236-66f1-4d8f-a1e4-11368fb69d09)

ncu shows lower sync instructions:

![image](https://github.com/user-attachments/assets/f5caba87-417b-43a0-a304-0abaecb093dc)

For the 32x32 kernel, nvcc in theory should optimize away the shared memory reduction loop completely and performance should be identical to the previous specialized kernel.",open,2025-01-31T19:45:47Z,,,release notes: cuda,main,ln1,3,62,129,8,1,0,,,True,https://api.github.com/repos/pytorch/pytorch/issues/146191
146189,[test],"Fixes #ISSUE_NUMBER
",open,2025-01-31T19:04:04Z,,,release notes: releng,main,csl/build_test_more_procs,7,276,362,30,1,0,,,True,https://api.github.com/repos/pytorch/pytorch/issues/146189
146182,[export] Allow bypassing version check with unsafe API.,"Summary:
as title.
https://fb.workplace.com/groups/1028545332188949/permalink/10024343514259357/

Test Plan:
```
with torch.export._unsafe_skip_version_check():
    ep = torch.export.load(...)
```
CI

Differential Revision: D68791202


",open,2025-01-31T18:22:22Z,,,"fb-exported, ciflow/trunk, release notes: export",main,export-D68791202,1,23,6,1,2,0,angelayi,APPROVED,False,https://api.github.com/repos/pytorch/pytorch/issues/146182
146180,[AOTI] Improve readability of package_cpp_only,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #146180

Summary: Made two improvements here: 1) Emit interface.cpp into a separate file instead of embedding it to the model code; 2) Add prefix to mark the generated files as model code or weights(constants).

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov",open,2025-01-31T17:53:29Z,,,"topic: improvements, module: inductor, ciflow/inductor, release notes: inductor",gh/desertfire/535/base,gh/desertfire/535/head,4,28,23,1,1,0,angelayi,APPROVED,False,https://api.github.com/repos/pytorch/pytorch/issues/146180
146176,[executorch hash update] update the pinned executorch hash,"Based on latest green in HUD https://hud.pytorch.org/hud/pytorch/executorch/main/1?per_page=50
",open,2025-01-31T17:35:35Z,,,"ciflow/trunk, topic: not user facing, ciflow/inductor",main,et_pin_bump,2,2,2,1,6,0,huydhn,APPROVED,False,https://api.github.com/repos/pytorch/pytorch/issues/146176
146173,[CI] Get rid of UCC builds,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #146173

There hasn't been any active development/testing of those in last 2 years",open,2025-01-31T17:09:55Z,,,topic: not user facing,gh/malfet/159/base,gh/malfet/159/head,5,0,99,1,4,0,,,False,https://api.github.com/repos/pytorch/pytorch/issues/146173
146172,Factory function support for NestedTensor,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #146172
* #146101
* #145922
* #141842
* #141841
* #146052

Rebase of https://github.com/pytorch/pytorch/pull/117904 removing unnecessary bits now that python nested int already holds the necessary metadata.


cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",open,2025-01-31T17:03:29Z,,,"release notes: nested tensor, module: dynamo, ciflow/inductor",gh/soulitzer/350/base,gh/soulitzer/350/head,16,229,7,6,1,0,"ezyang, Skylion007","APPROVED, COMMENTED",False,https://api.github.com/repos/pytorch/pytorch/issues/146172
146171,Noob attempt at tensor_pointer_to_tensor_handle accepting const,"Fairly certain this will fail lint but is there a reason creating an AtenTensorHandle is not const preserving?

Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #146171

",open,2025-01-31T16:35:50Z,,,"ciflow/inductor, release notes: inductor",gh/janeyx99/221/base,gh/janeyx99/221/head,1,5,0,1,1,0,desertfire,APPROVED,True,https://api.github.com/repos/pytorch/pytorch/issues/146171
146145,[CUDAEvent.h] support cuda events in cudagraphs,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #146145

",open,2025-01-31T09:36:23Z,,,release notes: cuda,gh/nmacchioni/39/base,gh/nmacchioni/39/head,5,104,5,9,7,0,"ngimel, nmacchioni, galv","COMMENTED, COMMENTED, COMMENTED",False,https://api.github.com/repos/pytorch/pytorch/issues/146145
146143,Fix C++20 build errors,"Without breaking C++17.


cc @EikanWang @jgong5 @wenzhe-nrv @sanchitintel",open,2025-01-31T08:21:24Z,,,"oncall: jit, triaged, open source, NNC, release notes: jit",main,cxx20_error,5,30,2,4,3,1,"albanD, cyyever, albanD, swolchok, cyyever, cyyever, malfet, malfet, malfet, cyyever","COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, CHANGES_REQUESTED, COMMENTED",False,https://api.github.com/repos/pytorch/pytorch/issues/146143
146142,Fix condition number invertible input(s) documented results,"`torch.linalg.cond` documentation states a singular input raises a RuntimeError, though unit tests show it in fact returns `inf` (https://github.com/pytorch/pytorch/blob/main/test/test_linalg.py#L1576).

 Fixes the documentation and adds an example.

It appears earlier documentation reflected this behavior (https://github.com/pytorch/pytorch/pull/45832/files/9008c10d63e7f5ddd0f06bbd5c7f1548c945d917#diff-316ce439a56491298e2d98deeca82606c52e5bde2f1ceb16c534ec03386c817eR358) 

and then got updated here: https://github.com/pytorch/pytorch/commit/d578e8cfa2db71e45c3565b42ff2b10d13643402.
",open,2025-01-31T07:29:14Z,,,"triaged, open source, release notes: linalg_frontend",main,redwrasse/linalg-cond-non-invertible-err,1,1,2,2,8,2,"lezcano, redwrasse, lezcano","CHANGES_REQUESTED, COMMENTED, COMMENTED",False,https://api.github.com/repos/pytorch/pytorch/issues/146142
146135,WIP: async compile,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #146135
* #146134



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @desertfire @chauhang @aakhundov",open,2025-01-31T05:36:47Z,,,"module: inductor, ciflow/inductor",gh/aorenste/214/base,gh/aorenste/214/head,2,340,0,5,2,0,,,True,https://api.github.com/repos/pytorch/pytorch/issues/146135
146134,Subprocess compile,"Add a mode to `fx_codegen_and_compile()` to compile in a separate process. This is to prepare for async compile where we'll compile and run eager in parallel (and also be able to move the compile phase to a remote computer).

Added a test based which runs the test_torchinductor tests with subprocess compiling turned on.

Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #146134



cc @ezyang @SherlockNoMad @EikanWang @jgong5 @wenzhe-nrv @voznesenskym @penguinwu @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @desertfire @chauhang @aakhundov",open,2025-01-31T05:36:42Z,,,"release notes: fx, fx, module: inductor, ciflow/inductor",gh/aorenste/213/base,gh/aorenste/213/head,7,615,58,6,1,1,"jamesjwu, jamesjwu","COMMENTED, APPROVED",False,https://api.github.com/repos/pytorch/pytorch/issues/146134
146133,Apply ruff fixes to torch/**/*py,"Fixes #ISSUE_NUMBER


cc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o @EikanWang @jgong5 @wenzhe-nrv @sanchitintel @ezyang @SherlockNoMad @voznesenskym @penguinwu @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @desertfire @chauhang @aakhundov",open,2025-01-31T05:10:03Z,,,"oncall: distributed, oncall: jit, triaged, open source, release notes: quantization, fx, module: inductor, module: dynamo, ciflow/inductor, release notes: export",main,ruff_fix,30,92,123,1,1,0,,,False,https://api.github.com/repos/pytorch/pytorch/issues/146133
146113,"Fix logging and test files which misspell ""precision""","Noticed this while working on something, decided to submit a quick fix.",open,2025-01-31T00:51:57Z,,,"ciflow/trunk, release notes: linalg_frontend, merging",main,spell,2,2,2,1,7,0,drisspg,APPROVED,False,https://api.github.com/repos/pytorch/pytorch/issues/146113
146110,[scan] Corrections for scan,"This PR resolves some minor issues with the scan HOP and unifies the handling of the additional_inputs in the same way as for associative_scan.

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @ydwu4 ",open,2025-01-31T00:26:28Z,,,"triaged, open source, topic: not user facing, module: dynamo",main,scan_hop_fixes,3,25,39,3,4,0,"ydwu4, bohnstingl, bohnstingl, Skylion007","COMMENTED, COMMENTED, COMMENTED, COMMENTED",False,https://api.github.com/repos/pytorch/pytorch/issues/146110
146109,cpp_wrapper: fix inductor triton tests,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #146452
* #146706
* #146424
* __->__ #146109
* #146449
* #144349
* #144293
* #144002



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @desertfire @chauhang @aakhundov",open,2025-01-31T00:12:22Z,,,"open source, topic: not user facing, module: inductor, ciflow/inductor",gh/benjaminglass1/63/base,gh/benjaminglass1/63/head,5,75,44,7,1,0,benjaminglass1,COMMENTED,False,https://api.github.com/repos/pytorch/pytorch/issues/146109
146104,[DO NOT MERGE] Testing C2 MI300 cluster.,"This PR is to test the stability of the C2 MI300x cluster.

cc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd",open,2025-01-30T23:49:22Z,,,"module: rocm, open source, topic: not user facing, ciflow/unstable",main,test-c2,3,11,8,1,1,0,,,False,https://api.github.com/repos/pytorch/pytorch/issues/146104
146101,(WIP) Update NJT ops to check data for raggedness check,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #146172
* __->__ #146101
* #145922
* #141842
* #141841
* #146052


Next:
   - make sure guards are okay

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",open,2025-01-30T23:22:38Z,,,"release notes: nested tensor, module: dynamo, ciflow/inductor",gh/soulitzer/349/base,gh/soulitzer/349/head,11,430,74,7,2,0,"albanD, soulitzer, soulitzer, soulitzer, soulitzer","COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED",False,https://api.github.com/repos/pytorch/pytorch/issues/146101
146098,Move get accelerator to use build time flags when possible,"This PR does two main things (they are in a single PR to show how the newly added APIs are used).

- Add isBuilt and isAvailable APIs to the AcceleratorHook interface. See inline doc for their exact semantic
- Use the newly added isBuilt for accelerator check to ensure it does not poison fork


cc @egienvalue we should do an MTIA patch for this and move to compile-time check once we figure out the CUDA+MTIA binary situation
cc @guangyey we would need to add these APIs to the HPU backend (which I don't have access to) and we can move it to be compile time as well to avoid initialization.",open,2025-01-30T23:13:45Z,,,"Merged, Reverted, ciflow/trunk, release notes: python_frontend, topic: bug fixes, ciflow/mps, ciflow/xpu, ci-no-td",main,acc_clean,7,78,28,11,12,0,"ngimel, janeyx99, janeyx99, janeyx99, janeyx99, malfet, egienvalue, albanD, EikanWang","APPROVED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, APPROVED, COMMENTED, COMMENTED, APPROVED",False,https://api.github.com/repos/pytorch/pytorch/issues/146098
146093,TEST3,"Fixes #ISSUE_NUMBER


cc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o",open,2025-01-30T22:36:55Z,,,"oncall: distributed, release notes: releng, ciflow/inductor",main,zainr/mypy-break-test3,8,23,18,4,1,0,,,True,https://api.github.com/repos/pytorch/pytorch/issues/146093
146092,TEST2,"Fixes #ISSUE_NUMBER


cc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o",open,2025-01-30T22:36:11Z,,,"oncall: distributed, release notes: releng, ciflow/inductor",main,zainr/mypy-break-test2,8,25,18,2,1,0,,,True,https://api.github.com/repos/pytorch/pytorch/issues/146092
146091,[WIP] TEST 1,"Fixes #ISSUE_NUMBER


cc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o",open,2025-01-30T22:35:18Z,,,"oncall: distributed, release notes: releng, ciflow/inductor",main,zainr/mypy-break-test,8,26,18,1,1,0,,,True,https://api.github.com/repos/pytorch/pytorch/issues/146091
146090,Fix TestDataLoader.test_segfault unexpected success on Aarch64,TestDataLoader.test_segfault gives unexpected success on linux Aarch64,open,2025-01-30T22:35:15Z,,,"triaged, open source, release notes: dataloader",main,dataloader,3,18,8,2,2,0,,,False,https://api.github.com/repos/pytorch/pytorch/issues/146090
146073,Nccl update to 2.25.1 for cuda 11.8-12.6,"Should resolve: https://github.com/pytorch/pytorch/issues/144768
We build one common nccl version for all cuda builds: ``NCCL_VERSION=v2.25.1-1``
For CUDA 11.8 we package this wheel instead of installing it from pypi.

",open,2025-01-30T21:09:07Z,,,"topic: not user facing, ciflow/binaries_wheel",main,nccl_module_test,15,174,177,14,4,1,"Skylion007, atalman, atalman, malfet","APPROVED, COMMENTED, COMMENTED, APPROVED",True,https://api.github.com/repos/pytorch/pytorch/issues/146073
146069,Set /NODEFAULTLIB:vcomp for MSVC when linking caffe2::mkl with libiomp5md.lib ,"Fixes:
- https://github.com/pytorch/pytorch/issues/113490

The PR sets `/NODEFAULTLIB:vcomp` link flag when linking caffe2::mkl with libiomp5md.lib.

The changes have been verified by checking build output with `VERBOSE=1`, for example:
```
C:\PROGRA~1\MICROS~1\2022\COMMUN~1\VC\Tools\MSVC\1442~1.344\bin\Hostx64\x64\link.exe /nologo caffe2\CMakeFiles\torch_global_deps.dir\__\torch\csrc\empty.c.obj /out:bin\torch_global_deps.dll /implib:lib\torch_global_deps.lib /pdb:bin\torch_global_deps.pdb /dll /version:0.0 /machine:x64 /ignore:4049 /ignore:4217 /ignore:4099 /debug /INCREMENTAL:NO /NODEFAULTLIB:vcomp -LIBPATH:\lib -LIBPATH:\lib\intel64 -LIBPATH:\lib\intel64_win -LIBPATH:\lib\win-x64 C:\lib\mkl_intel_lp64.lib C:\lib\mkl_intel_thread.lib C:\lib\mkl_core.lib C:\lib\libiomp5md.lib kernel32.lib user32.lib gdi32.lib winspool.lib shell32.lib ole32.lib oleaut32.lib uuid.lib comdlg32.lib advapi32.lib /MANIFEST:EMBED,ID=2
```


cc @malfet @seemethere @peterjc123 @mszhanyi @skyline75489 @nbcsm @iremyux @Blackhex",open,2025-01-30T20:33:33Z,,,"module: build, module: windows, triaged, open source, ciflow/trunk, release notes: build, topic: bug fixes",main,pytorch-113490,1,3,0,1,4,1,malfet,CHANGES_REQUESTED,False,https://api.github.com/repos/pytorch/pytorch/issues/146069
146068,Error handling for launcher method in CachingAutotuner,"Fixes #146018


cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang @aakhundov",open,2025-01-30T20:25:52Z,,,"triaged, open source, function request, topic: not user facing, module: inductor",main,error_handling_caching_autotuner,2,178,1,2,10,1,eellison,CHANGES_REQUESTED,False,https://api.github.com/repos/pytorch/pytorch/issues/146068
146064,[PT2] Support add/remove passes in pre_grad,"Summary:
support the same functionality with acc_tracer disabled, add a new config for pre_grad add/remove_passes, at the front end it still uses the same interface

some minor updates in pre_grad passes to make sure the passes are run in desired order, after added passes, still run pass like remove_noops at the end

Test Plan: add new UT, please see stacked diff for add pass tests (TODO: update diff link)

Differential Revision: D68909278




cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang @aakhundov",open,2025-01-30T19:17:34Z,,,"fb-exported, ciflow/trunk, topic: not user facing, module: inductor, ciflow/inductor",main,export-D68909278,3,77,94,1,3,0,frank-wei,APPROVED,False,https://api.github.com/repos/pytorch/pytorch/issues/146064
146063,[wip] torch._dynamo.disable on the CA graph,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #146063



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @yf225",open,2025-01-30T19:14:34Z,,,"module: dynamo, ciflow/inductor, module: compiled autograd",gh/xmfan/166/base,gh/xmfan/166/head,2,122,0,1,2,0,,,True,https://api.github.com/repos/pytorch/pytorch/issues/146063
146061,[inductor][triton] Fix average pool nd for int64 dtype,"The eager mode implementation of average pool nd returns an integer tensor if the input is also an integer tensor. This should also be preserved in inductor.

Fixes pytest -k test_comprehensive_nn_functional_avg_pool2d_cpu_int64 error: Triton compilation failed: triton_poi_fused_avg_pool2d_0

See WIP https://github.com/pytorch/pytorch/pull/145865#issuecomment-26200289890 to potentially enable such tests as they aren't enabled yet.

Fixes #ISSUE_NUMBER


cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang @aakhundov",open,2025-01-30T18:54:43Z,,,"triaged, open source, topic: not user facing, module: inductor, ciflow/inductor",main,mwizak/fix-avg-pool-int64-dtype,3,29,5,1,3,1,eellison,COMMENTED,False,https://api.github.com/repos/pytorch/pytorch/issues/146061
146055,[Win][CD] Install cmake and setuptools from PyPI,"And also avoid repeating the same command over and over
",open,2025-01-30T18:14:34Z,,,"topic: not user facing, ciflow/binaries_wheel",main,malfet-patch-8,1,10,9,3,1,2,"Skylion007, atalman, seemethere","APPROVED, APPROVED, APPROVED",False,https://api.github.com/repos/pytorch/pytorch/issues/146055
146051,inductor.config.descriptive_names = False is not actually supported (#145523) (#145523),"Summary:
This config is not supported (it throws an error when set), and doesn't really make sense imo.

Approved by: https://github.com/eellison

Test Plan: contbuild & OSS CI, see https://hud.pytorch.org/commit/pytorch/pytorch/edf266e9bbbf6063f7c4a336ffb50234e11a0a82

Differential Revision: D68846308




cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang @aakhundov",open,2025-01-30T17:52:47Z,,,"fb-exported, ciflow/trunk, topic: docs, module: inductor, ciflow/inductor, release notes: inductor",main,export-D68846308,1,2,3,1,4,0,Skylion007,APPROVED,False,https://api.github.com/repos/pytorch/pytorch/issues/146051
145999,"[CUDA] Optimize CUDA occupancy for indexing operators like index_select, index_add and index_reduce","### Background
For indexing operations such as **torch.index_select**, **torch.index_add**, and **torch.index_reduce**, GPU performance is relatively low when handling large input sizes. On an A100 GPU, `torch.index_select` achieves only about 30% of the theoretical memory bandwidth for large inputs. Notably, `torch.index_select` and `torch.index_add` are among the most time-consuming operations in the forward and backward passes of `torch.nn.Embedding`, which is widely used in NLP and deep learning ranking models (DLRM).

Upon analysis, I identified the following line of code`dim3 largeIndexGrid(std::min(ceil_div(sourceTotalSize, (uint64_t)128), (uint64_t)(mpc * 8)))`. Here, the blockSize is fixed at 128, and the number of blocks is set to `mpc * 8`, where mpc represents the number of streaming multiprocessors (SMs). This configuration results in only 1024 threads per SM (128 × 8). However, on an A100 GPU, each SM supports up to 2048 concurrent threads, meaning that the theoretical occupancy is limited to 50%.

This commit improves grid dimension calculation by leveraging maxThreadsPerMultiProcessor and blockSize, aligning with best practices commonly used in other GPU kernels in PyTorch.

### Tests Performed
1. Correctness testing: All tests in `test/test_torch.py` passed, including numerous tests covering `torch.index_select`, `torch.index_add` and `torch.index_reduce`. 
2. Performance testing: I run performance testing with the  following [script ](https://github.com/YyWangCS/FairySpeed/blob/main/embedding/bench_index_ops.py)on A100, the performance number is as follows. 

#### torch.index_select

| num_embedding | embedding_dim | input_size | kernel latency before optimization (µs) | kernel latency after optimization (µs) |
| ------------- | ------------- | ---------- | --------------------------------------- | -------------------------------------- |
| 1000000       | 128           | 307200     | 518.3                                   | 359.4                                  |
| 1000000       | 32            | 307200     | 141.4                                   | 97.3                                   |
| 1000000       | 128           | 204800     | 347.5                                   | 242.4                                  |
| 1000000       | 32            | 204800     | 96.2                                    | 66.2                                   |
| 128000        | 4096          | 4096       | 219.2                                   | 158.6                                  |

#### torch.index_add

| num_embedding | embedding_dim | input_size | kernel latency before optimization (µs) | kernel latency after optimization (µs) |
| ------------- | ------------- | ---------- | --------------------------------------- | -------------------------------------- |
| 1000000       | 128           | 307200     | 526.8                                   | 379.4                                  |
| 1000000       | 32            | 307200     | 143.4                                   | 103.3                                  |
| 1000000       | 128           | 204800     | 352.2                                   | 256.1                                  |
| 1000000       | 32            | 204800     | 98.9                                    | 69.9                                   |
| 128000        | 4096          | 4096       | 222.8                                   | 165.0                                  |

#### torch.index_reduce

| num_embedding | embedding_dim | input_size | kernel latency before optimization (µs) | kernel latency after optimization (µs) |
| ------------- | ------------- | ---------- | --------------------------------------- | -------------------------------------- |
| 1000000       | 128           | 307200     | 732.5                                   | 470.7                                  |
| 1000000       | 32            | 307200     | 197.5                                   | 126.6                                  |
| 1000000       | 128           | 204800     | 490.8                                   | 316.4                                  |
| 1000000       | 32            | 204800     | 133.7                                   | 86.1                                   |
### Reference
[Performance Optimization of Embedding Computation on GPU Part 1: GPU Occupancy Optimization](https://yywangcs.notion.site/Performance-Optimization-of-Embedding-Computation-on-GPU-Part-1-GPU-Occupancy-Optimization-178fc9f5d805800e91b6d4490afcc665)",open,2025-01-30T01:14:03Z,,,"triaged, open source, release notes: cuda",main,YyWangCS/opt_embedding,1,25,7,4,2,0,,,False,https://api.github.com/repos/pytorch/pytorch/issues/145999
145992,fix indirect broadcast,"Fixes #142250


cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang @aakhundov",open,2025-01-30T00:27:28Z,,,"topic: not user facing, module: inductor, ciflow/inductor",main,findhao/fix-indirect-access,2,37,1,2,1,0,,,True,https://api.github.com/repos/pytorch/pytorch/issues/145992
145990,[Profiler] Add Full PG ranks to Metadata,"Summary: We only add a shortened list of PG ranks if there is a job distributed across multiple nodes. Let's add the PG ranks to the JSON metadata

Test Plan:
https://www.internalfb.com/intern/perfdoctor/trace_view?filepath=tree/traces/dynocli/devvm2185.cco0.facebook.com/rank-0.Jan_29_16_19_52.2285734.pt.trace.json.gz&bucket=gpu_traces
{F1974810517}

Differential Revision: D68867518


",open,2025-01-30T00:25:11Z,,,"enhancement, fb-exported, release notes: profiler",main,export-D68867518,1,6,0,1,7,0,,,False,https://api.github.com/repos/pytorch/pytorch/issues/145990
145979,[draft_export] better stack logging for strict mode,"Strict-mode draft export tends to log unhelpful stack traces for guards/data-dependent errors, relying on `CapturedTraceback.extract()`, which is only accurate for non-strict. For dynamo, it's better to use `TracingContext.extract_stack()` and fallback to the former when this is empty, avoiding traces pointing to the top-level export call, or lambdas (in the case of `torch._check` calls).

e.g. before, for `test_draft_export.py -k test_offsets`:
```
    This occurred at the following stacktrace: 
        File /data/users/pianpwk/pytorch/test/export/test_draft_export.py, lineno 259, in test_offsets:
        `ep, report = draft_export(M(), inp, strict=True)`
```
after:
```
    This occurred at the following stacktrace: 
        File /data/users/pianpwk/pytorch/test/export/test_draft_export.py, lineno 254, in forward:
            `if a == 0:`
```

cc @ezyang @SherlockNoMad @EikanWang @jgong5 @wenzhe-nrv",open,2025-01-29T23:04:26Z,,,"ciflow/trunk, fx, ciflow/inductor, release notes: export",main,pianpwk/draft_strict_stack,2,44,30,1,2,0,,,False,https://api.github.com/repos/pytorch/pytorch/issues/145979
145969,Test of triton.compile in worker processes,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #145969



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang @aakhundov",open,2025-01-29T20:57:45Z,,,"module: inductor, ciflow/inductor",gh/jamesjwu/100/base,gh/jamesjwu/100/head,5,66,58,3,2,0,,,True,https://api.github.com/repos/pytorch/pytorch/issues/145969
145966,[BE] Upgrade to mypy 1.14,"Upgrade mypy version

cc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o",open,2025-01-29T20:48:32Z,,,"oncall: distributed, release notes: releng, ciflow/inductor",main,zainr/mypy-update,11,45,31,7,2,0,"Skylion007, Skylion007, Skylion007, ZainRizvi, Skylion007, Skylion007, Skylion007, Skylion007","COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, CHANGES_REQUESTED, COMMENTED",False,https://api.github.com/repos/pytorch/pytorch/issues/145966
145957,Fix invalid nested int guarding in broadcast_shapes(),"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #145957

Fixes #145874

This PR takes the approach of updating the logic determining whether multiple shapes broadcast together to handle nested ints specially.

Possible alternative approach: don't update `broadcast_shapes()` + indicate that e.g. `Ne(j0, 1)` should statically evaluate to False. I briefly tried this but it wasn't straightforward. Is it better?",open,2025-01-29T19:40:37Z,,,topic: not user facing,gh/jbschlosser/227/base,gh/jbschlosser/227/head,2,57,7,2,6,0,"bobrenjc93, jbschlosser, soulitzer","COMMENTED, COMMENTED, COMMENTED",False,https://api.github.com/repos/pytorch/pytorch/issues/145957
145955,"Add MPS OpInfo db, rework test_mps to use OpInfo",Infrastructure changes that will help enable: https://github.com/pytorch/pytorch/pull/142202,open,2025-01-29T19:29:09Z,,,"triaged, open source, release notes: mps, ciflow/mps, keep-going",main,dev/skotapati/mps_op_db,5,877,1037,17,1,0,,,False,https://api.github.com/repos/pytorch/pytorch/issues/145955
145946,[ROCm][TunableOp] hipblaslt tf32 support,"TF32 is supported by hipblaslt. Support added by #143549.  This PR expands integration to the TunableOp feature.

cc @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd",open,2025-01-29T18:06:41Z,,,"module: rocm, open source, ciflow/trunk, topic: not user facing, ciflow/rocm",main,rocm_tunableop_tf32,2,15,2,3,2,0,pruthvistony,APPROVED,False,https://api.github.com/repos/pytorch/pytorch/issues/145946
145942,Enable fast qlinear_dynamic path for AArch64 through ACL directly,"This enables a fast path for eager mode dynamic quantization for AArch64 through Arm Compute Library (ACL) directly.

Context: PR #126687 enabled an optimized implementation for `qlinear_dynamic` for AArch64 through ideep → oneDNN → ACL which improved performance by ~10x compared to the previous implementation. 

However, the current `qlinear_dynamic` path (ideep → oneDNN → ACL) suffers from high overhead due to the API friction between the stateless oneDNN API and the stateful ACL low-precision GEMM (`lowp_gemm`) API - for example, ACL's `lowp_gemm` objects cache information like weights reduction or weights in optimized memory format which oneDNN does not allow due to its stateless nature. Hence, ACL currently runs a (redundant) sum of columns and pre-transposition (to the gemm kernel's optimal format) for each GEMM operation. 

This PR addresses the sub-optimalities above by integrating ACL directly with `qlinear_dynamic`. This approach yields an average speedup (averaged over context_lengths of 2^3 up to 2^9) of ~ 50% for `bert-base-uncased`, `bert-large-uncased`, `roberta-base`, `distilbert-base-uncased` with 16 threads on a Neoverse-V1 (with `transformers==4.48`) - See benchmark code below. To achieve this, we:
* Use ACL which is already built with PyTorch as a shared library when `USE_MKLDNN_ACL` is set.
* Add ACL to ATen's CPU include and dependency libs
* Introduce `PackedLinearWeightsACL` (as a subclasses of `PackedLinearWeightsOnednn`) with an implementation of `qlinear_dynamic` that uses ACL directly, while `qlinear` still follows the oneDNN path.
* A future PR will introduce a direct ACL implementation `qlinear` and will allow us to remove the dependence on `PackedLinearWeightsOnednn`

Note, that the ACL `lowp_gemm` API changed slightly between v24.09 and v24.12 which will be the new version after #138889 - Hence, this PR targets the new version - v24.12.

The following code was used to benchmark `qlinear_dynamic` performance:
```
# SPDX-FileCopyrightText: Copyright 2025 Arm Limited and/or its affiliate <open-source-office@arm.com>
# SPDX-License-Identifier: BSD-3-Clause
import torch
from transformers import AutoModel, AutoConfig
import time
import numpy as np
from argparse import ArgumentParser

class ModelArgumentParser(ArgumentParser):
    def __init__(self) -> None:
        super().__init__(description=""huggingface model"")
        self.add_argument(""--context_length"",
                            help=""context length - number of input tokens"",
                            type=int,
                            default=64
        )
        self.add_argument(""--model"",
                            help=""model checkpoint - i.e. 'bert-base-uncased'"",
                            type=str,
                            default=None)
        self.add_argument(""--iters"",
                          help=""benchmark iterations"",
                          default=500)

if __name__ == ""__main__"":
    parser = ModelArgumentParser()
    args = parser.parse_args()
    model_name = args.model
    config = AutoConfig.from_pretrained(model_name)
    batch_size = 1
    model = AutoModel.from_pretrained(model_name)
    model = torch.quantization.quantize_dynamic(model, {torch.nn.Linear}, dtype=torch.qint8)
    model.eval()
    inputs = torch.randint(config.vocab_size, (batch_size, args.context_length), dtype=torch.long, device=""cpu"")
    times = []
    with torch.no_grad():
        # warmup
        for _ in range(10):
            model(inputs)
        # benchmark
        for _ in range(args.iters):
            s = time.time_ns()
            model(inputs)
            times.append((time.time_ns() - s) / 1e6)

    print(""Model = "", model_name)         
    print(""Context Length = "", args.context_length)
    print(""Min (ms) = "", min(times))
    print(""Mean (ms) = "", np.mean(times))  
```



Fixes #ISSUE_NUMBER


cc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @malfet @snadampal @milpuz01",open,2025-01-29T17:19:32Z,,,"module: cpu, triaged, open source, module: arm, release notes: quantization, release notes: releng, ciflow/linux-aarch64",main,acl_qlinear_dynamic,9,471,15,2,6,1,malfet,COMMENTED,False,https://api.github.com/repos/pytorch/pytorch/issues/145942
145936,`torch.tensordot`: performance improvements when contracting to a scalar.,"As per title.
Fixes https://github.com/pytorch/pytorch/issues/145731

Touches only compute. The CPU overhead can potentially be further reduced.

Before:
```python
In [3]: n = 512

In [4]: A = torch.rand(n, n)

In [5]: B = torch.rand(n, n)

In [6]: %timeit torch.tensordot(A, B, [[0, 1], [0, 1]])
2.04 ms ± 70 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)

In [7]: %timeit torch.tensordot(A, B, [[0, 1], [1, 0]])
2.85 ms ± 191 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)

In [8]: %timeit torch.tensordot(A, B, [[1, 0], [0, 1]])
2.9 ms ± 133 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)

In [9]: %timeit torch.tensordot(A, B, [[1, 0], [1, 0]])
4.07 ms ± 262 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
```

After
```python
In [2]: n = 512

In [3]: A = torch.rand(n, n)

In [4]: B = torch.rand(n, n)

In [5]: %timeit torch.tensordot(A, B, [[0, 1], [0, 1]])
30.7 µs ± 2.51 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)

In [6]: %timeit torch.tensordot(A, B, [[0, 1], [1, 0]])
141 µs ± 6.52 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)

In [7]: %timeit torch.tensordot(A, B, [[1, 0], [0, 1]])
142 µs ± 4.03 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)

In [8]: %timeit torch.tensordot(A, B, [[1, 0], [1, 0]])
62.8 µs ± 4.31 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)

```

cc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o",open,2025-01-29T16:18:48Z,,,"oncall: distributed, open source, ciflow/trunk, release notes: python_frontend, topic: performance, ciflow/inductor",main,nikitaved/tensordot,4,38,8,1,9,2,"albanD, albanD, nikitaved, nikitaved, nikitaved","APPROVED, APPROVED, COMMENTED, COMMENTED, COMMENTED",False,https://api.github.com/repos/pytorch/pytorch/issues/145936
145935,[CPU Stream] Add noop for CPU stream record_event() and wait_event(),"Summary: Adds wait_event and record_event endpoints to CPU stream in order to facilitate device-agnostic code. Both methods are noops.

Test Plan: CI

Differential Revision: D68833927




cc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10",open,2025-01-29T15:45:43Z,,,"module: cpu, fb-exported",main,export-D68833927,1,6,0,1,3,0,,,False,https://api.github.com/repos/pytorch/pytorch/issues/145935
145930,[Test][Linalg][CUDA] Increase niter in test_svd_lowrank_cuda_float64,"A recent PR #143049 attempted to increase tolerances to make test passable. However, we are still seeing errors like:
```
Traceback (most recent call last):
  File ""~git/pytorch/test/test_linalg.py"", line 2540, in test_svd_lowrank
    run_subtest(None, size, (), device, torch.svd_lowrank, density=density)
  File ""~git/pytorch/test/test_linalg.py"", line 2505, in run_subtest
    self.assertEqual(A, a, rtol=1e-7, atol=2e-7)
  File ""~git/pytorch/torch/testing/_internal/common_utils.py"", line 4044, in assertEqual
    raise error_metas.pop()[0].to_error(  # type: ignore[index]
AssertionError: Tensor-likes are not close!

Mismatched elements: 90 / 1000000 (0.0%)
Greatest absolute difference: 7.795904016052784e-07 at index (176, 930) (up to 2e-07 allowed)
Greatest relative difference: inf at index (6, 179) (up to 1e-07 allowed)
```
Increasing `niter` parameter actually decreases numerical differences.

cc @ptrblck @msaroufim @eqy @jianyuh @nikitaved @pearu @mruberry @walterddr @xwang233 @Lezcano",open,2025-01-29T14:22:18Z,,,"module: cuda, triaged, open source, module: linear algebra, topic: not user facing",main,increase_niter_in_test_svd_lowrank_cuda_float64,1,1,1,1,1,0,,,False,https://api.github.com/repos/pytorch/pytorch/issues/145930
145923,Update mi300 labels to account for multiple clusters.,"We now have multiple Kubernetes clusters of mi300x resources, and this commit updates labels accordingly to target both clusters evenly.

cc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd",open,2025-01-29T08:51:26Z,,,"module: rocm, triaged, open source, Merged, Reverted, topic: not user facing, ci-no-td",main,mi300-labels,2,20,20,1,11,0,jeffdaily,DISMISSED,False,https://api.github.com/repos/pytorch/pytorch/issues/145923
145922,Update NestedInt equality to take into account all metadata,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #146172
* #146101
* __->__ #145922
* #141842
* #141841
* #146052



cc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @ezyang @SherlockNoMad @EikanWang @wenzhe-nrv @voznesenskym @penguinwu @Guobing-Chen @zhuhaozhe @blzheng @jiayisunx @chenyang78 @kadeng @chauhang @amjames",open,2025-01-29T08:50:27Z,,,"module: cpu, release notes: fx, fx, module: dynamo, ciflow/inductor",gh/soulitzer/347/base,gh/soulitzer/347/head,8,107,35,17,1,0,,,False,https://api.github.com/repos/pytorch/pytorch/issues/145922
145920,Add basic Gaudi support to benchmarks/dynamo,"This PR adds basic Gaudi support to benchmarks/dynamo


cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",open,2025-01-29T08:28:00Z,,,"triaged, open source, topic: not user facing, oncall: pt2, module: dynamo",main,kfojcik/basic_hpu_dynamo_benchmark,1,9,1,1,4,0,,,False,https://api.github.com/repos/pytorch/pytorch/issues/145920
145917,Draft: fix: Some smaller mingw fixes,"Fixes #ISSUE_NUMBER
",open,2025-01-29T05:27:38Z,,,open source,main,fix-mingw,2,4,4,1,3,0,,,True,https://api.github.com/repos/pytorch/pytorch/issues/145917
145911,Add future lazy clone setting and deprecate `torch.reshape` view,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #145911



cc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @voznesenskym @penguinwu @EikanWang @Guobing-Chen @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @desertfire @chauhang @aakhundov @ColinPeppler",open,2025-01-29T03:27:56Z,,,"oncall: distributed, module: cpu, open source, release notes: python_frontend, module: inductor, module: dynamo, ciflow/inductor",gh/kurtamohler/31/base,gh/kurtamohler/31/head,30,470,48,6,2,0,kurtamohler,COMMENTED,True,https://api.github.com/repos/pytorch/pytorch/issues/145911
145910,Fix redundant move,"Fixes #ISSUE_NUMBER


cc @EikanWang @jgong5 @wenzhe-nrv @sanchitintel",open,2025-01-29T03:21:22Z,,,"oncall: jit, open source, NNC, release notes: jit",main,move,1,2,2,1,1,0,,,True,https://api.github.com/repos/pytorch/pytorch/issues/145910
145903,[NJT] Add cumsum support for nested tensors,"Summary: - Add cumsum support for NT

Test Plan: - added unit tests

Differential Revision: D68307097


",open,2025-01-29T01:24:45Z,,,"fb-exported, topic: improvements, release notes: nested tensor",main,export-D68307097,2,62,0,1,8,0,"jbschlosser, jbschlosser, ketansingh, jbschlosser","COMMENTED, COMMENTED, COMMENTED, COMMENTED",False,https://api.github.com/repos/pytorch/pytorch/issues/145903
145885,Hacky solution to bad interaction between AOTAutogradcache and Triton 3.1,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #145885

",open,2025-01-28T22:50:12Z,,,"topic: not user facing, ciflow/inductor",gh/jamesjwu/97/base,gh/jamesjwu/97/head,2,28,1,2,1,0,"bdhirsh, jamesjwu","COMMENTED, COMMENTED",False,https://api.github.com/repos/pytorch/pytorch/issues/145885
145873,[WIP] Allow generation of inductor backend specific tests using instantiate_device_type_tests ,"This allows the creation of inductor backend specific test classes. Since this is an extension point for out of tree backends, it also allows out of tree backends to customise test instantiation to fit their backend / device.

To maintain backwards compatibility, `only_inductor_backends` defaults to `None` so that the behaviour of test class instantiation matches the incumbent behaviour. If `only_inductor_backends` is not None, inductor backend specific test classes will be created from a test template, e.g. `TestInductorOpInfo -> TestInductorOpInfoTritonCUSTOMDEVICE, TestInductorOpInfoHalideCUSTOMDEVICE`

An illustration of the before/after changes:

```python

# in test_inductor.py
# Inductor test template
class TestInductor:
    def test_comprehensive(...)
    
# Original
instantiate_device_type_tests(TestSuiteTemplate)
# Generates something like this:
# TestInductorCPU
# TestInductorCUDA

# After changes
instantiate_device_type_tests(TestSuiteTemplate, enable_inductor_backend_classes=True, only_inductor_backends=[""cpp"", ""triton""])
# TestInductorCppCPU
# TestInductorCppTriton
# TestInductorTritonCUDA

# Additionally, the new test classes if a native inductor backend is used are guarded
# e.g.  TestInductorCppCPU
# is equivalent to the following class definition
# @skipUnless(HAS_CPU, ""Requires C++ compiler"")
# @config.patch(""cpu_backend"", ""cpp"")
# class TestInductorCppCPU(CPUTestBase)
#   ...



```

An illustration of the before/after changes:


Fixes #ISSUE_NUMBER


cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang @aakhundov",open,2025-01-28T20:54:32Z,,,"open source, module: inductor",main,mwizak/extend-device-agnostic-testing-with-inductor,4,195,71,5,2,0,,,True,https://api.github.com/repos/pytorch/pytorch/issues/145873
145866,[pytorch][cuda] Improve softmax backward pass native CUDA implementation,"This PR is similar to https://github.com/pytorch/pytorch/pull/122970, but works on the softmax backward pass.

Specifically, it uses shared memory to cache the gradOutput when it can fit in shared memory. Before this PR we were reading gradOutput twice.

On my H100 this seems to improve the softmax backward pass performance by about 5% for problem sizes that fit within shared memory. (Note that this is not the only kernel that runs when you call softmax backward pass -- there is an elementwise kernel that runs before this; optimizing that can be a separate PR).

**Important Note**: Currently the softmax backward pass consists of an [element-wise multiply operator](https://github.com/pytorch/pytorch/blob/7f65a208848205b38445423b7e2e93a2b4994e5e/aten/src/ATen/native/cuda/SoftMax.cu#L1216), followed by [this function](https://github.com/pytorch/pytorch/blob/7f65a208848205b38445423b7e2e93a2b4994e5e/aten/src/ATen/native/cuda/SoftMax.cu#L1062) which calls the `cunn_SoftMaxBackward` kernel. With my change the kernel time reduces by about 12% (see screenshot below), while the total time (including the elementwise) reduces by about 5%.

```
Baseline						This PR
N	size	FP32 bandwidth	FP16 bandwidth		N	size	FP32 bandwidth	FP16 bandwidth		fp32 diff	fp16 diff
0	256	134.340966	70.042039		0	256	133.70146	70.342753		-0.48%	0.43%
1	512	233.501185	129.945803		1	512	234.057145	132.933066		0.24%	2.30%
2	1024	340.667966	229.280464		2	1024	338.833265	226.441699		-0.54%	-1.24%
3	2048	379.643726	337.452058		3	2048	399.559017	338.432284		5.25%	0.29%
4	4096	416.597537	383.625364		4	4096	428.252403	396.137506		2.80%	3.26%
5	6000	431.198241	384.384384		5	6000	457.744577	406.06275		6.16%	5.64%
6	8192	462.811252	427.292573		6	8192	474.791032	428.281563		2.59%	0.23%
7	10000	464.258731	429.050294		7	10000	483.7643	446.849381		4.20%	4.15%
8	10013	465.199701	429.824179		8	10013	464.904407	428.72184		-0.06%	-0.26%
9	10240	477.07359	428.853737		9	10240	485.317024	444.902586		1.73%	3.74%
10	11000	473.038785	430.778663		10	11000	488.161438	453.462162		3.20%	5.27%
11	12000	474.342475	432.594814		11	12000	490.532418	458.427653		3.41%	5.97%
12	16384	487.468854	473.611576		12	16384	488.154406	476.264631		0.14%	0.56%
13	20000	482.029793	465.666186		13	20000	482.147092	483.886193		0.02%	3.91%
14	24000	478.368093	474.159464		14	24000	478.364948	491.447921		0.00%	3.65%
15	32000	476.523796	473.18868		15	32000	476.523796	474.398962		0.00%	0.26%
16	32768	476.104723	477.493634		16	32768	476.704463	477.330606		0.13%	-0.03%
17	36864	477.900663	475.472787		17	36864	477.973279	475.728454		0.02%	0.05%
18	40960	477.707561	475.559064		18	40960	478.445017	476.088067		0.15%	0.11%
19	45056	479.169812	475.865134		19	45056	479.143266	475.878202		-0.01%	0.00%
20	49152	477.804907	475.382982		20	49152	477.868404	475.976377		0.01%	0.12%
21	65536	481.274125	478.171806		21	65536	481.537733	478.703926		0.05%	0.11%
22	66000	481.64652	480.095457		22	66000	481.856013	480.466388		0.04%	0.08%
23	68608	481.745774	479.034704		23	68608	481.917596	478.856209		0.04%	-0.04%
24	80000	483.409361	480.356529		24	80000	483.330481	480.375277		-0.02%	0.00%
25	98304	480.736301	481.396882		25	98304	480.789858	481.320143		0.01%	-0.02%
```

NCU profiler shows lower DRAM fetches with the new kernel:

![image](https://github.com/user-attachments/assets/f3606725-d8fc-4ea5-ae6d-9c188bf32d72)

NCU reports about 12% elapsed time reduction in this kernel alone compared to baseline (and because of other kernels that are run, the overall backward pass time as seen by the user gets reduced by 5%).

I compared the binary size increase by running `python setup.py develop` before and after and diffing the .so files:

![image](https://github.com/user-attachments/assets/8e6cee2e-3c7a-4fa4-8836-954047ce8ffc)

libtorch_cuda.so goes from 274,752,224 bytes to 274,787,072 bytes. The increase in size is 34kB which is about 0.01%.

I measured the compilation time for incremental development:

```
touch ./aten/src/ATen/native/cuda/SoftMax.cu
time python setup.py develop
real    0m10.083s
user    0m8.197s
sys     0m3.149s
```

Note that this uses `ccache` and does a bunch of copies and is not just measuring the `nvcc` time. I measured the `nvcc` time separately by capturing the `nvcc` command shown in [1] below and running it on the baseline and modified kernels:

```
# baseline nvcc time for SoftMax.cu
real    0m35.341s
user    0m33.801s
sys     0m1.289s

# this PR's nvcc time for SoftMax.cu
real    0m36.513s
user    0m34.722s
sys     0m1.408s
```

So the `nvcc` time increases by about 1 second, or ~3% of the baseline.

[1] `nvcc` command is here:
```
# This is the nvcc command
/usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DAT_PER_OPERATOR_HEADERS -DFLASHATTENTION_DISABLE_ALIBI -DFMT_HEADER_ONLY=1 -DHAVE_MALLOC_USABLE_SIZE=1 -DHAVE_MMAP=1 -DHAVE_SHM_OPEN=1 -DHAVE_SHM_UNLINK=1 -DMINIZ_DISABLE_ZIP_READER_CRC32_CHECKS -DONNXIFI_ENABLE_EXT=1 -DONNX_ML=1 -DONNX_NAMESPACE=onnx_torch -DTORCH_CUDA_BUILD_MAIN_LIB -DTORCH_CUDA_USE_NVTX3 -DUSE_C10D_GLOO -DUSE_C10D_NCCL -DUSE_CUDA -DUSE_DISTRIBUTED -DUSE_EXTERNAL_MZCRC -DUSE_FLASH_ATTENTION -DUSE_MEM_EFF_ATTENTION -DUSE_NCCL -DUSE_RPC -DUSE_TENSORPIPE -D_FILE_OFFSET_BITS=64 -Dtorch_cuda_EXPORTS -I/home/ahmads/personal/pytorch/build/aten/src -I/home/ahmads/personal/pytorch/aten/src -I/home/ahmads/personal/pytorch/build -I/home/ahmads/personal/pytorch -I/home/ahmads/personal/pytorch/cmake/../third_party/benchmark/include -I/home/ahmads/personal/pytorch/third_party/onnx -I/home/ahmads/personal/pytorch/build/third_party/onnx -I/home/ahmads/personal/pytorch/nlohmann -I/home/ahmads/personal/pytorch/aten/src/THC -I/home/ahmads/personal/pytorch/aten/src/ATen/cuda -I/home/ahmads/personal/pytorch/third_party/fmt/include -I/home/ahmads/personal/pytorch/aten/src/ATen/../../../third_party/cutlass/include -I/home/ahmads/personal/pytorch/aten/src/ATen/../../../third_party/cutlass/tools/util/include -I/home/ahmads/personal/pytorch/build/caffe2/aten/src -I/home/ahmads/personal/pytorch/aten/src/ATen/.. -I/home/ahmads/personal/pytorch/build/nccl/include -I/home/ahmads/personal/pytorch/c10/cuda/../.. -I/home/ahmads/personal/pytorch/c10/.. -I/home/ahmads/personal/pytorch/third_party/tensorpipe -I/home/ahmads/personal/pytorch/build/third_party/tensorpipe -I/home/ahmads/personal/pytorch/third_party/tensorpipe/third_party/libnop/include -I/home/ahmads/personal/pytorch/torch/csrc/api -I/home/ahmads/personal/pytorch/torch/csrc/api/include -isystem /home/ahmads/personal/pytorch/build/third_party/gloo -isystem /home/ahmads/personal/pytorch/cmake/../third_party/gloo -isystem /home/ahmads/personal/pytorch/cmake/../third_party/tensorpipe/third_party/libuv/include -isystem /home/ahmads/personal/pytorch/cmake/../third_party/googletest/googlemock/include -isystem /home/ahmads/personal/pytorch/cmake/../third_party/googletest/googletest/include -isystem /home/ahmads/personal/pytorch/third_party/protobuf/src -isystem /home/ahmads/personal/pytorch/third_party/XNNPACK/include -isystem /home/ahmads/personal/pytorch/third_party/ittapi/include -isystem /home/ahmads/personal/pytorch/cmake/../third_party/eigen -isystem /usr/local/cuda/include -isystem /home/ahmads/personal/pytorch/torch/include -isystem /home/ahmads/personal/pytorch/third_party/ideep/include -isystem /home/ahmads/personal/pytorch/torch/include/oneapi/dnnl -isystem /home/ahmads/personal/pytorch/INTERFACE -isystem /home/ahmads/personal/pytorch/third_party/nlohmann/include -isystem /home/ahmads/personal/pytorch/third_party/NVTX/c/include -isystem /home/ahmads/personal/pytorch/cmake/../third_party/cudnn_frontend/include -DLIBCUDACXX_ENABLE_SIMPLIFIED_COMPLEX_OPERATIONS -D_GLIBCXX_USE_CXX11_ABI=1 -Xfatbin -compress-all -DONNX_NAMESPACE=onnx_torch -gencode arch=compute_90,code=sm_90 -Xcudafe --diag_suppress=cc_clobber_ignored,--diag_suppress=field_without_dll_interface,--diag_suppress=base_class_has_different_dll_interface,--diag_suppress=dll_interface_conflict_none_assumed,--diag_suppress=dll_interface_conflict_dllexport_assumed,--diag_suppress=bad_friend_decl --expt-relaxed-constexpr --expt-extended-lambda  -Wno-deprecated-gpu-targets --expt-extended-lambda -DCUB_WRAPPED_NAMESPACE=at_cuda_detail -DCUDA_HAS_FP16=1 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -O3 -DNDEBUG -std=c++17 -Xcompiler=-fPIC -DTORCH_USE_LIBUV -DCAFFE2_USE_GLOO -Xcompiler -Wall -Wextra -Wdeprecated -Wno-unused-parameter -Wno-missing-field-initializers -Wno-array-bounds -Wno-unknown-pragmas -Wno-strict-overflow -Wno-strict-aliasing -Wunused-function -Wunused-variable -Wunused-but-set-variable -Wno-maybe-uninitialized -MD -MT caffe2/CMakeFiles/torch_cuda.dir/__/aten/src/ATen/native/cuda/SoftMax.cu.o -MF caffe2/CMakeFiles/torch_cuda.dir/__/aten/src/ATen/native/cuda/SoftMax.cu.o.d -x cu -c /home/ahmads/personal/pytorch/aten/src/ATen/native/cuda/SoftMax.cu -o caffe2/CMakeFiles/torch_cuda.dir/__/aten/src/ATen/native/cuda/SoftMax.cu.o
```





",open,2025-01-28T20:26:34Z,,,release notes: cuda,main,softmax1,4,157,12,13,2,1,"ngimel, ahmadsharif1","COMMENTED, COMMENTED",False,https://api.github.com/repos/pytorch/pytorch/issues/145866
145865,[WIP] Add test_torchinductor_opinfo.py to triton-cpu tests,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #145865

I'm guessing this isn't going to pass, but I want to see how long the CI takes.",open,2025-01-28T20:21:09Z,,,"ciflow/inductor, keep-going",gh/davidberard98/335/base,gh/davidberard98/335/head,1,1,1,1,5,0,,,True,https://api.github.com/repos/pytorch/pytorch/issues/145865
145863,Cleanup VS 2019 refs in pytorch,"Related to: https://github.com/pytorch/pytorch/issues/128835
Follow up on PR: https://github.com/pytorch/pytorch/pull/145319",open,2025-01-28T19:26:12Z,,,"ciflow/binaries, ciflow/trunk, release notes: releng, test-config/default",main,cleanup_vs_2019,10,9,130,4,8,1,"Skylion007, huydhn, malfet, huydhn, huydhn, atalman","APPROVED, COMMENTED, APPROVED, APPROVED, COMMENTED, APPROVED",False,https://api.github.com/repos/pytorch/pytorch/issues/145863
145854,[c10d][UCC] Support coalesing in `c10d::ProcessGroupUCC` through alltoallv,"# What 
Add support for coalescing communication in `ProcessGroupUCC`, by implementing the methods `startCoalescing` and `endCoalescing`.

We need to impose several restrictions to the coalesced group:
1) we can only coalesce `send` and `recv` ops
2) we can only coalesce one `send` and one `recv` maximum per pair of ranks
3) all ranks must participate in the `startCoalescing` and `endCoalescing` calls, even if the group is empty.
4) we do not support tags for coalesced groups.

# Why

Despite the above restrictions, we cover a number of useful data patterns, such as ring p2p, allgather, broadcast, alltoall, etc. Those data patterns or other custom ones are conveniently written in terms of coalesced send/recv calls, which this patch makes possible.

Recall that for a p2p bidirectional transfer, the send and recv need to be coalesced to enjoy full bidirectional bandwidth

# How

Since UCC does not natively support Coalescing, we implement it at the ProcessGroup level. The implementation relies on calling UCC's alltoallv, setting the count to `0` and displacement to `nullptr` for ranks that do not exchange data.

cc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o",open,2025-01-28T17:16:51Z,,,"oncall: distributed, triaged, open source, release notes: distributed (c10d)",main,ucc_coalesced_a2av,3,151,21,2,2,0,,,False,https://api.github.com/repos/pytorch/pytorch/issues/145854
145853,cmake: fix detection logic when using system XNNPACK,"This commit makes the following improvements:

* The ""elseif"" branch now only runs when USE_XNNPACK is set.
* Use ""REQUIRED"" to enforce the existence of XNNPACK libraries, and remove the erroneous if statement ('or' should be 'OR').
* libmicrokernels-prod is built statically in XNNPACK [1], change in pytorch side accordingly.

[1]: https://github.com/google/XNNPACK/blob/d7f398ee5e135ef4f7045802eea973cc6cb26c6c/CMakeLists.txt#L819

",open,2025-01-28T17:05:50Z,,,"triaged, open source, topic: not user facing",main,fix-system-xnnpack,1,4,7,1,2,0,,,False,https://api.github.com/repos/pytorch/pytorch/issues/145853
145850,Skip search for MKL on ARM cpus,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #145871
* #145870
* __->__ #145850

It will not find it anyway and makes a bit easier parsing thru CMake log on non-x86 systems",open,2025-01-28T16:40:35Z,,,topic: not user facing,gh/malfet/155/base,gh/malfet/155/head,1,5,0,1,1,0,,,False,https://api.github.com/repos/pytorch/pytorch/issues/145850
145847,Improve the guards on some cpp wrapper tests ,"Since they're the CPU CPP wrapper tests, they should only run if the CPU backend we're using is the CPP one.

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang @aakhundov",open,2025-01-28T16:22:39Z,,,"triaged, open source, topic: not user facing, module: inductor",main,charliew/test-skip,1,1,1,1,5,0,desertfire,APPROVED,False,https://api.github.com/repos/pytorch/pytorch/issues/145847
145834,[inductor] Add features to docstring_linter (see #142496),"## Improvements to `docstring_linter`

* Add a ""grandfather list"" of existing undocumented classes and functions (`--grandfather`, `--grandfather-tolerance`, `--no-grandfather`, `--write-grandfather`)
* In classes, now just one of the class itself or its `__init__()` method needs to be documented (`--lint-init` turns the old behavior back on)
* Now classes and functions defined local to other functions do not need to be documented (`--lint-local` turns the old behavior back on)
* New `--report` flag produces a compact report of long, undocumented classes or function definitions: see attached example run over all pytorch: [pytorch-docs.json](https://github.com/user-attachments/files/18455981/pytorch-docs.json)

## Help text

```
$ python tools/linter/adapters/docstring_linter.py --help
usage: docstring_linter.py [-h] [-l] [-v] [--grandfather GRANDFATHER] [--grandfather-tolerance GRANDFATHER_TOLERANCE] [--lint-init]
                           [--lint-local] [--lint-protected] [--max-class MAX_CLASS] [--max-def MAX_DEF]
                           [--min-docstring MIN_DOCSTRING] [--no-grandfather] [--report] [--write-grandfather]
                           [files ...]

`docstring_linter` reports on long functions, methods or classes without docstrings

positional arguments:
  files                 A list of files or directories to lint

optional arguments:
  -h, --help            show this help message and exit
  -l, --lintrunner      Run for lintrunner and print LintMessages which aren't edits
  -v, --verbose         Print more debug info
  --grandfather GRANDFATHER, -g GRANDFATHER
                        Set the grandfather list
  --grandfather-tolerance GRANDFATHER_TOLERANCE, -t GRANDFATHER_TOLERANCE
                        Tolerance for grandfather sizes, in percent
  --lint-init, -i       Lint __init__ and class separately
  --lint-local, -o      Lint definitions inside other functions
  --lint-protected, -p  Lint functions, methods and classes that start with _
  --max-class MAX_CLASS, -c MAX_CLASS
                        Maximum number of lines for an undocumented class
  --max-def MAX_DEF, -d MAX_DEF
                        Maximum number of lines for an undocumented function
  --min-docstring MIN_DOCSTRING, -s MIN_DOCSTRING
                        Minimum number of characters for a docstring
  --no-grandfather, -n  Disable the grandfather list
  --report, -r          Print a report on all classes and defs
  --write-grandfather, -w
                        Rewrite the grandfather list
```

---

Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #144622
* #144621
* __->__ #145834
* #144620

",open,2025-01-28T12:03:29Z,,,"module: lint, open source, better-engineering, topic: not user facing, suppress-api-compatibility-check, suppress-bc-linter",gh/rec/128/base,gh/rec/128/head,9,1199,191,5,2,0,,,False,https://api.github.com/repos/pytorch/pytorch/issues/145834
145833,Implement KL for studentT,"Fixes #145729
",open,2025-01-28T10:37:07Z,,,"triaged, open source",main,fix/student_t_kl,1,66,0,1,2,0,,,False,https://api.github.com/repos/pytorch/pytorch/issues/145833
145832,[DO NOT MERGE] [TESTING] [ROCm] Triton cherry-picks for AMD backend perf optimisation,"Testing for rc/3.2.x PR

cc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @hongxiayang @naromero77amd",open,2025-01-28T10:21:32Z,,,"module: rocm, open source, ciflow/trunk, topic: not user facing, ciflow/periodic, ciflow/inductor, ciflow/inductor-perf-compare, ciflow/rocm, ciflow/inductor-rocm, ciflow/inductor-periodic",main,rc32-cps,2,2,2,2,1,0,,,True,https://api.github.com/repos/pytorch/pytorch/issues/145832
145822,Remove unneeded CUDA logic from _create_build_env,"Because FindCUDAToolkit.cmake has that logic.
",open,2025-01-28T03:34:42Z,,,"triaged, open source, topic: not user facing",main,cmake_find,1,0,10,1,1,0,,,False,https://api.github.com/repos/pytorch/pytorch/issues/145822
145819,Replace distutils.version with  copied looseversion,"distutils was deprecated.
",open,2025-01-28T03:17:56Z,,,"triaged, open source, topic: not user facing",main,looseversion,1,92,2,1,2,1,albanD,COMMENTED,False,https://api.github.com/repos/pytorch/pytorch/issues/145819
145812,[cutlass backend] check against arch >= 100,"Summary:
Want to add a guard against silent fallback to SM90.

GenerateSM100 was just added 3 days ago. https://github.com/NVIDIA/cutlass/blame/main/python/cutlass_library/generator.py#L8896

It should show up in CUTLASS 3.8 (not pinned yet).

Test Plan: ci

Differential Revision: D68748705




cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang @aakhundov",open,2025-01-28T02:03:04Z,,,"fb-exported, ciflow/trunk, topic: not user facing, module: inductor, ciflow/inductor",main,export-D68748705,1,21,2,1,7,0,"chenyang78, ColinPeppler, ColinPeppler, Aidyn-A, henrylhtsang, Aidyn-A, henrylhtsang","APPROVED, COMMENTED, APPROVED, CHANGES_REQUESTED, COMMENTED, COMMENTED, COMMENTED",False,https://api.github.com/repos/pytorch/pytorch/issues/145812
145811,[AsyncMM] re-enable and adapt to cutlass 3.6.0 (#144011),"Summary:




cc H-Huang awgu kwen2501 wanchaol fegin fduwjj wz337 wconstab d4l3k c-p-i-o

imported-using-ghimport

Test Plan: Imported from OSS

Differential Revision: D68734003

Pulled By: yifuwang




cc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o",open,2025-01-28T01:59:16Z,,,"oncall: distributed, fb-exported, release notes: distributed (c10d)",main,export-D68734003,2,262,119,1,4,0,,,False,https://api.github.com/repos/pytorch/pytorch/issues/145811
145798,[will-not-merge] tuning,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* (to be filled)



cc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o",open,2025-01-27T23:43:34Z,,,"oncall: distributed, release notes: distributed (c10d)",gh/yifuwang/192/base,gh/yifuwang/192/head,3,49,59,1,1,0,,,True,https://api.github.com/repos/pytorch/pytorch/issues/145798
145797,[Async-TP] improve algo selection,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #145798
* __->__ #145797
* #145796
* #145795
* #145794

",open,2025-01-27T23:43:30Z,,,oncall: distributed,gh/yifuwang/191/base,gh/yifuwang/191/head,2,134,87,1,2,0,,,True,https://api.github.com/repos/pytorch/pytorch/issues/145797
145796,[Async-TP] _pipelined_multi_all_gather_and_consume reduce overhead,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #145798
* #145797
* __->__ #145796
* #145795
* #145794

",open,2025-01-27T23:43:25Z,,,oncall: distributed,gh/yifuwang/190/base,gh/yifuwang/190/head,1,15,18,1,2,0,,,True,https://api.github.com/repos/pytorch/pytorch/issues/145796
145795,[AsyncMM] preliminary tuning,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #145798
* #145797
* #145796
* __->__ #145795
* #145794

",open,2025-01-27T23:43:21Z,,,"oncall: distributed, release notes: distributed (c10d)",gh/yifuwang/189/base,gh/yifuwang/189/head,1,57,4,1,1,0,,,True,https://api.github.com/repos/pytorch/pytorch/issues/145795
145794,[Async-TP] Port _fused_all_gather_matmul_native to cpp to reduce launching overhead,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #145798
* #145797
* #145796
* #145795
* __->__ #145794

`_fused_all_gather_matmul_native` schedules multiple tasks (e.g., kernel, copy engine transfers, and stream_write_value32_) onto the GPU. Previously, `fused_all_gather_matmul_native` was implemented in Python, and issuing most of these tasks incurred dispatcher overhead. When the problem size is small, the CPU overhead can exceed the GPU’s execution time. While this may be acceptable in workloads where the CPU runs ahead of the GPU, it still isn’t ideal.

This PR reduces CPU overhead by porting `_fused_all_gather_matmul_native` to C++. Specifically, it eliminates dispatcher overhead for:
- `aten.split` (calling `aten.narrow` × `world_size` times)
- `symm_mem::stream_write_value32_` × `world_size` times

<img width=""842"" alt=""image"" src=""https://github.com/user-attachments/assets/176ebc89-a2e1-4c07-b340-c2d4422def09"" />
<img width=""455"" alt=""image"" src=""https://github.com/user-attachments/assets/3768f0c1-876f-4c66-bd22-89aa80d0889c"" />",open,2025-01-27T23:43:16Z,,,"oncall: distributed, release notes: distributed (c10d)",gh/yifuwang/188/base,gh/yifuwang/188/head,4,135,46,1,1,1,lw,APPROVED,True,https://api.github.com/repos/pytorch/pytorch/issues/145794
145779,[CUDNN][CUDNN V8 API] Allow user-specified CUDNN V8 API benchmarking technique,"Useful for debugging apparent ""regressions"" when using cuDNN autotuning (""benchmarking"")

cc @csarofeen @ptrblck @xwang233",open,2025-01-27T21:33:12Z,,,"module: cudnn, triaged, open source, topic: not user facing",main,cudnnv8technique,1,73,6,3,1,0,"Skylion007, eqy","COMMENTED, COMMENTED",False,https://api.github.com/repos/pytorch/pytorch/issues/145779
145778,NJT support for cat() on the ragged dim,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #145778

Requested [here](https://github.com/pytorch/pytorch/issues/118107#issuecomment-2615705795).

There's still a fair amount of work left. TODO:
* Fix the backwards pass (need NJT-specific derivative formula, possibly `narrow()` on the ragged dim)
* Fix data-dependency errors in forward + torch.compile() due to `unbind()` usage",open,2025-01-27T21:28:00Z,,,topic: not user facing,gh/jbschlosser/226/base,gh/jbschlosser/226/head,3,161,6,1,1,0,Skylion007,COMMENTED,False,https://api.github.com/repos/pytorch/pytorch/issues/145778
145776,Update to NCCL 2.25.1 for 12.8,"https://github.com/pytorch/pytorch/issues/145570

follow up for https://github.com/pytorch/pytorch/pull/145567/files",open,2025-01-27T21:16:42Z,,,"triaged, open source, topic: not user facing",main,nccl-update-12.8,2,4,0,1,3,1,"eqy, Skylion007","APPROVED, CHANGES_REQUESTED",False,https://api.github.com/repos/pytorch/pytorch/issues/145776
145774,[POC] flat_apply HOP,"[no-ci]

Fixes #ISSUE_NUMBER


cc @ezyang @SherlockNoMad @EikanWang @jgong5 @wenzhe-nrv",open,2025-01-27T20:46:12Z,,,"release notes: fx, fx",main,flat_apply,3,183,0,2,1,0,,,True,https://api.github.com/repos/pytorch/pytorch/issues/145774
145772,Implement serializable getattr support for tensor subclasses,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #145772

builtins.getattr is not serializable, so we replace it with a custom op that has more refined schema. 

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames

Differential Revision: [D68899421](https://our.internmc.facebook.com/intern/diff/D68899421)",open,2025-01-27T20:19:13Z,,,"ciflow/trunk, module: dynamo, ciflow/inductor, release notes: export",gh/tugsbayasgalan/288/base,gh/tugsbayasgalan/288/head,5,49,30,8,4,1,"bdhirsh, bdhirsh, tugsbayasgalan, bdhirsh","COMMENTED, COMMENTED, COMMENTED, APPROVED",False,https://api.github.com/repos/pytorch/pytorch/issues/145772
145770,[torch][distributed] re-merge NCCLComm::split impl,"Summary:
These were originally forked between fbcode and oss. This has led to some drift, as bugfixes related to ncclCommSplit + non-blocking never made it to internal. Now we're hitting these bugs in monarch so it would be nice to fix.

Just upstream the forked code and delete the fb-only version.

Test Plan: Unit tests

Differential Revision: D68727854




cc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o",open,2025-01-27T20:12:04Z,,,"oncall: distributed, fb-exported, release notes: distributed (c10d)",main,export-D68727854,1,10,1,1,2,0,"Skylion007, kwen2501, suo","COMMENTED, COMMENTED, COMMENTED",False,https://api.github.com/repos/pytorch/pytorch/issues/145770
145751,[OSS] Update FileSystem methods to properly handle a string argument,"Summary: When testing, I tried to pass in a string argument to the FileSystem class' methods, which is a valid input, but the cast() that casted the string to a path wasn't working as was likely expected and was leading all the methods to fail with a string arg. Instead of a cast, a proper constructor should be used.

Test Plan: N6475361 methods don't throw an error with a string arg like they were previously

Differential Revision: D68713937




cc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o @LucasLLC @MeetVadakkanchery @mhorowitz @pradeepfn @ekr0",open,2025-01-27T18:57:16Z,,,"oncall: distributed, fb-exported, topic: not user facing, module: distributed_checkpoint",main,export-D68713937,1,19,6,1,3,1,"Skylion007, mhorowitz, Skylion007, ankitageorge, Skylion007, Skylion007, Skylion007, Skylion007, ankitageorge, Skylion007, Skylion007, ankitageorge, ankitageorge","COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, CHANGES_REQUESTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED",False,https://api.github.com/repos/pytorch/pytorch/issues/145751
145750,[dynamo] save/restore system random state more carefully,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #145750

Reattempt of https://github.com/pytorch/pytorch/pull/145435 since the state of the linked internal diff appears to be messed up.

Note: I have verified that the previously failing internal tests now pass internally.

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames

Differential Revision: [D68918404](https://our.internmc.facebook.com/intern/diff/D68918404)",open,2025-01-27T18:55:10Z,,,"Merged, Reverted, ciflow/trunk, topic: bug fixes, topic: not user facing, module: dynamo, ciflow/inductor, ci-no-td",gh/williamwen42/201/base,gh/williamwen42/201/head,10,156,12,10,15,1,"StrongerXi, jansel","APPROVED, CHANGES_REQUESTED",False,https://api.github.com/repos/pytorch/pytorch/issues/145750
145748,Set USE_CUFILE=1 by default and add pypi package to binary build matrix,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #146739
* __->__ #145748

",open,2025-01-27T18:47:54Z,,,"ciflow/trunk, topic: not user facing, ciflow/binaries_wheel",gh/mikaylagawarecki/311/base,gh/mikaylagawarecki/311/head,8,80,25,18,1,0,"Skylion007, mikaylagawarecki, atalman","COMMENTED, COMMENTED, COMMENTED",False,https://api.github.com/repos/pytorch/pytorch/issues/145748
145741,[BE]: Update Cutlass submodule to 3.8 candidate for SM100+ support,"Update CUTLASS submodule to 3.8 candidate for preliminary Blackwell support, without this PyTorch will not compile various CUTLASS kernels properly for Blackwell.",open,2025-01-27T17:39:08Z,,,"open source, better-engineering, ciflow/trunk, release notes: cuda, topic: not user facing",main,skylion007/update-cutlass-3-8-0-prc-0,1,1,1,3,15,1,albanD,CHANGES_REQUESTED,True,https://api.github.com/repos/pytorch/pytorch/issues/145741
145738,[Docs] Make comm handle wait & is_completed docs more clear for multi-stream,"Fixes #145713
Looks like a earlier fix on `wait()` (https://github.com/pytorch/pytorch/pull/143305) has been pushed but not reflected in the main branch, so feel free to revert my change on that part. 
Made coressponding clarifications on `is_completed`
cc @awgu @wconstab ",open,2025-01-27T16:31:18Z,,,"triaged, open source, release notes: distributed (c10d)",main,dist_docs,1,4,3,2,4,1,"awgu, wconstab, wconstab, Edenzzzz, Edenzzzz, wconstab, wconstab, Edenzzzz, Edenzzzz, Edenzzzz, wconstab, Edenzzzz, kwen2501, Edenzzzz","COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED",False,https://api.github.com/repos/pytorch/pytorch/issues/145738
145719,Fix support for nccl < 2.17,"Fix build failure with older (< 2.17) NCCL.

Refactoring NCCL version related code: 
1. Fix failure against old NCCL versions since #138527 cc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o ;
2. remove unused checks caused by unsupported NCCL version (since there's a static assert checking NCCL >= 2.7: #142023);
3. move NCCL macros to `torch/csrc/cuda/nccl.h` from various places and uniform some style (`#if` to `#ifdef`), which could improve maintainability of the NCCL part I hope.

Resolves #141914

cc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o",open,2025-01-27T06:09:47Z,,,"oncall: distributed, open source, ciflow/trunk, release notes: distributed (c10d), topic: not user facing",main,nccl-wraps,8,111,126,20,17,0,"wconstab, wconstab, c-p-i-o, c-p-i-o","COMMENTED, APPROVED, COMMENTED, APPROVED",False,https://api.github.com/repos/pytorch/pytorch/issues/145719
145717,add input shape check for _local_scalar_dense,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #145717


Fix https://github.com/pytorch/pytorch/issues/145066.
",open,2025-01-27T04:03:36Z,,,"open source, ciflow/trunk, topic: not user facing",gh/jiayisunx/55/base,gh/jiayisunx/55/head,2,7,0,2,1,0,,,False,https://api.github.com/repos/pytorch/pytorch/issues/145717
145700,[POC] [CPU][Inductor] Support INT8 SDPA based on CPP template,"This PR implements the Int8 SDPA CPU kernel based on CPP template following the RFC #144941.

ARs:

- [ ] INT8 SDPA patterns:
       - Done: FP32 with/wo mask, batch size >/= 1;
       - Remain: BF16 with/wo mask, batch size >/= 1.
- [ ] Add the `select_strategy` to generate the kernel with various parallel loop strategies.
- [ ] Enable and validate on related models, and make sure the good accuracy/perf.
- [ ] Refactor the codes, and make best use of the common parts.
- [ ] Add necessary comments.


cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang @aakhundov",open,2025-01-26T07:15:35Z,,,"open source, topic: not user facing, module: inductor, ciflow/inductor",main,int8_sdpa_template,6,2390,8,3,1,0,,,True,https://api.github.com/repos/pytorch/pytorch/issues/145700
145694,feat: improve indexing error messages,"Improves the error messages when certain tensor operations fail, for example

`RuntimeError: index_select(): Expected dtype int32 or int64 for index` will now say
e.g.
`RuntimeError: index_select(): Expected dtype int32 or int64 for index, got float32`
",open,2025-01-26T02:35:49Z,,,"triaged, open source, release notes: mps",main,mattb.feat.improve-indexing-error-msg,1,4,3,1,2,0,,,False,https://api.github.com/repos/pytorch/pytorch/issues/145694
145685,[Easy] update pip sources for ROCm in nightly pull tool,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #145685



cc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd",open,2025-01-25T18:44:59Z,,,"module: rocm, open source, topic: not user facing, ciflow/rocm",gh/XuehaiPan/239/base,gh/XuehaiPan/239/head,1,8,0,3,3,0,,,False,https://api.github.com/repos/pytorch/pytorch/issues/145685
145681,Avoid data-dependent errors by runtime assert substitution.,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #142372
* __->__ #145681

This PR adds a simplification method using runtime asserts, whenever we
are about to raise a data-dependent error. We use the recorded runtime
asserts as a source of knowledge for substituting the free symbols in
the given expression.

This is useful for avoiding data-dependent errors, specifically avoiding
guarding on expressions with unbacked integers, that are deducible from
the past runtime asserts.

cc @ezyang @SherlockNoMad @EikanWang @jgong5 @wenzhe-nrv @voznesenskym @penguinwu @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @jiayisunx @chenyang78 @kadeng @chauhang @amjames",open,2025-01-25T16:08:37Z,,,"open source, release notes: fx, fx, module: dynamo, ciflow/inductor",gh/ysiraichi/81/base,gh/ysiraichi/81/head,2,88,0,1,6,0,,,False,https://api.github.com/repos/pytorch/pytorch/issues/145681
145680,Add icdf to Gamma dist,"Fixes #145679
",open,2025-01-25T15:30:52Z,,,"triaged, open source",main,fix/chi2,1,5,0,1,5,0,,,False,https://api.github.com/repos/pytorch/pytorch/issues/145680
145677,[1/N] Improve typing in  torch/_C/__init__.pyi.in,"Fixes #ISSUE_NUMBER


cc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o @mcarilli @ptrblck @leslie-fang-intel @jgong5",open,2025-01-25T10:39:28Z,,,"oncall: distributed, triaged, open source, module: amp (automated mixed precision), ciflow/trunk, topic: not user facing",main,C_init_py2,1,183,183,3,12,2,"guangyey, cyyever, guangyey, XuehaiPan, XuehaiPan, albanD, cyyever, cyyever, XuehaiPan, XuehaiPan, cyyever, cyyever, XuehaiPan, XuehaiPan, cyyever, cyyever, XuehaiPan, cyyever, XuehaiPan, cyyever, cyyever, Skylion007, albanD, cyyever","COMMENTED, COMMENTED, APPROVED, COMMENTED, APPROVED, CHANGES_REQUESTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED",False,https://api.github.com/repos/pytorch/pytorch/issues/145677
145674,Convert Tensor lr to 0-dim as needed for the optimizer to normally work,"Fixes #145461
",open,2025-01-25T06:52:07Z,,,"triaged, open source, release notes: optim",main,fix-tensor-lr-check,4,33,23,10,12,0,"janeyx99, janeyx99, Tony-Y, janeyx99, Tony-Y, Tony-Y, janeyx99, Tony-Y, Tony-Y, janeyx99, Tony-Y, janeyx99, Tony-Y","COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED",False,https://api.github.com/repos/pytorch/pytorch/issues/145674
145652,[c10d] implement ReduceOp.unbox(),"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #145652
* #144886

```python
>>> import torch
>>> op = torch.classes.c10d.ReduceOp()
>>> op
<torch.ScriptObject object at 0x5b688b0>
>>> torch.distributed.ReduceOp.unbox(op)
<torch.distributed.distributed_c10d.ReduceOp object at 0x7fd9e3066ff0>
>>> torch.distributed.ReduceOp.unbox(op).op
<RedOpType.SUM: 0>
```


cc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o",open,2025-01-24T20:55:44Z,,,"oncall: distributed, release notes: distributed (c10d)",gh/yifuwang/187/base,gh/yifuwang/187/head,1,7,1,1,1,0,,,True,https://api.github.com/repos/pytorch/pytorch/issues/145652
145651,Add link to non_blocking/pinmem tutorial in `Tensor.to` docstrings,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #145651



cc @svekars @brycebortree @sekyondaMeta @AlannaBurke",open,2025-01-24T20:48:22Z,,,"module: docs, topic: not user facing",gh/vmoens/19/base,gh/vmoens/19/head,1,11,6,1,1,0,svekars,APPROVED,False,https://api.github.com/repos/pytorch/pytorch/issues/145651
145648,Test distributions compilation,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #145648
* #145647
* #145646
* #145645

",open,2025-01-24T20:04:23Z,,,topic: not user facing,gh/vmoens/18/base,gh/vmoens/18/head,1,20,0,2,2,0,,,False,https://api.github.com/repos/pytorch/pytorch/issues/145648
145647,"Fix distributions dynamo tracing (`__init__`, sample and log_prob)","Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #145648
* __->__ #145647
* #145646
* #145645

",open,2025-01-24T20:04:15Z,,,"module: dynamo, ciflow/inductor",gh/vmoens/17/base,gh/vmoens/17/head,8,94,35,2,3,0,,,False,https://api.github.com/repos/pytorch/pytorch/issues/145647
145646,refactor Distribution class for compile support,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #145648
* #145647
* __->__ #145646
* #145645

",open,2025-01-24T20:04:07Z,,,,gh/vmoens/16/base,gh/vmoens/16/head,2,27,7,1,2,0,,,False,https://api.github.com/repos/pytorch/pytorch/issues/145646
145645,Parametrize distributions tests,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #145648
* #145647
* #145646
* __->__ #145645

",open,2025-01-24T20:03:59Z,,,topic: not user facing,gh/vmoens/15/base,gh/vmoens/15/head,1,878,745,1,1,0,,,False,https://api.github.com/repos/pytorch/pytorch/issues/145645
145636,Simplify functional composition in _aot_autograd/dispatch_and_compile_graph.py,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #145636

",open,2025-01-24T18:34:01Z,,,"open source, topic: not user facing, ciflow/inductor",gh/rec/125/base,gh/rec/125/head,1,115,60,10,3,0,,,False,https://api.github.com/repos/pytorch/pytorch/issues/145636
145628,Add __all__ for torch.nn.init,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #145628

",open,2025-01-24T17:17:45Z,,,,gh/mikaylagawarecki/310/base,gh/mikaylagawarecki/310/head,2,31,3,1,2,0,,,True,https://api.github.com/repos/pytorch/pytorch/issues/145628
145622,[AOTI] Update test runner to use the new APIs,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #145622

Summary: Switch to the newer aoti_compile_and_package APIs. Some tests still kept using legacy APIs, and will follow up with internal test refactoring.

cc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @ColinPeppler

Differential Revision: [D69306100](https://our.internmc.facebook.com/intern/diff/D69306100)",open,2025-01-24T15:42:41Z,,,"oncall: distributed, ciflow/trunk, topic: not user facing, module: inductor, ciflow/inductor, keep-going",gh/desertfire/531/base,gh/desertfire/531/head,7,133,87,6,3,0,,,False,https://api.github.com/repos/pytorch/pytorch/issues/145622
145612,[BE] Bump huggingface pin,,open,2025-01-24T13:47:11Z,,,"topic: not user facing, ciflow/inductor, ciflow/inductor-periodic",main,desertfire/update_hf_pin,1,1,1,1,1,0,huydhn,APPROVED,False,https://api.github.com/repos/pytorch/pytorch/issues/145612
145606,[BE][CI] bump ruff to 0.9.5,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #145606
* #144546
* #144569
* #145148
* #146509

",open,2025-01-24T12:12:33Z,,,"open source, ciflow/trunk, topic: not user facing",gh/XuehaiPan/238/base,gh/XuehaiPan/238/head,1,2,2,7,15,1,malfet,APPROVED,False,https://api.github.com/repos/pytorch/pytorch/issues/145606
145605,WIP error_prop sc,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #145605



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",open,2025-01-24T11:44:25Z,,,"module: dynamo, ciflow/inductor",gh/IvanKobzarev/98/base,gh/IvanKobzarev/98/head,3,172,1,1,2,0,,,False,https://api.github.com/repos/pytorch/pytorch/issues/145605
145603,"[dynamo] refactor dynamo__custom_eval_frame to C++, refactor SKIP_CODE[_RECURSIVE]","Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #146355
* __->__ #145603



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",open,2025-01-24T11:19:43Z,,,"topic: not user facing, module: dynamo, ciflow/inductor",gh/williamwen42/200/base,gh/williamwen42/200/head,9,376,325,9,2,1,"jansel, jansel, anijain2305","CHANGES_REQUESTED, APPROVED, APPROVED",False,https://api.github.com/repos/pytorch/pytorch/issues/145603
145600,"Add device support for chunk_cat, all_gather_copy_in, and split_with_…","…sizes_copy in _fsdp_collectives.py


cc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o",open,2025-01-24T09:29:41Z,,,"oncall: distributed, open source, release notes: distributed (fsdp)",main,fsdp_add_device,1,3,0,2,2,0,awgu,APPROVED,False,https://api.github.com/repos/pytorch/pytorch/issues/145600
145595,[micro_pipeline_tp] support pattern matching row-wise scaled_mm with sharded scale,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #145595
* #145594



cc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang @aakhundov",open,2025-01-24T07:46:58Z,,,"oncall: distributed, release notes: distributed (pipeline), module: inductor, ciflow/inductor",gh/yifuwang/186/base,gh/yifuwang/186/head,3,123,35,1,1,0,,,True,https://api.github.com/repos/pytorch/pytorch/issues/145595
145594,[micro_pipeline_tp] add logging for all-gather-matmul fusion,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* (to be filled)



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang @aakhundov",open,2025-01-24T07:46:54Z,,,"module: inductor, ciflow/inductor",gh/yifuwang/185/base,gh/yifuwang/185/head,1,53,9,1,2,0,,,True,https://api.github.com/repos/pytorch/pytorch/issues/145594
145589,Replace decorators in UTs  to cover additional devices,"This is follow-up of https://github.com/pytorch/pytorch/pull/128584.  Covering additional files for execution.

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang @aakhundov",open,2025-01-24T05:51:12Z,,,"triaged, open source, topic: not user facing, module: inductor",main,expand_ops_execution,19,314,313,1,2,1,albanD,COMMENTED,False,https://api.github.com/repos/pytorch/pytorch/issues/145589
145584,layer_norm_kernel.cu eliminate the need for divisions for default vector size,"Eliminate the need for divisions in layernorm for default vector size.

The divisions performed in the online sum section can be replaced with immediate values as the vector size used is always 4. We special case this and leave the alternative in place in case other vector sizes are to be explored in the future.

For the combine step the division is always a power of 2 as long as the vector size used is a power of two so we can use shuffles to perform the division.

cc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd",open,2025-01-24T03:39:02Z,,,"module: rocm, triaged, open source, release notes: cuda, ciflow/rocm",main,remove-divisions-from-layernorm,1,35,6,2,3,1,"jeffdaily, jeffdaily","APPROVED, CHANGES_REQUESTED",True,https://api.github.com/repos/pytorch/pytorch/issues/145584
145562,[Not for land] hacking up mx,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #145562


https://www.internalfb.com/intern/paste/P1717686991/
",open,2025-01-24T00:08:37Z,,,ciflow/inductor,gh/drisspg/119/base,gh/drisspg/119/head,6,77,84,3,3,0,,,True,https://api.github.com/repos/pytorch/pytorch/issues/145562
145559,[dynamo][builtin-skipfile-cleanup] Remove collections,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #145559



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",open,2025-01-23T23:43:02Z,,,"ciflow/trunk, topic: not user facing, module: dynamo, ciflow/inductor, keep-going",gh/anijain2305/634/base,gh/anijain2305/634/head,1,0,2,17,1,0,,,True,https://api.github.com/repos/pytorch/pytorch/issues/145559
145540,[BE][hop] make it easier to use speculate_subgraph,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #145540


Previously, it's tricky to construct the required inputs for speculate_subgraph as discussed in https://github.com/pytorch/pytorch/issues/144805 because we have to program against variable trackers. This PR turns speculate_subgraph into a hop and uses _make_inlined to get the subgraphs. Now HOP developers only need to program against normal tensors and uses the hop speculate_subgraph to trace the subgraph.

Using the flex attention hop as an example, now we can write the sub graph tracing code with following code:
```python
@_make_inlinable
def _create_scalar(query: torch.Tensor):
    return query.new_empty([], dtype=torch.int32)

@_make_inlinable
def _create_scalars(query: torch.Tensor):
    return (
        _create_scalar(query),
        _create_scalar(query),
        _create_scalar(query),
        _create_scalar(query),
    )

# A normal python function that works on tensors and operators
@_make_inlinable
def _fn(query: torch.Tensor, fn: Callable):
    # since these return tensors are created in speculate_subgraph
    # it will not affect the current graph.
    score, *_ = torch.ops.higher_order.speculate_subgraph(
        _create_scalar, (query,)
    )
    (b, h, m, n), *_ = torch.ops.higher_order.speculate_subgraph(
        _create_scalars, (query,)
    )
    return torch.ops.higher_order.speculate_subgraph(
        fn, (score, b, h, m, n)
    )

# This is the driving logic, essentially, it inlines into _fn and returns whatever _fn returns
# in this case, higher_order.speculate_subgraph's output is 
# (tensor_var, tree_spec_var, UserdefinedObject(fx.Graph), UserdefinedObject(parent_proxy_to_child_proxy_map))
with TransformGetItemToIndex():
    (
        _body_output,
        _body_tree_spec,
        body_graph_var,
        body_lifted_freevars_var,
    ) = _make_inlined(tx, _fn)(query, fn).unpack_var_sequence(tx)
```
The other benefit is that: we can avoid putting unnecessary tensor calls before the hop because they can be put  in speculate_subgraph, which also addresses the issue in https://github.com/pytorch/pytorch/issues/144803

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",open,2025-01-23T21:50:20Z,,,"topic: not user facing, module: dynamo, ciflow/inductor",gh/ydwu4/201/base,gh/ydwu4/201/head,3,128,36,4,5,0,"ydwu4, ydwu4, ydwu4, xmfan, zou3519, ydwu4, zou3519, zou3519, zou3519, zou3519, zou3519","COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED",False,https://api.github.com/repos/pytorch/pytorch/issues/145540
145527,fix intermediate debug information with cpp_wrapper,"Summary: before fix, code like:
```cpp
    aoti_torch_print_tensor_handle(buf0, ""after_launch - triton_poi_fused_randn_0 - buf0"");
    aoti_torch_print_tensor_handle(buf1, ""after_launch - triton_poi_fused_randn_0 - buf1"");
    printf(""[  after_launch - triton_poi_fused_randn_0 - 0: %ld  ]"", 0); printf(""
"");
    printf(""[  after_launch - triton_poi_fused_randn_0 - 1228800L: %ld  ]"", 1228800L); printf(""
"");
```
was generated, which is a syntax error.

Test Plan:
New unit test.



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang @aakhundov",open,2025-01-23T20:53:07Z,,,"ciflow/trunk, topic: bug fixes, module: inductor, ciflow/inductor, release notes: inductor, merging",main,exclamaforte/cpp-wrapper-debug,2,21,1,1,19,0,desertfire,APPROVED,False,https://api.github.com/repos/pytorch/pytorch/issues/145527
145526,[MPS] Add bilineard2d_aa implementation,"It works, but still struggling with correctness issues...",open,2025-01-23T20:52:21Z,,,"topic: improvements, release notes: mps, ciflow/mps",main,malfet/mps-add-bilineard2d-aa,4,95,1,1,2,0,,,True,https://api.github.com/repos/pytorch/pytorch/issues/145526
145523,inductor.config.descriptive_names = False is not actually supported,"Summary:
This config is not supported (it throws an error when set), and doesn't really make sense imo.


cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang @aakhundov",open,2025-01-23T20:44:23Z,,,"Merged, Reverted, ciflow/trunk, topic: deprecation, module: inductor, ciflow/inductor, release notes: inductor, ci-no-td",main,exclamaforte/remove-desc-names,1,2,3,1,13,0,eellison,APPROVED,False,https://api.github.com/repos/pytorch/pytorch/issues/145523
145521,General Changes for multi accelerators,"Intend to generailze the framework for multiple accelerators.
Major changes includes:
> Add TEST_CUDA & TEST_HPU condition for generalization at common place.
> Move "".cuda()"" to "".to(device_type)"" call


cc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o",open,2025-01-23T20:17:22Z,,,"oncall: distributed, triaged, open source, release notes: distributed (fsdp)",main,common_fsdp,17,744,602,2,5,0,"wconstab, rahulsingh-intel, ankurneog, ankurneog, ankurneog, ankurneog, ankurneog, ankurneog","COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED",False,https://api.github.com/repos/pytorch/pytorch/issues/145521
145512,Add missing autoreleasepool around runUniqueGraph to prevent leaks,"References were held onto longer than needed. Added autoreleasepool around the runUniqueGraph to allow the memory to be freed.

Fixes #145151


cc @kulinseth @albanD @malfet @DenisVieriu97",open,2025-01-23T18:41:38Z,,,"module: memory usage, triaged, open source, module: mps, release notes: mps, ciflow/mps",main,dev/joona/unique_leak,1,7,4,4,3,1,"Skylion007, Skylion007, malfet","COMMENTED, COMMENTED, CHANGES_REQUESTED",False,https://api.github.com/repos/pytorch/pytorch/issues/145512
145510,Add istft option to align window for center = false,"Following up from https://github.com/pytorch/pytorch/pull/145324, also add the `align_to_window` parameter for the inverse short fourier transform op.

_PENDING: stft round trip tests for center = false and align_window = true_

Pr chain:
- [Advance past fc window for stft center #145437](https://github.com/pytorch/pytorch/pull/145437)
- [Add stft option to align window for center = false #145324](https://github.com/pytorch/pytorch/pull/145324)
- -> [Add istft option to align window for center = false](https://github.com/pytorch/pytorch/pull/145510)",open,2025-01-23T18:36:31Z,,,release notes: onnx,jz/stft-old-fc,jz/istft,9,113,22,30,1,0,Skylion007,COMMENTED,False,https://api.github.com/repos/pytorch/pytorch/issues/145510
145504,[DO NOT MERGE] Update workflow to use root user instead of jenkins user,cc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd,open,2025-01-23T17:41:06Z,,,"module: rocm, open source, topic: not user facing, ciflow/periodic, ciflow/rocm, ci-no-td, ciflow/inductor-rocm",main,patch-5,3,3,2,11,1,0,,,True,https://api.github.com/repos/pytorch/pytorch/issues/145504
145499,Remove truncated normal initialization for 16-bit (and lower) tensors,"Fixes #145498


cc @albanD @mruberry @jbschlosser @walterddr @mikaylagawarecki",open,2025-01-23T17:09:38Z,,,"module: nn, triaged, open source",main,fix-bf16-inits,1,2,0,1,3,0,,,False,https://api.github.com/repos/pytorch/pytorch/issues/145499
145492,[BE]: Fix OrderedSet equality oversight,"Test to see if #145489 even causes behavior difference in test suite
",open,2025-01-23T15:47:17Z,,,"open source, topic: bug fixes",main,skylion007/fix-orderedset-equality-2025-01-23,1,7,2,1,2,1,"Skylion007, eellison","COMMENTED, COMMENTED",True,https://api.github.com/repos/pytorch/pytorch/issues/145492
145487,"Remove unnecessary ""special linking"" for `BLAS_LIBRARIES`","Remove the ""special linking"" that involves listing `BLAS_LIBRARIES` thrice if `TH_BINARY_BUILD` is set, as it should not be any different from listing it just once.

The code seems to date back to commit cfcf2af95f91a88ec61cbcac8b30a718e7332aa5. The original code already listed `BLAS_LIBRARIES` thrice, but it provided no explanation for doing that — and without `TH_BINARY_BUILD`, BLAS was not linked at all.  The current version seems to originate in d6a8d28d6529a4f0b80a8c046ca9c36ca6c8b347 — and it already provided an `ELSE` clause listing `BLAS_LIBRARIES` only once.  From this, I suspect that it is probably an unnecessary leftover.

cc @malfet @seemethere",open,2025-01-23T15:13:03Z,,,"module: build, triaged, open source, topic: not user facing",main,blas-libs-special-linking,1,2,9,1,3,1,malfet,APPROVED,False,https://api.github.com/repos/pytorch/pytorch/issues/145487
145486,feat: add SVE dispatch for non-FBGEMM qembeddingbag,"Adds an accelerated kernel for `quantized::embedding_bag_byte` and integrates it with the dispatch mechanism.

The bulk of the SVE code has already been seen before and can be found here: https://github.com/pytorch/pytorch/pull/139753.

cc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @malfet @snadampal @milpuz01",open,2025-01-23T15:06:46Z,,,"module: cpu, triaged, open source, module: arm, release notes: quantization",main,qembeddingbag,3,565,128,1,5,0,,,False,https://api.github.com/repos/pytorch/pytorch/issues/145486
145476,Adapt Dynamo Tests to HPUs,"This PR is a continuation of https://github.com/pytorch/pytorch/pull/144387 .  Adapted two more files with the approach described below.

#MOTIVATION

We recently integrated support for Intel Gaudi devices (identified as 'hpu') into the common_device_type framework via the pull request at https://github.com/pytorch/pytorch/pull/126970. This integration allows tests to be automatically instantiated for Gaudi devices upon loading the relevant library. Building on this development, the current pull request extends the utility of these hooks by adapting selected CUDA tests to operate on Gaudi devices. Additionally, we have confirmed that these modifications do not interfere with the existing tests on CUDA devices.

Other accelerators can also extend the functionality by adding the device in the devices list. ( For eg: xpu )

#CHANGES

Create a separate class for test functions running on CUDA devices
Extend the functionality of these tests to include HPUs
Use instantiate_device_type_tests with targeted attributes to generate device-specific test instances within the new classes
Apply skipIfHPU decorator to bypass tests that are not yet compatible with HPU devices


cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",open,2025-01-23T10:20:41Z,,,"triaged, open source, topic: not user facing, module: dynamo",main,dynamo_changes1,2,268,251,4,4,0,,,False,https://api.github.com/repos/pytorch/pytorch/issues/145476
145475,[dynamo] added support to trace torch.cuda.is_current_stream_capturing,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #145475

enabled tracing torch.cuda.is_current_stream_capturing

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @kadeng @chauhang @amjames",open,2025-01-23T10:06:58Z,,,"topic: not user facing, module: dynamo, ciflow/inductor, ciflow/rocm",gh/chenyang78/1/base,gh/chenyang78/1/head,2,10,0,2,2,0,mlazos,COMMENTED,False,https://api.github.com/repos/pytorch/pytorch/issues/145475
145474,fix test_convolution error when use cudnn.flags,"Fixes #145473


cc @csarofeen @ptrblck @xwang233 @eqy",open,2025-01-23T10:01:29Z,,,"module: cudnn, module: convolution, triaged, open source, topic: not user facing",main,cudnn_flags_use_fix,1,2,2,1,1,1,eqy,COMMENTED,False,https://api.github.com/repos/pytorch/pytorch/issues/145474
145459,[AOTInductor] Align behavior between CPU and GPU,"Summary:
(1) Make sure CPU and GPU doesn't have different implementation and behavior when calling from the same path and API. Only difference between CPU and GPU after this PR should ONLY be the running hardware.
(2) This PR fixes the issue of memory access when it==constants_map.end()
(3) This PR resolves T179437596

Test Plan: buck2 run mode/dev sigmoid/inference/test:e2e_test_cpu

Differential Revision: D68540744


",open,2025-01-23T04:33:17Z,,,"fb-exported, topic: not user facing, ciflow/inductor",main,export-D68540744,2,108,66,1,14,0,"desertfire, muchulee8, muchulee8, muchulee8","COMMENTED, COMMENTED, COMMENTED, COMMENTED",False,https://api.github.com/repos/pytorch/pytorch/issues/145459
145450,Replace is_same with is_same_v for concise syntax,"Replace `std::is_same<T, U>::value` with `std::is_same_v` for concise and consistent syntax with other code.

cc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10",open,2025-01-23T02:28:06Z,,,"module: cpu, open source, ciflow/trunk, release notes: sparse",main,opt/aten/syntax,6,10,10,1,16,0,Skylion007,APPROVED,False,https://api.github.com/repos/pytorch/pytorch/issues/145450
145437,Advance past fc window for stft center,"Long overdue follow-up on https://github.com/pytorch/pytorch/pull/73432/files#diff-5f3d4caa0693a716fc46fd7f6339312f1b5f0bf89e3a3ff58e9dc13a9486b17aR719

Onnx stft doesn't support centering, [and all of the existing tests are for center = False](https://github.com/pytorch/pytorch/blob/main/test/onnx/test_pytorch_onnx_onnxruntime.py#L8026). I will open a follow-up issue to address this, this is just a nice-to-have.

Pr chain:
- -> [Advance past fc window for stft center #145437](https://github.com/pytorch/pytorch/pull/145437)
- [Add stft option to align window for center = false #145324](https://github.com/pytorch/pytorch/pull/145324)
- [Add istft option to align window for center = false](https://github.com/pytorch/pytorch/pull/145510)",open,2025-01-23T00:49:05Z,,,"Merged, Reverted, ciflow/trunk, topic: not user facing, ciflow/slow, ci-no-td",main,jz/stft-old-fc,9,65,20,5,13,2,"justinchuby, titaiwangms, jackzhxng, iseeyuan","APPROVED, COMMENTED, COMMENTED, APPROVED",False,https://api.github.com/repos/pytorch/pytorch/issues/145437
145435,[dynamo] save/restore system random state more carefully,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #145435

Fixes https://github.com/pytorch/pytorch/issues/145329.

We we need to save/restore system `random` state in 2 places:
- in `eval_frame.py`, we need to make sure that wrapper code between the user call to a `torch.compile`d function and the actual function call (intercepted by eval_frame.c) doesn't modify random state (https://github.com/pytorch/pytorch/blob/b2c89bc115123aea8e075e882ee121537ec92f89/torch/_dynamo/eval_frame.py#L532)
- in `eval_frame.c`, we need to make sure that guard eval and calling convert_frame don't modify random state (https://github.com/pytorch/pytorch/blob/b2c89bc115123aea8e075e882ee121537ec92f89/torch/csrc/dynamo/eval_frame.c#L575)

Followup - perhaps more global state from `convert_frame.py:preserve_global_state` can be moved to `eval_frame.py/c`.

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames

Differential Revision: [D68532640](https://our.internmc.facebook.com/intern/diff/D68532640)",open,2025-01-23T00:41:37Z,,,"ciflow/trunk, topic: bug fixes, module: dynamo, ciflow/inductor, release notes: dynamo",gh/williamwen42/199/base,gh/williamwen42/199/head,9,200,22,8,9,1,"jansel, williamwen42, williamwen42, williamwen42, jansel","CHANGES_REQUESTED, COMMENTED, COMMENTED, COMMENTED, APPROVED",False,https://api.github.com/repos/pytorch/pytorch/issues/145435
145426,Fix aot inductor intermediate debug printing,"Fixes https://github.com/pytorch/pytorch/issues/145425

The other way to fix this would be to change `_print_debugging_tensor_value_info` to handle constants.

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang @aakhundov",open,2025-01-22T23:55:41Z,,,"ciflow/trunk, topic: bug fixes, module: inductor, ciflow/inductor, release notes: inductor, merging, module: aotinductor",main,exclamaforte/aot-inductor-debug,2,23,1,1,7,1,"desertfire, desertfire","COMMENTED, APPROVED",False,https://api.github.com/repos/pytorch/pytorch/issues/145426
145424,Tag storages with offset in file when  with FakeTensorMode,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #145424

",open,2025-01-22T23:33:36Z,,,,gh/mikaylagawarecki/307/base,gh/mikaylagawarecki/307/head,1,3,0,1,2,0,,,True,https://api.github.com/repos/pytorch/pytorch/issues/145424
145406,[export][be] Clean up local imports from export [2/n],"Summary: as title

Test Plan: CI

Differential Revision: D68450108
",open,2025-01-22T21:39:02Z,,,"fb-exported, ciflow/trunk, release notes: export",main,export-D68450108,1,8,13,1,5,0,tugsbayasgalan,APPROVED,False,https://api.github.com/repos/pytorch/pytorch/issues/145406
145404,[distributions] Catch inf gradient in beta distribution,"Fixes #127387

Under the conditions in the issue, the calculations in [_beta_grad_beta_small](https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/Distributions.h#L397) are numerically unstable (due to the `betas = betas * (beta - casted_i);` blowing up, since in that code path `beta` is large), and the gradient can end up being `nan` when `x` is close to 1 (and hence is close to 0 in that function as it uses `1-x`).
It seems that sometimes rather than become `nan`, the series ends up being `inf`, which isn't currently caught. I was able to verify this through some debug/print statements. I struggled to recreate the issue directly with a size of 1, even with directly calling the backward function with `x` values close to 1.

This PR amends the `nan` check by also checking for `inf`, and adds a test based on the failing case from the linked issue.
",open,2025-01-22T21:27:50Z,,,"triaged, open source, topic: not user facing",main,fix-dirichlet-grad-inf-cpu,2,9,1,1,3,0,,,False,https://api.github.com/repos/pytorch/pytorch/issues/145404
145381,Use AOTI as inductor backend with precompile mode.,"Summary:
Design doc: https://docs.google.com/document/d/1Z15cBBPjoZ7gH00TSgCdgaYko7a7Br-ERd3_hA-g2IU/edit?usp=sharing

In this diff we are trying to introduce some stateful API to enable a global mode which will force inductor to use AOTI as a backend. Different from PR https://github.com/pytorch/pytorch/pull/141700, we didn't try to populate the package file into caching system, instead we bypass caching to simplify the implementation in the current form.

Similar to PR https://github.com/pytorch/pytorch/pull/141700, I did a quick benchmark to the loading time and it looks like the following:
- Precompile
```
buck run mode/opt scripts/zhxchen17:precompile
```
- Load using cache:
```
time buck run mode/opt scripts/zhxchen17:precompile -- --loader cache
```
Output:
```
real    0m24.593s
user    0m59.342s
sys     0m17.201s
```
- Load using load_fullgraph_package
```
time buck run mode/opt scripts/zhxchen17:precompile -- --loader precompile
```
Output:
```
real    0m10.907s
user    0m9.210s
sys     0m1.173s
```

Test Plan:
buck run mode/opt caffe2/test:test_export -- -r test_fullgraph_package_basic
_function

Differential Revision: D68459341


cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang @aakhundov",open,2025-01-22T16:27:55Z,,,"fb-exported, ciflow/trunk, topic: not user facing, module: inductor, ciflow/inductor",main,export-D68459341,6,274,10,2,8,1,"ezyang, ezyang, ezyang, jamesjwu, jamesjwu, jamesjwu, jamesjwu, zhxchen17, jansel, zhxchen17, zhxchen17, jansel, jansel, ezyang","COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, CHANGES_REQUESTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED",False,https://api.github.com/repos/pytorch/pytorch/issues/145381
145367,[ARM] Fix broken tests in test_tensor_creation_ops on AArch64,"We have broken tests on Aarch64 which are not enabled upstream, this PR will fix and enable those tests.

```
AssertionError: Tensor-likes are not equal!

Mismatched elements: 2 / 3 (66.7%)
Greatest absolute difference: 1 at index (1,)
Greatest relative difference: 1.0842021724855044e-19 at index (1,)

To execute this test, run the following from the base repo dir:
    python test/test_tensor_creation_ops.py TestTensorCreationCPU.test_float_to_int_conversion_nonfinite_cpu_int64

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
```",open,2025-01-22T11:29:20Z,,,"triaged, open source, topic: not user facing",main,tensor_creation,2,13,7,2,5,0,,,False,https://api.github.com/repos/pytorch/pytorch/issues/145367
145366,removed check for ConvTranspose3D on MPS,"Fixes #130256

I removed `TORCH_CHECK(input_t.dim() < 5, ""ConvTranspose 3D is not supported on MPS"");` as it is actually supported.",open,2025-01-22T10:20:05Z,,,"triaged, open source, release notes: mps",main,convtranspose_mps_remove_check,1,0,2,1,2,0,,,False,https://api.github.com/repos/pytorch/pytorch/issues/145366
145358,Fix avg_pool crash with negative numbers,"Fixes #145077


cc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10",open,2025-01-22T08:53:13Z,,,"module: cpu, triaged, open source, release notes: quantization",main,avg_pool_positive,5,56,7,4,2,0,,,False,https://api.github.com/repos/pytorch/pytorch/issues/145358
145353,[dtensor][cp] experiment: call flex_attention on DTensor,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #145353

```
  File ""/data/users/xilunwu/oss/pytorch/torch/_higher_order_ops/flex_attention.py"", line 459, in flex_attention_fake_impl
    out = _permute_strides(out, query.stride())
  File ""/data/users/xilunwu/oss/pytorch/torch/_higher_order_ops/flex_attention.py"", line 70, in _permute_strides
    new_out = out.new_empty(out.shape).as_strided(out.shape, out_strides)
  File ""/data/users/xilunwu/oss/pytorch/torch/_compile.py"", line 51, in inner
    return disable_fn(*args, **kwargs)
  File ""/data/users/xilunwu/oss/pytorch/torch/_dynamo/eval_frame.py"", line 745, in _fn
    return fn(*args, **kwargs)
  File ""/data/users/xilunwu/oss/pytorch/torch/distributed/tensor/_api.py"", line 348, in __torch_dispatch__
    return DTensor._op_dispatcher.dispatch(
  File ""/data/users/xilunwu/oss/pytorch/torch/distributed/tensor/_dispatch.py"", line 174, in dispatch
    self.sharding_propagator.propagate(op_info)
  File ""/data/users/xilunwu/oss/pytorch/torch/distributed/tensor/_sharding_prop.py"", line 207, in propagate
    OutputSharding, self.propagate_op_sharding(op_info.schema)
  File ""/data/users/xilunwu/oss/pytorch/torch/distributed/tensor/_sharding_prop.py"", line 47, in __call__
    return self.cache(*args, **kwargs)
  File ""/data/users/xilunwu/oss/pytorch/torch/distributed/tensor/_sharding_prop.py"", line 456, in propagate_op_sharding_non_cached
    raise NotImplementedError(
torch._dynamo.exc.InternalTorchDynamoError: NotImplementedError: Operator aten.as_strided.default does not have a sharding strategy registered.
```


cc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o",open,2025-01-22T06:51:47Z,,,"oncall: distributed, ciflow/inductor",gh/XilunWu/110/base,gh/XilunWu/110/head,2,77,1,1,2,0,,,True,https://api.github.com/repos/pytorch/pytorch/issues/145353
145331,[WIP] [AOTInductor] Use AtenTensorHandle as the constant map's holder.,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #145331

Summary:
Previously, all constants are held by RAIIAtenTensorHandle, which
implicitly indicates constants' lifetime is managed by the model itself.
We want to provide the flexibility to let users control the tensor's
lifetime instead.

This change is the first PR, aims to introduce a holder to act as the original
RAII holder managing the lifetime by the model and change the constant map to use AtenTensorHandle.
All behavior should be exactly the same as previous cases.

Test Plan:
Existing test cases. Not yet introducing new functionalities in this PR.

Reviewers:

Subscribers:

Tasks:

Tags:

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @ColinPeppler @amjames @desertfire @chauhang @aakhundov

Differential Revision: [](https://our.internmc.facebook.com/intern/diff/)

Differential Revision: [D68472175](https://our.internmc.facebook.com/intern/diff/D68472175)",open,2025-01-22T00:52:35Z,,,"ciflow/trunk, topic: not user facing, module: inductor, ciflow/inductor",gh/muchulee8/41/base,gh/muchulee8/41/head,4,83,17,1,3,0,,,False,https://api.github.com/repos/pytorch/pytorch/issues/145331
145310,[Utilization] post-test-process workflow,"# Overview
Add reusable workflow to trigger the post-test right after each test job is complete.

Cousion with pr to setup the runner permissions:
https://github.com/pytorch-labs/pytorch-gha-infra/pull/595/files

 

",open,2025-01-21T21:44:31Z,,,topic: not user facing,main,addDispatchPr,3,64,0,22,3,0,"huydhn, huydhn, yangw-dev","COMMENTED, COMMENTED, COMMENTED",False,https://api.github.com/repos/pytorch/pytorch/issues/145310
145265,Fix SEGFAULT when None arg was passed in GraphContext.op(..),"Fixes #145261
",open,2025-01-21T11:32:33Z,,,"module: onnx, triaged, open source, onnx-triaged, release notes: onnx, topic: bug fixes",main,fix_optional_arg_onnx_export,1,1,1,1,2,0,,,False,https://api.github.com/repos/pytorch/pytorch/issues/145265
145260,[ARM] Add test_ops and test_memory_profiler to aarch64 tests,"Fixes #142371
",open,2025-01-21T10:59:02Z,,,"triaged, open source, topic: not user facing",main,test_ops,2,2,2,1,9,0,,,False,https://api.github.com/repos/pytorch/pytorch/issues/145260
145254,[TEST] tmp storage with CONSTANTHANDLE,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #145254



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @ColinPeppler @amjames @desertfire @chauhang @aakhundov

Differential Revision: [D68430118](https://our.internmc.facebook.com/intern/diff/D68430118)",open,2025-01-21T08:02:18Z,,,"ciflow/trunk, module: inductor, ciflow/inductor",gh/muchulee8/40/base,gh/muchulee8/40/head,6,126,17,2,3,0,,,False,https://api.github.com/repos/pytorch/pytorch/issues/145254
145250,[Inductor][CPU] Add a lowering pass for _weight_int4pack_mm_for_cpu,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #146756
* __->__ #145250
* #145245

**Summary**
It's part of the task to enable max-autotune with GEMM template for WoQ INT4 GEMM on CPU. Early work: #133846

This PR adds a lowering pass for `torch.ops.aten_weight_int4pack_mm_for_cpu`. This op is used for WoQ int4 in Torchao. The lowering pass is a prerequisite for max-autotune, which is planed to be enabled for this op in subsequent PRs.

**Test plan**
```
python test/inductor/test_mkldnn_pattern_matcher.py -k test_woq_int4
python test/inductor/test_cpu_cpp_wrapper.py -k test_woq_int4
```

cc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @voznesenskym @penguinwu @EikanWang @Guobing-Chen @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang @aakhundov",open,2025-01-21T06:17:47Z,,,"module: cpu, open source, ciflow/trunk, release notes: quantization, topic: not user facing, intel, module: inductor, ciflow/inductor",gh/Xia-Weiwen/29/base,gh/Xia-Weiwen/29/head,6,140,2,9,1,0,"sanchitintel, Xia-Weiwen, Xia-Weiwen, Xia-Weiwen, sanchitintel","COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED",True,https://api.github.com/repos/pytorch/pytorch/issues/145250
145248,[Break XPU][Inductor UT] Set input tensors to corresponding device for test case in test_aot_indutor.py,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #146763
* __->__ #145248
* #146762

Fix #145247

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang @aakhundov",open,2025-01-21T06:09:54Z,,,"open source, ciflow/trunk, topic: not user facing, module: inductor, ciflow/inductor, ciflow/xpu",gh/etaf/95/base,gh/etaf/95/head,3,30,17,6,5,3,"malfet, malfet, desertfire, etaf, desertfire, etaf, desertfire, jansel","APPROVED, CHANGES_REQUESTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, APPROVED, APPROVED",False,https://api.github.com/repos/pytorch/pytorch/issues/145248
145245,[Don't review][Quant][CPU] add a wrapper op for _weight_int4pack_mm_for_cpu with tensor args,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #146756
* #145250
* __->__ #145245

**Summary**
It's part of the task to enable max-autotune with GEMM template for WoQ INT4 GEMM on CPU. Early work: #133846

This PR adds a wrapper op in `quantized_decomposed` lib for `torch.ops.aten_weight_int4pack_mm_for_cpu`, whose arguments are all tensors. It will be used in Inductor lowering with max-autotune where scalar arguments are difficult to handle.

**Test plan**
```
python test/test_linalg.py -k test__int4_mm
```

cc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @voznesenskym @penguinwu @EikanWang @Guobing-Chen @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang @aakhundov",open,2025-01-21T05:08:51Z,,,"module: cpu, open source, ciflow/trunk, release notes: quantization, release notes: linalg_frontend, intel, module: inductor, ciflow/inductor",gh/Xia-Weiwen/28/base,gh/Xia-Weiwen/28/head,5,30,1,9,3,0,"leslie-fang-intel, jgong5, sanchitintel, sanchitintel, jgong5, leslie-fang-intel, Xia-Weiwen, Xia-Weiwen, sanchitintel, Xia-Weiwen","APPROVED, APPROVED, COMMENTED, COMMENTED, APPROVED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED",True,https://api.github.com/repos/pytorch/pytorch/issues/145245
145244,Improve typing by using bool and int,"Fixes #ISSUE_NUMBER
",open,2025-01-21T04:52:30Z,,,"triaged, open source, topic: not user facing",main,buildin_type,6,734,742,4,5,0,,,False,https://api.github.com/repos/pytorch/pytorch/issues/145244
145243,[inductor] Make serialized inductor patterns path configurable instead of using …,"…fixed path in the inductor module

Fixes [145242](https://github.com/pytorch/pytorch/issues/145242)


cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang @aakhundov",open,2025-01-21T04:42:58Z,,,"triaged, open source, module: inductor, release notes: inductor",main,expose_inductor_patt_path,3,27,8,6,6,1,"aorenste, kareemshaik80, aorenste, kareemshaik80, aorenste, aorenste","CHANGES_REQUESTED, COMMENTED, COMMENTED, COMMENTED, APPROVED, CHANGES_REQUESTED",False,https://api.github.com/repos/pytorch/pytorch/issues/145243
145241,add grad_output shape check for adaptive_avg_pool2d_backward,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #145241

Fix https://github.com/pytorch/pytorch/issues/145070.
",open,2025-01-21T04:33:24Z,,,"open source, ciflow/trunk, topic: not user facing",gh/jiayisunx/54/base,gh/jiayisunx/54/head,3,41,12,4,2,1,malfet,COMMENTED,False,https://api.github.com/repos/pytorch/pytorch/issues/145241
145239,Turn Stream into protocol and improve typing in torch/_C/__init__.pyi.in,"Fixes #ISSUE_NUMBER


cc @EikanWang @jgong5 @wenzhe-nrv @sanchitintel",open,2025-01-21T03:10:24Z,,,"oncall: jit, triaged, open source, topic: not user facing",main,C_init_py,1,81,79,6,6,0,"guangyey, cyyever, guangyey, cyyever, XuehaiPan","COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED",True,https://api.github.com/repos/pytorch/pytorch/issues/145239
145228,Expose the rendezvous keepalive arguments,"Enables support for this:

```python
from torch.distributed.launcher.api import LaunchConfig

config = LaunchConfig(
    ...,
    rdzv_configs={""keep_alive_interval"": 1122, ""heartbeat_timeout"": 321, ""keep_alive_max_attempt"" 5},
)
```

These arguments are currently hard-coded inside torchrun. The default values are not suitable for jobs with thousands of ranks.

Today, `rdzv_configs` only allows the keys `join_timeout`, `last_call_timeout`, `close_timeout`

cc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o",open,2025-01-20T22:58:17Z,,,"oncall: distributed, triaged, open source, release notes: distributed (torchelastic)",main,patch-1,1,17,3,3,1,0,,,False,https://api.github.com/repos/pytorch/pytorch/issues/145228
145224,update sympy version 1.13.3 in setup.py (previously update only in requirement.txt),"Previously, only update `sympy` version number in `requirement.txt`, but `setup.py` is unchanged. In PyPI, the wheel will relay on the dependency spec in `setup.py`, so only change in `setup.py` will be effective.",open,2025-01-20T20:18:42Z,,,"open source, ciflow/binaries, ciflow/trunk, topic: not user facing",main,main,1,1,1,1,13,0,Skylion007,APPROVED,False,https://api.github.com/repos/pytorch/pytorch/issues/145224
145223,Raise MutationError if there are side effects when returning generator,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #142513
* __->__ #145223
* #144420
* #144424
* #144423
* #144422
* #144421
* #141055



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",open,2025-01-20T18:05:17Z,,,"open source, module: dynamo, ciflow/inductor, release notes: dynamo",gh/guilhermeleobas/91/base,gh/guilhermeleobas/91/head,6,313,24,30,4,1,"Skylion007, guilhermeleobas, zou3519, zou3519, zou3519, StrongerXi, zou3519, guilhermeleobas, guilhermeleobas, zou3519, zou3519, zou3519, zou3519, guilhermeleobas","COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, APPROVED, COMMENTED",False,https://api.github.com/repos/pytorch/pytorch/issues/145223
145209,Fix incorrect citation of authors in documentation,"This PR corrects the citation of Adafactor authors ""Noam Shazeer"" and ""Mitchell Stern"" in the documentation.
The current text incorrectly lists them as ""Shazeer, Noam, and Mitchell Stern,"" which seems to be a result of a data parsing issue of some reference manager(s) [as you can find many papers with the same issue](https://www.google.com/search?q=%22Shazeer%2C+Noam%2C+and+Mitchell+Stern%22). 
The updated citation follows standard conventions for author names.",open,2025-01-20T09:19:08Z,,,"open source, release notes: optim",main,fix-adafactor-citation,1,7,7,1,2,1,janeyx99,APPROVED,False,https://api.github.com/repos/pytorch/pytorch/issues/145209
145197,Use std::string_view in get_fully_qualified_type_name,The same as #139164 but open a new PR due to messy history there.,open,2025-01-20T03:28:55Z,,,"triaged, open source, topic: not user facing",main,stringview30,2,33,38,1,2,0,,,False,https://api.github.com/repos/pytorch/pytorch/issues/145197
145194,Add transpose support for CppMicroGemmFP32Vec,cc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @voznesenskym @penguinwu @EikanWang @Guobing-Chen @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang @aakhundov,open,2025-01-20T01:45:49Z,,,"module: cpu, open source, ciflow/trunk, topic: improvements, module: inductor, ciflow/inductor, release notes: inductor",main,trans_gemm,5,856,20,3,1,0,,,True,https://api.github.com/repos/pytorch/pytorch/issues/145194
145180,Added torch check to ensure indices are not empty,"Fixes #142459

cc @mruberry @jbschlosser @walterddr @mikaylagawarecki",open,2025-01-19T23:49:19Z,,,"triaged, open source, topic: not user facing",main,max-pool3d,2,4,0,5,3,1,"Skylion007, mikaylagawarecki","COMMENTED, COMMENTED",False,https://api.github.com/repos/pytorch/pytorch/issues/145180
145169,Added weight to MSELoss Criterion,"- Changed Inheritance of MSELoss from _Loss to _WeightedLoss
- Modified MSELoss to include weight parameter
- Removed TODO
- Added weight documentation to MSELoss Class

topic: enhancement
release notes: nn

I couldn't find this in any issues or under any existing PR Requests, I only found it by finding the TODO in the loss.py file. 

Edit - Accidental Markdown all caps removed
",open,2025-01-19T09:40:11Z,,,"triaged, open source, release notes: nn, topic: improvements",main,add_weighted_MSELoss,1,6,5,1,3,0,,,False,https://api.github.com/repos/pytorch/pytorch/issues/145169
145167,[BE]: Update NCCL submodule to 2.24.3,"Update NCCL to the latest version

Last bump was in https://github.com/pytorch/pytorch/pull/124014

See upstream release notes here: https://docs.nvidia.com/deeplearning/nccl/release-notes/rel_2-24-3.html#rel_2-24-3

cc @Skylion007
",open,2025-01-19T04:12:34Z,,,"triaged, open source, topic: not user facing",main,tmm1/bump-nccl-dep,8,54,54,2,3,0,"Skylion007, tmm1","COMMENTED, COMMENTED",False,https://api.github.com/repos/pytorch/pytorch/issues/145167
145153,[BE]: Apply ruff PERF401 to torch,"Applies PERF401 optimizations to torch.


cc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o @EikanWang @jgong5 @wenzhe-nrv @sanchitintel @ezyang @SherlockNoMad @voznesenskym @penguinwu @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang @aakhundov",open,2025-01-18T16:38:34Z,,,"oncall: distributed, oncall: jit, open source, better-engineering, ciflow/trunk, release notes: quantization, fx, module: inductor, module: dynamo, ciflow/inductor, release notes: AO frontend",main,skylion007/apply-ruff-PERF401-2025-01-18,21,80,91,5,9,1,"XuehaiPan, albanD, albanD","APPROVED, APPROVED, APPROVED",False,https://api.github.com/repos/pytorch/pytorch/issues/145153
145150,[inductor] Simplify _inductor/utils.py slightly,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #145150
* #144108



cc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang @aakhundov",open,2025-01-18T15:51:22Z,,,"oncall: distributed, module: rocm, open source, better-engineering, topic: not user facing, module: inductor, ciflow/inductor",gh/rec/124/base,gh/rec/124/head,11,89,130,8,1,1,"eellison, eellison, rec, rec","COMMENTED, COMMENTED, COMMENTED, COMMENTED",False,https://api.github.com/repos/pytorch/pytorch/issues/145150
145148,[BE][PYFMT] bump `ruff format` target version to py39: add parentheses around long `with`-statements,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #145606
* #144546
* #144569
* __->__ #145148
* #146509



cc @ezyang @SherlockNoMad @EikanWang @jgong5 @wenzhe-nrv @voznesenskym @penguinwu @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @jiayisunx @chenyang78 @kadeng @chauhang @amjames",open,2025-01-18T14:34:48Z,,,"open source, ciflow/trunk, release notes: onnx, topic: not user facing, fx, module: dynamo, ciflow/inductor",gh/XuehaiPan/237/base,gh/XuehaiPan/237/head,14,137,66,11,9,1,"XuehaiPan, justinchuby, Skylion007, XuehaiPan, malfet","COMMENTED, APPROVED, COMMENTED, COMMENTED, CHANGES_REQUESTED",False,https://api.github.com/repos/pytorch/pytorch/issues/145148
145147,[BE][Easy] increase pip timeout for nightly tool: 15s -> 60s,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #145147

",open,2025-01-18T12:30:38Z,,,"open source, topic: not user facing",gh/XuehaiPan/236/base,gh/XuehaiPan/236/head,1,7,1,2,2,0,,,False,https://api.github.com/repos/pytorch/pytorch/issues/145147
145146,improve perf for layer_norm,"Fixes #145145

Please see more details in the issue.
",open,2025-01-18T12:21:27Z,,,"triaged, open source, release notes: cuda",main,layernorm,1,258,3,1,1,0,,,False,https://api.github.com/repos/pytorch/pytorch/issues/145146
145136,[inductor] [bug fix] Fix `conv` on processing uint ,"Fixes #144314


ut
```
pytest -s -v test/inductor/test_torchinductor.py -k test_conv_errors_with_uint
```

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang @aakhundov",open,2025-01-18T02:55:12Z,,,"open source, ciflow/trunk, topic: not user facing, module: inductor",main,fix_conv_uint,2,20,0,11,18,0,"jansel, shaoyuyoung, jansel, etaf, shaoyuyoung","CHANGES_REQUESTED, COMMENTED, APPROVED, COMMENTED, COMMENTED",False,https://api.github.com/repos/pytorch/pytorch/issues/145136
145130,[cuBLAS][cuBLASLt] Unify `cuBLASLt` workspaces with `cuBLAS` workspaces,"As `cuBLAS` workspaces are already per-stream, there shouldn't be kernel execution overlap with `cuBLASLt` kernels.

This PR reuses `cuBLAS` workspaces for `cuBLASLt` for the following benefits:

+ caching (`cuBLAS` workspaces were already cached, so now we get that for `cuBLASLt`)
+ ""free"" workspace size bump for `cuBLASLt` `cuBLASLt` workspace sizes were previously smaller than those for `cuBLAS` by default which potentially hurts performance, and we encountered difficulty in increasing the size due to downstream OOMs , see also #120925
+ fixes behavior broken behavior with the memtracker; https://github.com/pytorch/pytorch/pull/139442 attempted to handle peaky allocation behavior that broke memtracker equivalence tests but it didn't seem to fully work, here the cached/reused `cuBLAS` workspace seems to fix it
+ one environment variable to rule them all: `CUBLAS_WORKSPACE_CONFIG` applies directly to `cuBLASLt` without a confusing `CUBLASLT_WORKSPACE_SIZE` that users would also need to consider

cc @ptrblck @msaroufim @csarofeen @xwang233 @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",open,2025-01-18T00:33:07Z,,,"module: cuda, triaged, module: cublas, open source, Merged, Reverted, ciflow/trunk, topic: not user facing, ciflow/periodic, module: dynamo, ciflow/inductor, matrix multiplication, ciflow/rocm, ci-no-td",main,unifiedcublasltworkspace,4,73,16,6,30,0,"ngimel, eqy, ngimel","COMMENTED, COMMENTED, APPROVED",False,https://api.github.com/repos/pytorch/pytorch/issues/145130
145128,[executorch hash update] update the pinned executorch hash,"This PR is auto-generated nightly by [this action](https://github.com/pytorch/pytorch/blob/main/.github/workflows/nightly.yml).
Update the pinned executorch hash.",open,2025-01-18T00:22:18Z,,,"open source, ciflow/trunk, topic: not user facing, ciflow/inductor",main,update-executorch-commit-hash/12838938822-1425-1,1,1,1,1,30,0,pytorchbot,APPROVED,False,https://api.github.com/repos/pytorch/pytorch/issues/145128
145125,Repro collective timeout and FR dump,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #145125
* #144834
* #145099
* #145011
* #145010

the timeout is unfortunatley not reliable to repro.  I'm not yet sure
what the root cause is, so for now I am just uploading my FR trace files
to improve the analyzer script.

Unfortunatley, these traces that I got on one instance were apparently corrupted, or at least fr_trace complained of an unpickling error
[traces.tar.gz](https://github.com/user-attachments/files/18461972/traces.tar.gz)

cc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @d4l3k @c-p-i-o",open,2025-01-18T00:05:59Z,,,"oncall: distributed, topic: not user facing",gh/wconstab/391/base,gh/wconstab/391/head,1,21,7,1,2,0,,,False,https://api.github.com/repos/pytorch/pytorch/issues/145125
145119,WIP sccache simplified,Trying to see if we can get rid of all of the wrapper code now,open,2025-01-17T22:50:05Z,,,"ciflow/binaries, ciflow/trunk, topic: not user facing, ciflow/inductor",main,wdvr/sccache_simplified,1,2,33,3,1,0,,,True,https://api.github.com/repos/pytorch/pytorch/issues/145119
145117,[EXPERIMENTAL][dynamo] optimize `DictGetItemGuardAccessor`,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #145117
* #143313



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",open,2025-01-17T22:44:58Z,,,"module: dynamo, ciflow/inductor",gh/StrongerXi/67/base,gh/StrongerXi/67/head,1,38,5,1,2,0,,,False,https://api.github.com/repos/pytorch/pytorch/issues/145117
145116,WIP remove -E workaround for nvcc,"follow up on https://github.com/pytorch/pytorch/pull/145012 to remove workaround https://github.com/pytorch/pytorch/pull/142813/files

Testing to see if sccache now handles the nvcc caching correctly",open,2025-01-17T22:41:59Z,,,"ciflow/trunk, topic: not user facing",main,wdvr/sccache_nvcc,1,1,27,2,1,0,,,True,https://api.github.com/repos/pytorch/pytorch/issues/145116
145104,"futher scheduler changes for invoke_quant: prologue low prec, (slightly) more aggressive fusion","Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #145104
* #139102

Respect invoke_quant low precision options, also, be more aggressive in attepmting fusion.

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang @aakhundov",open,2025-01-17T20:04:38Z,,,"ciflow/trunk, topic: not user facing, module: inductor, ciflow/inductor",gh/eellison/752/base,gh/eellison/752/head,7,108,33,12,1,1,"shunting314, jansel, jansel, jansel","APPROVED, CHANGES_REQUESTED, APPROVED, APPROVED",False,https://api.github.com/repos/pytorch/pytorch/issues/145104
145098,Use STL string_view header,"Fixes #ISSUE_NUMBER


cc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o @EikanWang @jgong5 @wenzhe-nrv @sanchitintel",open,2025-01-17T19:24:03Z,,,"oncall: distributed, oncall: jit, release notes: cpp, topic: improvements",main,richard/string_view_header,21,40,32,1,1,1,Skylion007,APPROVED,False,https://api.github.com/repos/pytorch/pytorch/issues/145098
145090,Test,"Fixes #ISSUE_NUMBER
",open,2025-01-17T17:43:35Z,,,topic: not user facing,main,try-speedup-docbuild,2,42,1,14,1,0,,,True,https://api.github.com/repos/pytorch/pytorch/issues/145090
145089,"[POC] Extend torch function support to ALL arguments, not just scalar type (but not insides of list)","Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #145089

Signed-off-by: Edward Z. Yang <ezyang@meta.com>",open,2025-01-17T17:28:38Z,,,"release notes: fx, no-stale",gh/ezyang/3068/base,gh/ezyang/3068/head,3,28,20,3,4,0,,,False,https://api.github.com/repos/pytorch/pytorch/issues/145089
145076,"Revert ""Fix for MSVC problem on Windows Arm64 (#136765)""","This reverts commit f7e36d8d6f9706ee9b9653538c4c8d2ba375a181. This commit was to provide a workaround for the problem reported here: https://developercommunity.visualstudio.com/t/MSVC-loop-unrolling-problem-194033813-/10720692 . Now that this is resolved and released, I'm reverting the workaround.



cc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10",open,2025-01-17T14:36:02Z,,,"module: cpu, triaged, open source, topic: not user facing",main,revert-msvc-workaround,1,0,23,2,1,1,malfet,CHANGES_REQUESTED,False,https://api.github.com/repos/pytorch/pytorch/issues/145076
145061,[Inductor] optimize welford reduction,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #145061


Fix https://github.com/pytorch/pytorch/issues/141541.
Fix https://github.com/pytorch/pytorch/issues/142839.
Fix https://github.com/pytorch/pytorch/issues/143182.

cc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @desertfire @chauhang @aakhundov @ColinPeppler",open,2025-01-17T10:09:49Z,,,"oncall: distributed, open source, ciflow/trunk, module: inductor, ciflow/inductor, release notes: inductor",gh/jiayisunx/53/base,gh/jiayisunx/53/head,2,245,88,17,1,0,,,True,https://api.github.com/repos/pytorch/pytorch/issues/145061
145056,Update test_c10d_object_collectives.py with DistributedTestBase class,"# MOTIVATION
To generalize distributed test cases for non-CUDA devices, we are leveraging the DistributedTestBase class introduced in [PR #138216](https://github.com/pytorch/pytorch/pull/138216). This new class is derived from MultiProcessTestCase and abstracts the creation/deletion of process groups and other functionality for specific devices. In this PR, we extend the scope of these tests to support HPUs.

# CHANGES

Replaced MultiProcessTestCase with the DistributedTestBase class.
Extended test functionality to include support for HPUs.
Utilized instantiate_device_type_tests with targeted attributes to generate device-specific test instances.
Applied the skipIfHPU decorator to skip tests that are not yet compatible with HPU devices.

cc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o @ankurneog ",open,2025-01-17T07:23:04Z,,,"oncall: distributed, triaged, open source, ciflow/trunk, topic: not user facing",main,distributed_changes,1,44,59,4,16,1,"Skylion007, Skylion007, wconstab, AnantGulati, kwen2501, amathewc, amathewc, amathewc, guangyey","COMMENTED, COMMENTED, COMMENTED, COMMENTED, APPROVED, COMMENTED, COMMENTED, COMMENTED, APPROVED",False,https://api.github.com/repos/pytorch/pytorch/issues/145056
145034,Use GiB on the axis of memory viz to match how we textually print it.,"I didn't do this before because d3 doesn't really support it. However, I argued with an LLM for awhile to get it to reproduce basically what d3's nice axis behavior is but have it work for 2^(10c) multiples.",open,2025-01-17T02:23:48Z,,,,main,zdevito-patch-2,1,58,7,1,2,0,,,False,https://api.github.com/repos/pytorch/pytorch/issues/145034
145025,[pytorch/ncclx] Remove Alltoallv specialization for PTD all_to_all,"Summary:
PTD all_to_all uses a list of tensors, while ncclAllToAllv (provided
by NCCLX and RCCL) assumes that a single contiguous buffer is used.
These are fundamentally mismatched.  The list of tensors might not be
contiguous or even ordered (buffer addresses might not be in
increasing order).

This patch removes the ncclAllToAllv specialization for PTD
all_to_all, and instead let's it directly call ncclSend/ncclRecv.

Test Plan: CI

Differential Revision: D68289467


",open,2025-01-17T00:50:00Z,,,fb-exported,main,export-D68289467,1,0,43,1,4,0,,,False,https://api.github.com/repos/pytorch/pytorch/issues/145025
145024,Made partitioning more(?) deterministic,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #145024
* #145059
* #145029

",open,2025-01-17T00:47:38Z,,,ciflow/inductor,gh/chillee/377/base,gh/chillee/377/head,1,32,24,6,2,1,"ezyang, eellison","APPROVED, COMMENTED",False,https://api.github.com/repos/pytorch/pytorch/issues/145024

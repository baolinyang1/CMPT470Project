[
    {
        "Issue ID": 6671,
        "Issue State": "closed",
        "Issue Title": "Fix getting annotations for _parse_sitemap() at the runtime.",
        "Comment ID": 2656641935,
        "Author": "codecov[bot]",
        "Created At": "2025-02-13T13:43:19Z",
        "Comment Body": "## [Codecov](https://app.codecov.io/gh/scrapy/scrapy/pull/6671?dropdown=coverage&src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scrapy) Report\nAll modified and coverable lines are covered by tests :white_check_mark:\n> Project coverage is 91.36%. Comparing base [(`00167ed`)](https://app.codecov.io/gh/scrapy/scrapy/commit/00167edca0677fd9657a8c5995d30600dd37fe37?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scrapy) to head [(`1f04a1d`)](https://app.codecov.io/gh/scrapy/scrapy/commit/1f04a1d586e83577100b993eb910a0345fc9ea62?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scrapy).\n> Report is 1 commits behind head on master.\n\n<details><summary>Additional details and impacted files</summary>\n\n\n```diff\n@@           Coverage Diff           @@\n##           master    #6671   +/-   ##\n=======================================\n  Coverage   91.36%   91.36%           \n=======================================\n  Files         162      162           \n  Lines       12047    12048    +1     \n  Branches     1551     1551           \n=======================================\n+ Hits        11007    11008    +1     \n  Misses        735      735           \n  Partials      305      305           \n```\n\n| [Files with missing lines](https://app.codecov.io/gh/scrapy/scrapy/pull/6671?dropdown=coverage&src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scrapy) | Coverage \u0394 | |\n|---|---|---|\n| [scrapy/spiders/sitemap.py](https://app.codecov.io/gh/scrapy/scrapy/pull/6671?src=pr&el=tree&filepath=scrapy%2Fspiders%2Fsitemap.py&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scrapy#diff-c2NyYXB5L3NwaWRlcnMvc2l0ZW1hcC5weQ==) | `85.54% <100.00%> (+0.17%)` | :arrow_up: |\n\n</details>"
    },
    {
        "Issue ID": 6664,
        "Issue State": "closed",
        "Issue Title": "Improve diagnostics for sync-only spider middlewares.",
        "Comment ID": 2645934186,
        "Author": "codecov[bot]",
        "Created At": "2025-02-08T20:41:55Z",
        "Comment Body": "## [Codecov](https://app.codecov.io/gh/scrapy/scrapy/pull/6664?dropdown=coverage&src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scrapy) Report\nAll modified and coverable lines are covered by tests :white_check_mark:\n> Project coverage is 91.36%. Comparing base [(`f041f26`)](https://app.codecov.io/gh/scrapy/scrapy/commit/f041f26a6ff636b764d2bf584ddbc9b9e4334d1b?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scrapy) to head [(`ede9e9c`)](https://app.codecov.io/gh/scrapy/scrapy/commit/ede9e9c3c3f9a9049fea2a6be0339b2c7434b8a1?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scrapy).\n> Report is 3 commits behind head on master.\n\n<details><summary>Additional details and impacted files</summary>\n\n\n```diff\n@@           Coverage Diff           @@\n##           master    #6664   +/-   ##\n=======================================\n  Coverage   91.36%   91.36%           \n=======================================\n  Files         162      162           \n  Lines       12050    12047    -3     \n  Branches     1551     1551           \n=======================================\n- Hits        11009    11007    -2     \n  Misses        735      735           \n+ Partials      306      305    -1     \n```\n\n| [Files with missing lines](https://app.codecov.io/gh/scrapy/scrapy/pull/6664?dropdown=coverage&src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scrapy) | Coverage \u0394 | |\n|---|---|---|\n| [scrapy/core/spidermw.py](https://app.codecov.io/gh/scrapy/scrapy/pull/6664?src=pr&el=tree&filepath=scrapy%2Fcore%2Fspidermw.py&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scrapy#diff-c2NyYXB5L2NvcmUvc3BpZGVybXcucHk=) | `100.00% <100.00%> (+0.54%)` | :arrow_up: |\n| [scrapy/utils/python.py](https://app.codecov.io/gh/scrapy/scrapy/pull/6664?src=pr&el=tree&filepath=scrapy%2Futils%2Fpython.py&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scrapy#diff-c2NyYXB5L3V0aWxzL3B5dGhvbi5weQ==) | `89.15% <\u00f8> (\u00f8)` | |\n\n</details>"
    },
    {
        "Issue ID": 5214,
        "Issue State": "closed",
        "Issue Title": "Add support for YAML export format",
        "Comment ID": 883956450,
        "Author": "wRAR",
        "Created At": "2021-07-21T07:24:25Z",
        "Comment Body": "Sorry?"
    },
    {
        "Issue ID": 5214,
        "Issue State": "closed",
        "Issue Title": "Add support for YAML export format",
        "Comment ID": 884672222,
        "Author": "wRAR",
        "Created At": "2021-07-22T05:55:11Z",
        "Comment Body": "@toqp can you please elaborate what were you trying to say here?"
    },
    {
        "Issue ID": 5214,
        "Issue State": "closed",
        "Issue Title": "Add support for YAML export format",
        "Comment ID": 886071170,
        "Author": "toqp",
        "Created At": "2021-07-24T15:50:52Z",
        "Comment Body": "I noticed that the result of parsing can not be displayed in yml, I wanted to open it myself\r\nPull requests, but no time for that. I ask you to add functionality as an output of the parsing result to the yml file."
    },
    {
        "Issue ID": 5214,
        "Issue State": "closed",
        "Issue Title": "Add support for YAML export format",
        "Comment ID": 886134910,
        "Author": "elacuesta",
        "Created At": "2021-07-25T01:51:10Z",
        "Comment Body": "Given that there is no stdlib support for YAML and that a functional (although admittedly limited) YAML item exporter is [relatively easy to implement](https://github.com/elacuesta/scrapy-yaml-item-exporter/blob/master/scrapy_yaml_item_exporter/scrapy_yaml_item_exporter.py), I must say I'm -1 on adding support."
    },
    {
        "Issue ID": 5214,
        "Issue State": "closed",
        "Issue Title": "Add support for YAML export format",
        "Comment ID": 2651993412,
        "Author": "wRAR",
        "Created At": "2025-02-11T20:26:17Z",
        "Comment Body": "Closing per the previous comment."
    },
    {
        "Issue ID": 6663,
        "Issue State": "closed",
        "Issue Title": "HtmlResponse and AttributeError",
        "Comment ID": 2644284437,
        "Author": "oweitman",
        "Created At": "2025-02-07T23:02:24Z",
        "Comment Body": "ok i found it.\n\nscrapy.http.HtmlRespons"
    },
    {
        "Issue ID": 3110,
        "Issue State": "closed",
        "Issue Title": "Is there better way to call Scrapy programatically?",
        "Comment ID": 363534266,
        "Author": "fabioaanthony",
        "Created At": "2018-02-06T19:19:11Z",
        "Comment Body": "@Urahara you can try something like this:\r\n\r\n```\r\nprocess.crawl(ProductListSpider, username=username, password=password)\r\n```\r\n\r\nInside of the spider, `ProductListSpider`, you'd have something like this:\r\n\r\n```\r\nclass ProductListSpider(scrapy.Spider):\r\n    name = 'product_list'\r\n    start_urls = [\r\n        '...'\r\n    ]\r\n\r\n    def __init__(self, username=\"\", password=\"\"):\r\n        self.username = username\r\n        self.password = password\r\n\r\n    def parse(self, response):\r\n        return scrapy.FormRequest.from_response(response,\r\n            formdata={\r\n                'username': self.username,\r\n                'password': self.password\r\n            },\r\n            callback=self.__after_login\r\n        )\r\n\r\n    // ... do more stuff\r\n```"
    },
    {
        "Issue ID": 3110,
        "Issue State": "closed",
        "Issue Title": "Is there better way to call Scrapy programatically?",
        "Comment ID": 363733000,
        "Author": "Urahara",
        "Created At": "2018-02-07T10:56:14Z",
        "Comment Body": "Thanks man, for future reference i will leave what i code here:\n\n```python\n#code ommited\n\nusername = 'username'\npassword = 'password'\n\ncrawled_items = []\n\ndef add_item(item):\n    crawled_items.append(item)\n\nconfigure_logging(install_root_handler=False)\nlogging.basicConfig(\n    filename='log.txt'\n)\n\nrunner = CrawlerRunner()\n\ncrawler = Crawler(ProductListSpider, get_project_settings())\ncrawler.signals.connect(add_item, signals.item_scraped)\nd = runner.crawl(crawler, username=username, password=password) # keywords works too: **{ 'username' : username, 'senha' : password}\nd.addBoth(lambda _: reactor.stop())\nreactor.run()\n```"
    },
    {
        "Issue ID": 6661,
        "Issue State": "closed",
        "Issue Title": "Json separators added",
        "Comment ID": 2642376153,
        "Author": "wRAR",
        "Created At": "2025-02-07T09:15:41Z",
        "Comment Body": "kwargs are already passed to the exporter, so your change is a no-op and your tests pass without your change."
    },
    {
        "Issue ID": 6661,
        "Issue State": "closed",
        "Issue Title": "Json separators added",
        "Comment ID": 2643698816,
        "Author": "Amin-Eb",
        "Created At": "2025-02-07T18:32:42Z",
        "Comment Body": "> kwargs are already passed to the exporter, so your change is a no-op and your tests pass without your change.\r\n\r\nThanks for your mention, got it.\r\nIm not much capable in python and im learning and testing new things."
    },
    {
        "Issue ID": 6459,
        "Issue State": "closed",
        "Issue Title": "get_project_settings() is not working",
        "Comment ID": 2281308087,
        "Author": "lordsoffallen",
        "Created At": "2024-08-10T11:57:33Z",
        "Comment Body": "Current workaround that I have found is this:\r\n\r\n```\r\ndef get_crawler_process() -> CrawlerProcess:\r\n    with patch('scrapy.utils.conf.closest_scrapy_cfg') as mocked:\r\n        mocked.return_value = str(\r\n            Path(__file__).parent.parent.parent.resolve().joinpath(\"scrapy.cfg\")\r\n        )\r\n        return CrawlerProcess(settings=get_project_settings())\r\n```\r\n\r\nBasically I manually point to cfg file location and then configuration is being picked up afterwards. "
    },
    {
        "Issue ID": 6459,
        "Issue State": "closed",
        "Issue Title": "get_project_settings() is not working",
        "Comment ID": 2281842399,
        "Author": "wRAR",
        "Created At": "2024-08-10T13:42:44Z",
        "Comment Body": "Are you trying to report a bug in Scrapy?"
    },
    {
        "Issue ID": 6459,
        "Issue State": "closed",
        "Issue Title": "get_project_settings() is not working",
        "Comment ID": 2282015877,
        "Author": "lordsoffallen",
        "Created At": "2024-08-10T14:16:41Z",
        "Comment Body": "> Are you trying to report a bug in Scrapy?\r\n\r\nYes, I believe it is bug that `get_project_settings` don't return expected settings depending on which python file the process is invoked."
    },
    {
        "Issue ID": 6459,
        "Issue State": "closed",
        "Issue Title": "get_project_settings() is not working",
        "Comment ID": 2282177776,
        "Author": "wRAR",
        "Created At": "2024-08-10T14:54:55Z",
        "Comment Body": "Not 100% sure what do you mean by \"python file there\" but what do you think it's a bug here and why?"
    },
    {
        "Issue ID": 6459,
        "Issue State": "closed",
        "Issue Title": "get_project_settings() is not working",
        "Comment ID": 2282220034,
        "Author": "lordsoffallen",
        "Created At": "2024-08-10T17:32:08Z",
        "Comment Body": "Okay so im running a function called `test_my_func` from my tests folder (structure can be seen above). I am invoking `CrawlerProcess` which requires `settings` to be passed. Scrapy provides a util function for this -> from scrapy.utils.project import get_project_settings.\r\n\r\nget_project_settings() calls some other functions to correctly create settings object. However, there is an issue with some of the internal function here: https://github.com/scrapy/scrapy/blob/master/scrapy/utils/conf.py#L95\r\n\r\nclosest_scrapy_cfg() (linked above) is traversing the directory starting from `.` which is a relative path. Considering the folder structure I have shared above, this function can't find the scrapy.cfg because it's only going to upper directories so it tries to check folder `test/scraper/` first to see if there is a scrapy.cfg file. then if not check `tests/` and so on. Since the scrapy.cfg in my structure is under `src` folder it never finds it. This is why I shared the patched solution. User should be able to pass the directory to scrapy to avoid this as right now we can't do it.\r\n\r\nI hope this clears it now"
    },
    {
        "Issue ID": 6459,
        "Issue State": "closed",
        "Issue Title": "get_project_settings() is not working",
        "Comment ID": 2282220845,
        "Author": "wRAR",
        "Created At": "2024-08-10T17:34:52Z",
        "Comment Body": "> closest_scrapy_cfg() (linked above) is traversing the directory starting from . which is a relative path. Considering the folder structure I have shared above, this function can't find the scrapy.cfg because it's only going to upper directories \r\n\r\nCorrect.\r\n\r\nWhat do you consider a bug here?"
    },
    {
        "Issue ID": 6459,
        "Issue State": "closed",
        "Issue Title": "get_project_settings() is not working",
        "Comment ID": 2282285170,
        "Author": "lordsoffallen",
        "Created At": "2024-08-10T21:48:15Z",
        "Comment Body": "> > closest_scrapy_cfg() (linked above) is traversing the directory starting from . which is a relative path. Considering the folder structure I have shared above, this function can't find the scrapy.cfg because it's only going to upper directories\r\n> \r\n> Correct.\r\n> \r\n> What do you consider a bug here?\r\n\r\nWell, calling a function that invokes the crawling from somewhere else in python breaks the code. Are we expecting user to only do in certain directories? To me this is indeed bug. There should be a way for me to pass scrapy.cfg file path if the underlying code can't find its location properly. \r\n\r\nWhat I would expect is that the function that invokes scrapy should work wherever I invoke it."
    },
    {
        "Issue ID": 6459,
        "Issue State": "closed",
        "Issue Title": "get_project_settings() is not working",
        "Comment ID": 2282294356,
        "Author": "wRAR",
        "Created At": "2024-08-10T22:09:06Z",
        "Comment Body": "> Are we expecting user to only do in certain directories? \r\n\r\nYes. When we talk about \"running inside a project\" we mean exactly this.\r\n\r\n> What I would expect is that the function that invokes scrapy should work wherever I invoke it.\r\n\r\nIt works as documented, by finding the current project. If there is no current project it obviously can't find it.\r\n\r\n> There should be a way for me to pass scrapy.cfg file path if the underlying code can't find its location properly.\r\n\r\n\"can't find its location properly\" (together with some of the earlier comments and proposed \"solution\") sounds like you expect Scrapy to find some random scrapy.cfg in some unrelated directory. I don't think that's possible to implement.\r\n\r\nAs for passing it explicitly, you can try reimplementing `get_project_settings()` instead of calling it, but be prepared that some Scrapy code that calls e.g. `inside_project()` will assume there is no active project."
    },
    {
        "Issue ID": 6459,
        "Issue State": "closed",
        "Issue Title": "get_project_settings() is not working",
        "Comment ID": 2640672787,
        "Author": "sandorkazi",
        "Created At": "2025-02-06T18:21:54Z",
        "Comment Body": "I did this as a workaround to use the location I wanted to:\n\n```python\ndef get_spider_loader() -> SpiderLoader:\n    cwd = os.getcwd()\n    os.chdir(loc)  # the location of the scrapy.cfg I wanted\n    settings = project.get_project_settings()\n    spider_loader = spiderloader.SpiderLoader.from_settings(settings)\n    os.chdir(cwd)\n    return spider_loader\n```"
    },
    {
        "Issue ID": 6653,
        "Issue State": "closed",
        "Issue Title": "Support dark mode in scrapy's documentation",
        "Comment ID": 2632981559,
        "Author": "codecov[bot]",
        "Created At": "2025-02-04T06:20:44Z",
        "Comment Body": "## [Codecov](https://app.codecov.io/gh/scrapy/scrapy/pull/6653?dropdown=coverage&src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scrapy) Report\nAll modified and coverable lines are covered by tests :white_check_mark:\n> Project coverage is 91.36%. Comparing base [(`ba5df62`)](https://app.codecov.io/gh/scrapy/scrapy/commit/ba5df629a2004ca0d919d8b7f0a7f5725448e50a?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scrapy) to head [(`40dd044`)](https://app.codecov.io/gh/scrapy/scrapy/commit/40dd0443fc0e716ffd28b3bd6a8a40f1ed65066c?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scrapy).\n> Report is 8 commits behind head on master.\n\n<details><summary>Additional details and impacted files</summary>\n\n\n```diff\n@@            Coverage Diff             @@\n##           master    #6653      +/-   ##\n==========================================\n+ Coverage   91.34%   91.36%   +0.01%     \n==========================================\n  Files         162      162              \n  Lines       12055    12050       -5     \n  Branches     1550     1551       +1     \n==========================================\n- Hits        11012    11009       -3     \n  Misses        735      735              \n+ Partials      308      306       -2     \n```\n\n[see 4 files with indirect coverage changes](https://app.codecov.io/gh/scrapy/scrapy/pull/6653/indirect-changes?src=pr&el=tree-more&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scrapy)\n\n</details>"
    },
    {
        "Issue ID": 6653,
        "Issue State": "closed",
        "Issue Title": "Support dark mode in scrapy's documentation",
        "Comment ID": 2639310923,
        "Author": "protokoul",
        "Created At": "2025-02-06T09:43:53Z",
        "Comment Body": "I think I should be able to change the style for API objects. Let me see. I also noticed that the toggle button placement can be improved for mobile screens. Will push some changes for that too"
    },
    {
        "Issue ID": 6653,
        "Issue State": "closed",
        "Issue Title": "Support dark mode in scrapy's documentation",
        "Comment ID": 2640209497,
        "Author": "protokoul",
        "Created At": "2025-02-06T15:50:34Z",
        "Comment Body": "I have raised a PR in the plugin's repo to improve the styles used in API object names. If it gets approved, then I can remove them from custom.css and update the plugin version.\r\n\r\n@Gallaecio @wRAR Shall I implement this for any other documentations as well?"
    },
    {
        "Issue ID": 6653,
        "Issue State": "closed",
        "Issue Title": "Support dark mode in scrapy's documentation",
        "Comment ID": 2640489614,
        "Author": "Gallaecio",
        "Created At": "2025-02-06T17:06:20Z",
        "Comment Body": "> Shall I implement this for any other documentations as well?\r\n\r\nI am tempted to say \u201cThat would be awesome!\u201d, but I wonder if it would not be better to get it fixed upstream first, so that applying the change in other repos becomes trivial. But I also suspect the upstream is dead."
    },
    {
        "Issue ID": 6653,
        "Issue State": "closed",
        "Issue Title": "Support dark mode in scrapy's documentation",
        "Comment ID": 2640500189,
        "Author": "protokoul",
        "Created At": "2025-02-06T17:10:57Z",
        "Comment Body": "Sure. If the plugin gets updated then I can raise new PR and likewise work on implementing this for other documentations if needed"
    },
    {
        "Issue ID": 6605,
        "Issue State": "closed",
        "Issue Title": "Fix Crawler.request_fingerprinter typing",
        "Comment ID": 2569718697,
        "Author": "codecov[bot]",
        "Created At": "2025-01-03T19:33:07Z",
        "Comment Body": "## [Codecov](https://app.codecov.io/gh/scrapy/scrapy/pull/6605?dropdown=coverage&src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scrapy) Report\nAll modified and coverable lines are covered by tests :white_check_mark:\n> Project coverage is 85.56%. Comparing base [(`176ae34`)](https://app.codecov.io/gh/scrapy/scrapy/commit/176ae348c57a536d661ca1b469035e7e1ccbca6a?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scrapy) to head [(`ec18a49`)](https://app.codecov.io/gh/scrapy/scrapy/commit/ec18a49e1266056a615187ab74d5cc1373e29ab1?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scrapy).\n> Report is 4 commits behind head on master.\n\n<details><summary>Additional details and impacted files</summary>\n\n\n```diff\n@@            Coverage Diff             @@\n##           master    #6605      +/-   ##\n==========================================\n+ Coverage   80.92%   85.56%   +4.63%     \n==========================================\n  Files         162      162              \n  Lines       12013    12018       +5     \n  Branches     1543     1543              \n==========================================\n+ Hits         9722    10283     +561     \n+ Misses       2038     1463     -575     \n- Partials      253      272      +19     \n```\n\n| [Files with missing lines](https://app.codecov.io/gh/scrapy/scrapy/pull/6605?dropdown=coverage&src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scrapy) | Coverage \u0394 | |\n|---|---|---|\n| [scrapy/crawler.py](https://app.codecov.io/gh/scrapy/scrapy/pull/6605?src=pr&el=tree&filepath=scrapy%2Fcrawler.py&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scrapy#diff-c2NyYXB5L2NyYXdsZXIucHk=) | `77.47% <100.00%> (\u00f8)` | |\n\n... and [24 files with indirect coverage changes](https://app.codecov.io/gh/scrapy/scrapy/pull/6605/indirect-changes?src=pr&el=tree-more&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scrapy)\n\n</details>"
    },
    {
        "Issue ID": 6655,
        "Issue State": "closed",
        "Issue Title": "Remove deprecated signals",
        "Comment ID": 2636248646,
        "Author": "codecov[bot]",
        "Created At": "2025-02-05T09:52:42Z",
        "Comment Body": "## [Codecov](https://app.codecov.io/gh/scrapy/scrapy/pull/6655?dropdown=coverage&src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scrapy) Report\nAll modified and coverable lines are covered by tests :white_check_mark:\n> Project coverage is 91.36%. Comparing base [(`cc48068`)](https://app.codecov.io/gh/scrapy/scrapy/commit/cc480680d784886bff97cdcdc789238187a56f3a?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scrapy) to head [(`9d35428`)](https://app.codecov.io/gh/scrapy/scrapy/commit/9d35428770326a3e833a2720c4f641fa70b58d29?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scrapy).\n> Report is 2 commits behind head on master.\n\n<details><summary>Additional details and impacted files</summary>\n\n\n```diff\n@@            Coverage Diff             @@\n##           master    #6655      +/-   ##\n==========================================\n- Coverage   91.36%   91.36%   -0.01%     \n==========================================\n  Files         162      162              \n  Lines       12055    12050       -5     \n  Branches     1550     1550              \n==========================================\n- Hits        11014    11009       -5     \n  Misses        735      735              \n  Partials      306      306              \n```\n\n| [Files with missing lines](https://app.codecov.io/gh/scrapy/scrapy/pull/6655?dropdown=coverage&src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scrapy) | Coverage \u0394 | |\n|---|---|---|\n| [scrapy/signals.py](https://app.codecov.io/gh/scrapy/scrapy/pull/6655?src=pr&el=tree&filepath=scrapy%2Fsignals.py&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scrapy#diff-c2NyYXB5L3NpZ25hbHMucHk=) | `100.00% <\u00f8> (\u00f8)` | |\n\n</details>"
    },
    {
        "Issue ID": 6657,
        "Issue State": "closed",
        "Issue Title": "fix: Reactor info logged twice",
        "Comment ID": 2638104072,
        "Author": "codecov[bot]",
        "Created At": "2025-02-05T21:53:19Z",
        "Comment Body": "## [Codecov](https://app.codecov.io/gh/scrapy/scrapy/pull/6657?dropdown=coverage&src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scrapy) Report\nAll modified and coverable lines are covered by tests :white_check_mark:\n> Project coverage is 91.36%. Comparing base [(`02ad6bd`)](https://app.codecov.io/gh/scrapy/scrapy/commit/02ad6bd1f6ab9623ac0c2a2b01c766d36e6c3c43?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scrapy) to head [(`48e6d95`)](https://app.codecov.io/gh/scrapy/scrapy/commit/48e6d95a771683ee73190ce3ceaf1b61c1bd2a9a?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scrapy).\n> Report is 1 commits behind head on master.\n\n<details><summary>Additional details and impacted files</summary>\n\n\n```diff\n@@           Coverage Diff           @@\n##           master    #6657   +/-   ##\n=======================================\n  Coverage   91.36%   91.36%           \n=======================================\n  Files         162      162           \n  Lines       12050    12050           \n  Branches     1550     1551    +1     \n=======================================\n  Hits        11009    11009           \n  Misses        735      735           \n  Partials      306      306           \n```\n\n| [Files with missing lines](https://app.codecov.io/gh/scrapy/scrapy/pull/6657?dropdown=coverage&src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scrapy) | Coverage \u0394 | |\n|---|---|---|\n| [scrapy/crawler.py](https://app.codecov.io/gh/scrapy/scrapy/pull/6657?src=pr&el=tree&filepath=scrapy%2Fcrawler.py&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scrapy#diff-c2NyYXB5L2NyYXdsZXIucHk=) | `97.30% <100.00%> (\u00f8)` | |\n\n</details>"
    },
    {
        "Issue ID": 6657,
        "Issue State": "closed",
        "Issue Title": "fix: Reactor info logged twice",
        "Comment ID": 2639467279,
        "Author": "wRAR",
        "Created At": "2025-02-06T10:47:29Z",
        "Comment Body": "Thanks!"
    },
    {
        "Issue ID": 6450,
        "Issue State": "closed",
        "Issue Title": "Twisted 24.7.0rc1 deprecation: \"twisted.web.http.HTTPClient was deprecated in Twisted 24.7.0rc1: Use twisted.web.client.Agent instead\"",
        "Comment ID": 2261149230,
        "Author": "Gallaecio",
        "Created At": "2024-07-31T18:24:32Z",
        "Comment Body": "Which version of Scrapy are you using? [I thought we addressed this in 2.4](https://github.com/scrapy/scrapy/pull/4165)."
    },
    {
        "Issue ID": 6450,
        "Issue State": "closed",
        "Issue Title": "Twisted 24.7.0rc1 deprecation: \"twisted.web.http.HTTPClient was deprecated in Twisted 24.7.0rc1: Use twisted.web.client.Agent instead\"",
        "Comment ID": 2261288121,
        "Author": "jpmckinney",
        "Created At": "2024-07-31T19:35:52Z",
        "Comment Body": "Using 2.11.2\r\n\r\nThe deprecated import is here:\r\n\r\nhttps://github.com/scrapy/scrapy/blob/03a15ced4f0a4284c75a917fdfb07c44b21f9ff2/scrapy/core/downloader/webclient.py#L10\r\n\r\nLink for default branch: https://github.com/scrapy/scrapy/blob/master/scrapy/core/downloader/webclient.py#L10"
    },
    {
        "Issue ID": 6450,
        "Issue State": "closed",
        "Issue Title": "Twisted 24.7.0rc1 deprecation: \"twisted.web.http.HTTPClient was deprecated in Twisted 24.7.0rc1: Use twisted.web.client.Agent instead\"",
        "Comment ID": 2614074519,
        "Author": "wRAR",
        "Created At": "2025-01-25T19:28:51Z",
        "Comment Body": "All of this is very old HTTP/1.0 code, which is not used by default for quite some years (you need to change the `DOWNLOAD_HANDLERS` setting for \"http\" from `scrapy.core.downloader.handlers.http.HTTPDownloadHandler` to `scrapy.core.downloader.handlers.http.HTTP10DownloadHandler` to use it, losing HTTP/1.1 support). I don't think it's possible to actually port it, so we can either merge the non-overridden parts of `twisted.web.http.HTTPClient` into `scrapy.core.downloader.webclient.ScrapyHTTPPageGetter` like we did with `twisted.web.client.HTTPClientFactory` in #4165, or deprecate and drop the code.\n\nIf we want to drop the code, we could extract it into a separate project but that one won't be maintained. Or we could simply drop it and maybe suggest something to use instead (a custom download handler based on... something? or, more likely, just an older Scrapy version)."
    },
    {
        "Issue ID": 6450,
        "Issue State": "closed",
        "Issue Title": "Twisted 24.7.0rc1 deprecation: \"twisted.web.http.HTTPClient was deprecated in Twisted 24.7.0rc1: Use twisted.web.client.Agent instead\"",
        "Comment ID": 2615351869,
        "Author": "Gallaecio",
        "Created At": "2025-01-27T10:19:18Z",
        "Comment Body": "I think we can just deprecate it and eventually drop it. No need to research into potential recommendations now. We can always try to guide anyone that shows an interest in implementing or maintaining a Scrapy plugin for this in the future."
    },
    {
        "Issue ID": 6450,
        "Issue State": "closed",
        "Issue Title": "Twisted 24.7.0rc1 deprecation: \"twisted.web.http.HTTPClient was deprecated in Twisted 24.7.0rc1: Use twisted.web.client.Agent instead\"",
        "Comment ID": 2618463089,
        "Author": "wRAR",
        "Created At": "2025-01-28T09:50:52Z",
        "Comment Body": "AFAICS it shouldn't be possible to get the warning without using the HTTP/1.0 code that we just deprecated. If (in the future) someone uses Scrapy that still imports this with Twisted that no longer has this class, they will need to upgrade Scrapy or, easier, downgrade Twisted."
    },
    {
        "Issue ID": 6450,
        "Issue State": "closed",
        "Issue Title": "Twisted 24.7.0rc1 deprecation: \"twisted.web.http.HTTPClient was deprecated in Twisted 24.7.0rc1: Use twisted.web.client.Agent instead\"",
        "Comment ID": 2638230225,
        "Author": "andersk",
        "Created At": "2025-02-05T23:04:15Z",
        "Comment Body": "@wRAR That is not true, it\u2019s pulled in via `scrapy.core.downloader.handlers.http11` (not `http10`). This affects a totally default Scrapy project, and can be reproduced just by following the [tutorial](https://docs.scrapy.org/en/latest/intro/tutorial.html) with the `PYTHONDEVMODE=1` environment variable set.\n\nHere\u2019s a traceback with `PYTHONWARNINGS=error`:\n\n```pytb\n2025-02-05 15:02:55 [scrapy.core.downloader.handlers] ERROR: Loading \"scrapy.core.downloader.handlers.http.HTTPDownloadHandler\" for scheme \"http\"\nTraceback (most recent call last):\n  File \"/tmp/v/lib/python3.12/site-packages/scrapy/core/downloader/handlers/__init__.py\", line 76, in _load_handler\n    dhcls: type[DownloadHandlerProtocol] = load_object(path)\n                                           ^^^^^^^^^^^^^^^^^\n  File \"/tmp/v/lib/python3.12/site-packages/scrapy/utils/misc.py\", line 71, in load_object\n    mod = import_module(module)\n          ^^^^^^^^^^^^^^^^^^^^^\n  File \"/nix/store/qrc496n6fsqp4p5m5h8wmw5d5jwyw5mr-python3-3.12.8/lib/python3.12/importlib/__init__.py\", line 90, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"<frozen importlib._bootstrap>\", line 1387, in _gcd_import\n  File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\n  File \"<frozen importlib._bootstrap>\", line 1331, in _find_and_load_unlocked\n  File \"<frozen importlib._bootstrap>\", line 935, in _load_unlocked\n  File \"<frozen importlib._bootstrap_external>\", line 999, in exec_module\n  File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n  File \"/tmp/v/lib/python3.12/site-packages/scrapy/core/downloader/handlers/http.py\", line 2, in <module>\n    from scrapy.core.downloader.handlers.http11 import (\n  File \"/tmp/v/lib/python3.12/site-packages/scrapy/core/downloader/handlers/http11.py\", line 30, in <module>\n    from scrapy.core.downloader.webclient import _parse\n  File \"/tmp/v/lib/python3.12/site-packages/scrapy/core/downloader/webclient.py\", line 10, in <module>\n    from twisted.web.http import HTTPClient\n  File \"/tmp/v/lib/python3.12/site-packages/twisted/python/deprecate.py\", line 483, in __getattribute__\n    value = deprecatedAttribute.get()\n            ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/v/lib/python3.12/site-packages/twisted/python/deprecate.py\", line 538, in get\n    warn(message, DeprecationWarning, stacklevel=3)\nDeprecationWarning: twisted.web.http.HTTPClient was deprecated in Twisted 24.7.0: Use twisted.web.client.Agent instead.\n```\n\nPlease reopen this."
    },
    {
        "Issue ID": 6450,
        "Issue State": "closed",
        "Issue Title": "Twisted 24.7.0rc1 deprecation: \"twisted.web.http.HTTPClient was deprecated in Twisted 24.7.0rc1: Use twisted.web.client.Agent instead\"",
        "Comment ID": 2638934542,
        "Author": "wRAR",
        "Created At": "2025-02-06T06:16:14Z",
        "Comment Body": "@andersk you are testing 2.12.0, not master."
    },
    {
        "Issue ID": 6450,
        "Issue State": "closed",
        "Issue Title": "Twisted 24.7.0rc1 deprecation: \"twisted.web.http.HTTPClient was deprecated in Twisted 24.7.0rc1: Use twisted.web.client.Agent instead\"",
        "Comment ID": 2638965736,
        "Author": "andersk",
        "Created At": "2025-02-06T06:41:32Z",
        "Comment Body": "My apologies, I assumed from the \u201cquite some years\u201d claim that 2.12.0 was thought to be new enough. I now see that this was fixed in the `http11` and `http2` handlers by #6635."
    },
    {
        "Issue ID": 6450,
        "Issue State": "closed",
        "Issue Title": "Twisted 24.7.0rc1 deprecation: \"twisted.web.http.HTTPClient was deprecated in Twisted 24.7.0rc1: Use twisted.web.client.Agent instead\"",
        "Comment ID": 2638979835,
        "Author": "wRAR",
        "Created At": "2025-02-06T06:51:51Z",
        "Comment Body": "Well, both things are true, `_parse()` didn't use the deprecated code, it was just defined in the module that imports `twisted.web.http.HTTPClient`."
    },
    {
        "Issue ID": 6656,
        "Issue State": "closed",
        "Issue Title": "docs: Remove AjaxCrawlMiddleware mention from built-in downloader middleware",
        "Comment ID": 2637408586,
        "Author": "codecov[bot]",
        "Created At": "2025-02-05T16:23:18Z",
        "Comment Body": "## [Codecov](https://app.codecov.io/gh/scrapy/scrapy/pull/6656?dropdown=coverage&src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scrapy) Report\nAll modified and coverable lines are covered by tests :white_check_mark:\n> Project coverage is 91.36%. Comparing base [(`9fd08a9`)](https://app.codecov.io/gh/scrapy/scrapy/commit/9fd08a92a8a112ede765a19d5c29d056b2c7325f?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scrapy) to head [(`2eb3c75`)](https://app.codecov.io/gh/scrapy/scrapy/commit/2eb3c75c697685af595b08023b4dc27d49403274?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scrapy).\n> Report is 2 commits behind head on master.\n\n<details><summary>Additional details and impacted files</summary>\n\n\n```diff\n@@           Coverage Diff           @@\n##           master    #6656   +/-   ##\n=======================================\n  Coverage   91.36%   91.36%           \n=======================================\n  Files         162      162           \n  Lines       12050    12050           \n  Branches     1550     1550           \n=======================================\n  Hits        11009    11009           \n  Misses        735      735           \n  Partials      306      306           \n```\n\n</details>"
    },
    {
        "Issue ID": 6652,
        "Issue State": "closed",
        "Issue Title": "Remove the AjaxCrawlMiddleware docs",
        "Comment ID": 2636293132,
        "Author": "Laerte",
        "Created At": "2025-02-05T10:11:20Z",
        "Comment Body": "Hey @Gallaecio so for this one we need to remove AjaxCrawlMiddleware from downloader-middleware.rst and only leave the mention on https://docs.scrapy.org/en/latest/news.html#scrapy-1-0-5-2016-02-04 or add a mention to deprecation on `news.rst` for the new release?\n\nThanks!"
    },
    {
        "Issue ID": 6652,
        "Issue State": "closed",
        "Issue Title": "Remove the AjaxCrawlMiddleware docs",
        "Comment ID": 2636822025,
        "Author": "Gallaecio",
        "Created At": "2025-02-05T13:12:35Z",
        "Comment Body": "> for this one we need to remove AjaxCrawlMiddleware from downloader-middleware.rst and only leave the mention on https://docs.scrapy.org/en/latest/news.html#scrapy-1-0-5-2016-02-04\n\nExactly."
    },
    {
        "Issue ID": 6654,
        "Issue State": "closed",
        "Issue Title": "Deprecate `stats_spider_*`, `item_passed` and `request_received` signal aliases",
        "Comment ID": 2636214191,
        "Author": "Laerte",
        "Created At": "2025-02-05T09:38:45Z",
        "Comment Body": "@wRAR I think is safe to remove altogether: \n\n- `stats_spider_*` https://docs.scrapy.org/en/latest/news.html#scrapy-0-16-0-released-2012-10-18\n- `item_passed`: https://docs.scrapy.org/en/latest/news.html#code-rearranged-and-removed\n\nOnly for `request_received` I did not saw any documentation mention, can I push the removal?"
    },
    {
        "Issue ID": 6654,
        "Issue State": "closed",
        "Issue Title": "Deprecate `stats_spider_*`, `item_passed` and `request_received` signal aliases",
        "Comment ID": 2636227635,
        "Author": "wRAR",
        "Created At": "2025-02-05T09:44:26Z",
        "Comment Body": "Yeah, please remove all of them."
    },
    {
        "Issue ID": 3852,
        "Issue State": "closed",
        "Issue Title": "Got no cookie support when using builtin FilesPipeline",
        "Comment ID": 997561213,
        "Author": "ngodinhnhien",
        "Created At": "2021-12-20T03:24:16Z",
        "Comment Body": "Is there any update for this?"
    },
    {
        "Issue ID": 3852,
        "Issue State": "closed",
        "Issue Title": "Got no cookie support when using builtin FilesPipeline",
        "Comment ID": 997838359,
        "Author": "rowheel",
        "Created At": "2021-12-20T11:25:18Z",
        "Comment Body": "> Is there any update for this?\n\nHmmm, no"
    },
    {
        "Issue ID": 6644,
        "Issue State": "closed",
        "Issue Title": "Unable to use CrawlerProcess to run a spider as a script to debug it at breakpoints",
        "Comment ID": 2628891959,
        "Author": "wRAR",
        "Created At": "2025-02-01T10:20:26Z",
        "Comment Body": "Works for me.\n\nIn any case this is very unlikely to be a problem in Scrapy."
    },
    {
        "Issue ID": 6644,
        "Issue State": "closed",
        "Issue Title": "Unable to use CrawlerProcess to run a spider as a script to debug it at breakpoints",
        "Comment ID": 2628991538,
        "Author": "synodriver",
        "Created At": "2025-02-01T15:16:29Z",
        "Comment Body": "I have exactly the same problem\n\n## Versions\n\n```\nscrapy version --verbose\nScrapy       : 2.12.0\nlxml         : 4.9.2.0\nlibxml2      : 2.9.12\ncssselect    : 1.2.0\nparsel       : 1.8.1\nw3lib        : 2.1.1\nTwisted      : 24.3.0\nPython       : 3.10.12 | packaged by conda-forge | (main, Jun 23 2023, 22:34:57) [MSC v.1936 64 bit (AMD64)]\npyOpenSSL    : 24.1.0 (OpenSSL 3.2.1 30 Jan 2024)\ncryptography : 42.0.7\nPlatform     : Windows-10-10.0.19045-SP0\n```"
    },
    {
        "Issue ID": 6644,
        "Issue State": "closed",
        "Issue Title": "Unable to use CrawlerProcess to run a spider as a script to debug it at breakpoints",
        "Comment ID": 2629037800,
        "Author": "wRAR",
        "Created At": "2025-02-01T17:28:27Z",
        "Comment Body": "I can't test it on Windows so it may be specific to the Windows version of PyCharm."
    },
    {
        "Issue ID": 6644,
        "Issue State": "closed",
        "Issue Title": "Unable to use CrawlerProcess to run a spider as a script to debug it at breakpoints",
        "Comment ID": 2633749836,
        "Author": "devfox-se",
        "Created At": "2025-02-04T12:23:17Z",
        "Comment Body": "oh yeah I see the problem, its `PyCharm2024.3` ! try older PyCharm version, `2024.2.5` should work fine!"
    },
    {
        "Issue ID": 6644,
        "Issue State": "closed",
        "Issue Title": "Unable to use CrawlerProcess to run a spider as a script to debug it at breakpoints",
        "Comment ID": 2634290963,
        "Author": "RishavDaredevil",
        "Created At": "2025-02-04T15:19:41Z",
        "Comment Body": "> oh yeah I see the problem, its `PyCharm2024.3` ! try older PyCharm version, `2024.2.5` should work fine!\n\nOk will try with this "
    },
    {
        "Issue ID": 2154,
        "Issue State": "closed",
        "Issue Title": "response encoding is not utf-8 and impossible to change it",
        "Comment ID": 236642347,
        "Author": "redapple",
        "Created At": "2016-08-01T17:04:42Z",
        "Comment Body": "@mayouf , I believe the encoding of the page is ISO-8859-1, close enough to cp1252.\n\nWebsite response headers contain `Content-Type:text/html; charset=ISO-8859-1`\nand the page itself has `<meta http-equiv=\"Content-Type\" content=\"text/html; charset=iso-8859-1\" />\n`\nIn scrapy shell, you can look for \"plut\u00f4t\" and check how it's encoded withing the raw bytes of `response.body`:\n\n```\n$ scrapy shell http://www.kelquartier.com/rhone_alpes_ain_commune_divonne_les_bains_01220-c1143/revenu_moyen.html\n2016-08-01 18:53:02 [scrapy] INFO: Scrapy 1.1.0 started (bot: scrapybot)\n...\n2016-08-01 18:53:02 [scrapy] INFO: Spider opened\n2016-08-01 18:53:03 [scrapy] DEBUG: Crawled (200) <GET http://www.kelquartier.com/rhone_alpes_ain_commune_divonne_les_bains_01220-c1143/revenu_moyen.html> (referer: None)\n...\nIn [1]: import re\n\nIn [2]: re.search(rb'(plut.{4})', response.body).groups()\nOut[2]: (b'plut\\xf4t p',)\n\nIn [3]: b'plut\\xf4t p'.decode('latin1')\nOut[3]: 'plut\u00f4t p'\n\nIn [4]: b'plut\\xf4t p'.decode('cp1252')\nOut[4]: 'plut\u00f4t p'\n\nIn [5]: b'plut\\xf4t p'.decode('ISO-8859-1')\nOut[5]: 'plut\u00f4t p'\n\nIn [6]: b'plut\\xf4t p'.decode('utf8')\n---------------------------------------------------------------------------\nUnicodeDecodeError                        Traceback (most recent call last)\n<ipython-input-6-f8baa01db51d> in <module>()\n----> 1 b'plut\\xf4t p'.decode('utf8')\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0xf4 in position 4: invalid continuation byte\nIn [7]: \n```\n\nThe raw bytes are from latin1/cp1252 encoding, not UTF8.\n\nBack to the `<` question,\nwhat version of scrapy and lxml/libxml2 are you using? (paste the output of `scrapy version -v)\n\nI'm using this:\n\n```\n$ scrapy version -v\nScrapy    : 1.1.0\nlxml      : 3.6.0.0\nlibxml2   : 2.9.3\nTwisted   : 16.2.0\nPython    : 3.5.1+ (default, Mar 30 2016, 22:46:26) - [GCC 5.3.1 20160330]\npyOpenSSL : 16.0.0 (OpenSSL 1.0.2g-fips  1 Mar 2016)\nPlatform  : Linux-4.4.0-31-generic-x86_64-with-Ubuntu-16.04-xenial\n```\n\nand I am able to get \"Densit\u00e9 de logements\" line:\n\n```\nIn [7]: for cell in response.xpath(u'//tr[normalize-space(td[1])=\"Densit\u00e9 de logements\"]//td'):\n    print(cell.xpath('normalize-space()').extract())\n   ....:     \n['Densit\u00e9 de logements']\n['1 log./ha']\n['']\n['<1 log./ha']\n```\n\nIt could be that the libxml version you have on your system is older than mine and having trouble parsing `<`, which should not appear like that in HTML text, rather as `&lt;` (so in theory, there's a bug in the website HTML).\nThe page has it correct for `Accueil > Divonne-les-Bains > Habitants - Revenu moyen` though:\n\n```\n          <div class=\"fil_ariane\">\n                <a href=\"/\">Accueil</a>\n                &gt;<a href=\"/rhone_alpes_ain_commune_divonne_les_bains_01220-c1143/revenu_moyen.html\">\n                                Divonne-les-Bains               </a>\n                &gt; <a href=\"/rhone_alpes_ain_commune_divonne_les_bains_01220-c1143/revenu_moyen.html\">Habitants - Revenu moyen</a>\n            </div>\n```\n"
    },
    {
        "Issue ID": 2154,
        "Issue State": "closed",
        "Issue Title": "response encoding is not utf-8 and impossible to change it",
        "Comment ID": 236991827,
        "Author": "mayouf",
        "Created At": "2016-08-02T18:11:02Z",
        "Comment Body": "here is my scrapy version:\n\n```\nmic@mic-ordi:~$ scrapy version --verbose\nScrapy    : 1.1.0\nlxml      : 3.6.0.0\nlibxml2   : 2.9.2\nTwisted   : 16.2.0\nPython    : 2.7.11 |Anaconda custom (64-bit)| (default, Jun 15 2016, 15:21:30) - [GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]\npyOpenSSL : 0.15.1 (OpenSSL 1.0.2h  3 May 2016)\nPlatform  : Linux-4.2.0-42-generic-x86_64-with-debian-jessie-sid\n```\n\nI even did a new intstall of libxml2 and it gives me the 2.9.2 version, I can't get the same as you.\n\n```\nmic@mic-ordi:~$ conda install libxml2\nFetching package metadata .......\nSolving package specifications: ..........\n\n# All requested packages already installed.\n# packages in environment at /home/mic/anaconda2:\n#\nlibxml2                   2.9.2                         0 \n```\n\nBUT I saw something in your response: You are using python 3.5 and I do with the 2.7 one.\n\nSo here is what I did and unfortunately, it did not show a difference:\n1. I created a python 3.5 environnement in Anaconda (http://conda.pydata.org/docs/py2or3.html)\n\n`conda create -n py35 python=3.5 anaconda\n`\n1. I installed scrapy with the conda command:\n   `conda install scrapy`\n2. I checked the scrapy version as you asked above:\n\n```\n(py35) mic@mic-ordi:~$ scrapy version -v\nScrapy    : 1.1.1\nlxml      : 3.6.0.0\nlibxml2   : 2.9.2\nTwisted   : 16.3.0\nPython    : 3.5.2 |Anaconda 4.1.1 (64-bit)| (default, Jul  2 2016, 17:53:06) - [GCC 4.4.7 20120313 (Red \nHat 4.4.7-1)]\npyOpenSSL : 16.0.0 (OpenSSL 1.0.2h  3 May 2016)\nPlatform  : Linux-4.2.0-42-generic-x86_64-with-debian-jessie-sid\n```\n1. I did the same command when I wrote this issue:\n\n`scrapy shell http://www.kelquartier.com/rhone_alpes_ain_commune_divonne_les_bains_01220-c1143/revenu_moyen.html`\n\n```\nIn [11]: [i.strip() for i in response.xpath('//tr[@id=\"carteNum_12\"]//td//text()').extract()]\nOut[11]: ['', 'Densit\u00e9 de logements', '', '1 log./ha', '', '', '']\n```\n\n```\nIn [12]: for cell in response.xpath(u'//tr[normalize-space(td[1])=\"Densit\u00e9 de logements\"]//td'):\n   ....:         print(cell.xpath('normalize-space()').extract())\n   ....:     \n['Densit\u00e9 de logements']\n['1 log./ha']\n['']\n['']\n```\n1. I also check if my ipython was on the right interpreter version:\n\n```\nIn [9]: import sys\nIn [10]: sys.version_info[0:30]\nOut[10]: (3, 5, 2, 'final', 0)\n```\n\nI don't really see what is wrong here....only the libxml2 library version 2.9.2 instead of 2.9.3 is left to explain this output difference we have...\n\nHow did you get the libxml 2.9.3 library please mate because neither my pip install or my conda upgrade provides me this version ?\n"
    },
    {
        "Issue ID": 2154,
        "Issue State": "closed",
        "Issue Title": "response encoding is not utf-8 and impossible to change it",
        "Comment ID": 241514909,
        "Author": "nyov",
        "Created At": "2016-08-22T19:00:02Z",
        "Comment Body": "I am not sure this is a bug?\nUsually in HTML/XML, `<` can not occur unescaped, it should be `&laquo;` or entity-encoded, so perhaps the parser considers it an invalid start tag in the code and eats it.\nMaybe @redapple has some version or workaround of `lxml` to relax the parsing there?\n\nPerhaps there is some way to configure `lxml.html.HTMLParser` to handle such issues, or perhaps it's something with debian's packaging of libxml2.\nI don't think that the difference of behavior would be in a minor version of libxml (2 to 3), but I could be wrong.\n\nFWIW, I'm also on debian with libxml 2.9.2\n\n```\n$ scrapy version --verbose\nScrapy    : 1.1.1\nlxml      : 3.6.0.0\nlibxml2   : 2.9.2\nTwisted   : 16.2.0\nPython    : 2.7.12 (default, Jun 29 2016, 08:18:26) - [GCC 5.4.0 20160609]\npyOpenSSL : 16.0.0 (OpenSSL 1.0.2h  3 May 2016)\nPlatform  : Linux-3.16.0-4-amd64-x86_64-with-debian-stretch-sid\n```\n\nand I get the same results:\n\n```\nIn [1]: for cell in response.xpath(u'//tr[normalize-space(td[1])=\"Densit\u00e9 de logements\"]//td'):\n   ...:         print(cell.xpath('normalize-space()').extract())\n   ...:     \n[u'Densit\\xe9 de logements']\n[u'1 log./ha']\n[u'']\n[u'']\n```\n"
    },
    {
        "Issue ID": 2154,
        "Issue State": "closed",
        "Issue Title": "response encoding is not utf-8 and impossible to change it",
        "Comment ID": 241707452,
        "Author": "redapple",
        "Created At": "2016-08-23T11:53:07Z",
        "Comment Body": "@mayouf, @nyov , I don't know what's different with my setup.\n\nI'm pasting console logs from a fresh Python 2.7 virtualenv creation, pip install of scrapy and scrapy shell session:\n\n```\n$ mkvirtualenv scrapy-gh-2154\nUsing real prefix '/usr'\nNew python executable in /home/paul/.virtualenvs/scrapy-gh-2154/bin/python2\nAlso creating executable in /home/paul/.virtualenvs/scrapy-gh-2154/bin/python\nInstalling setuptools, pip, wheel...done.\nvirtualenvwrapper.user_scripts creating /home/paul/.virtualenvs/scrapy-gh-2154/bin/predeactivate\nvirtualenvwrapper.user_scripts creating /home/paul/.virtualenvs/scrapy-gh-2154/bin/postdeactivate\nvirtualenvwrapper.user_scripts creating /home/paul/.virtualenvs/scrapy-gh-2154/bin/preactivate\nvirtualenvwrapper.user_scripts creating /home/paul/.virtualenvs/scrapy-gh-2154/bin/postactivate\nvirtualenvwrapper.user_scripts creating /home/paul/.virtualenvs/scrapy-gh-2154/bin/get_env_details\n\n(scrapy-gh-2154) paul@laptop:~$ pip install scrapy\nCollecting scrapy\n  Using cached Scrapy-1.1.2-py2.py3-none-any.whl\nCollecting w3lib>=1.14.2 (from scrapy)\n  Using cached w3lib-1.15.0-py2.py3-none-any.whl\nCollecting Twisted>=10.0.0 (from scrapy)\nCollecting pyOpenSSL (from scrapy)\n  Using cached pyOpenSSL-16.0.0-py2.py3-none-any.whl\nCollecting parsel>=0.9.3 (from scrapy)\n  Using cached parsel-1.0.3-py2.py3-none-any.whl\nCollecting PyDispatcher>=2.0.5 (from scrapy)\nCollecting queuelib (from scrapy)\n  Using cached queuelib-1.4.2-py2.py3-none-any.whl\nCollecting cssselect>=0.9 (from scrapy)\n  Using cached cssselect-0.9.2-py2.py3-none-any.whl\nCollecting lxml (from scrapy)\n  Using cached lxml-3.6.4-cp27-cp27mu-manylinux1_x86_64.whl\nCollecting service-identity (from scrapy)\n  Using cached service_identity-16.0.0-py2.py3-none-any.whl\nCollecting six>=1.5.2 (from scrapy)\n  Using cached six-1.10.0-py2.py3-none-any.whl\nCollecting zope.interface>=3.6.0 (from Twisted>=10.0.0->scrapy)\nCollecting cryptography>=1.3 (from pyOpenSSL->scrapy)\nCollecting pyasn1 (from service-identity->scrapy)\n  Using cached pyasn1-0.1.9-py2.py3-none-any.whl\nCollecting attrs (from service-identity->scrapy)\n  Using cached attrs-16.0.0-py2.py3-none-any.whl\nCollecting pyasn1-modules (from service-identity->scrapy)\n  Using cached pyasn1_modules-0.0.8-py2.py3-none-any.whl\nRequirement already satisfied (use --upgrade to upgrade): setuptools in ./.virtualenvs/scrapy-gh-2154/lib/python2.7/site-packages (from zope.interface>=3.6.0->Twisted>=10.0.0->scrapy)\nCollecting cffi>=1.4.1 (from cryptography>=1.3->pyOpenSSL->scrapy)\n  Using cached cffi-1.7.0-cp27-cp27mu-manylinux1_x86_64.whl\nCollecting ipaddress (from cryptography>=1.3->pyOpenSSL->scrapy)\n  Using cached ipaddress-1.0.16-py27-none-any.whl\nCollecting enum34 (from cryptography>=1.3->pyOpenSSL->scrapy)\n  Using cached enum34-1.1.6-py2-none-any.whl\nCollecting idna>=2.0 (from cryptography>=1.3->pyOpenSSL->scrapy)\n  Using cached idna-2.1-py2.py3-none-any.whl\nCollecting pycparser (from cffi>=1.4.1->cryptography>=1.3->pyOpenSSL->scrapy)\nInstalling collected packages: six, w3lib, zope.interface, Twisted, pycparser, cffi, ipaddress, enum34, idna, pyasn1, cryptography, pyOpenSSL, lxml, cssselect, parsel, PyDispatcher, queuelib, attrs, pyasn1-modules, service-identity, scrapy\nSuccessfully installed PyDispatcher-2.0.5 Twisted-16.3.2 attrs-16.0.0 cffi-1.7.0 cryptography-1.4 cssselect-0.9.2 enum34-1.1.6 idna-2.1 ipaddress-1.0.16 lxml-3.6.4 parsel-1.0.3 pyOpenSSL-16.0.0 pyasn1-0.1.9 pyasn1-modules-0.0.8 pycparser-2.14 queuelib-1.4.2 scrapy-1.1.2 service-identity-16.0.0 six-1.10.0 w3lib-1.15.0 zope.interface-4.2.0\n\n(scrapy-gh-2154) paul@laptop:~$ scrapy version -v\nScrapy    : 1.1.2\nlxml      : 3.6.4.0\nlibxml2   : 2.9.4\nTwisted   : 16.3.2\nPython    : 2.7.10 (default, Oct 14 2015, 16:09:02) - [GCC 5.2.1 20151010]\npyOpenSSL : 16.0.0 (OpenSSL 1.0.2g-fips  1 Mar 2016)\nPlatform  : Linux-4.4.0-34-generic-x86_64-with-Ubuntu-16.04-xenial\n\n\n(scrapy-gh-2154) paul@laptop:~$ pip freeze\nattrs==16.0.0\ncffi==1.7.0\ncryptography==1.4\ncssselect==0.9.2\nenum34==1.1.6\nidna==2.1\nipaddress==1.0.16\nlxml==3.6.4\nparsel==1.0.3\npyasn1==0.1.9\npyasn1-modules==0.0.8\npycparser==2.14\nPyDispatcher==2.0.5\npyOpenSSL==16.0.0\nqueuelib==1.4.2\nScrapy==1.1.2\nservice-identity==16.0.0\nsix==1.10.0\nTwisted==16.3.2\nw3lib==1.15.0\nzope.interface==4.2.0\n\n```\n\n```\n(scrapy-gh-2154) paul@laptop:~$ scrapy shell http://www.kelquartier.com/rhone_alpes_ain_commune_divonne_les_bains_01220-c1143/revenu_moyen.html\n2016-08-23 13:38:54 [scrapy] INFO: Scrapy 1.1.2 started (bot: scrapybot)\n2016-08-23 13:38:54 [scrapy] INFO: Overridden settings: {'LOGSTATS_INTERVAL': 0, 'DUPEFILTER_CLASS': 'scrapy.dupefilters.BaseDupeFilter'}\n2016-08-23 13:38:54 [scrapy] INFO: Enabled extensions:\n['scrapy.extensions.telnet.TelnetConsole',\n 'scrapy.extensions.corestats.CoreStats']\n2016-08-23 13:38:54 [scrapy] INFO: Enabled downloader middlewares:\n['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n 'scrapy.downl"
    },
    {
        "Issue ID": 2154,
        "Issue State": "closed",
        "Issue Title": "response encoding is not utf-8 and impossible to change it",
        "Comment ID": 241710714,
        "Author": "redapple",
        "Created At": "2016-08-23T12:09:27Z",
        "Comment Body": "@mayouf , another option is to try with [`html5lib`](http://lxml.de/html5parser.html):\n\n```\n$ pip install html5lib\nCollecting html5lib\nRequirement already satisfied (use --upgrade to upgrade): six in ./.virtualenvs/scrapy-gh-2154/lib/python2.7/site-packages (from html5lib)\nRequirement already satisfied (use --upgrade to upgrade): setuptools>=18.5 in ./.virtualenvs/scrapy-gh-2154/lib/python2.7/site-packages (from html5lib)\nCollecting webencodings (from html5lib)\nInstalling collected packages: webencodings, html5lib\nSuccessfully installed html5lib-0.999999999 webencodings-0.5\n\n(scrapy-gh-2154) paul@paul-SATELLITE-R830:~$ scrapy shell http://www.kelquartier.com/rhone_alpes_ain_commune_divonne_les_bains_01220-c1143/revenu_moyen.html\n2016-08-23 13:56:37 [scrapy] INFO: Scrapy 1.1.2 started (bot: scrapybot)\n(...)\n2016-08-23 13:56:39 [scrapy] DEBUG: Crawled (200) <GET http://www.kelquartier.com/rhone_alpes_ain_commune_divonne_les_bains_01220-c1143/revenu_moyen.html> (referer: None)\n(...)\n>>> from lxml.html import html5parser\n>>> response.text.find('<1 log')\n127742\n>>> print(response.text[127742-1000:127742+1000])\n\n                                        <td class=\"td_B\">\n                                        <a href=\"/rhone_alpes_ain_commune_divonne_les_bains_01220-c1143/densite_de_logements.html\" onclick=\"changeMap('/rhone_alpes_ain_commune_divonne_les_bains_01220-c1143/densite_de_logements.html');return false;\"> Densit\u00e9 de logements</a>\n                                        </td>\n                                        <td class=\"td_B\">1 log./ha</td>\n                                        <td align=\"center\" class=\"td_B\"  style=\"width:40px !important\">\n                                                                                                                                            <img src=\"../../../images/fr/fleche_nbr_horizontale.png\" width=\"13\" height=\"14\" />\n                                                                                                                                    </td>\n                                        <td class=\"td_B\"><1 log./ha</td>\n                                    </tr>\n\n\n\n                                    <tr class=\"hoverCarte cursor_pointer\" id=\"carteNum_13\" title=\"Allez \u00e0 la carte : Type de maison - Divonne-les-Bains\"  onclick=\"changeMap('/rhone_alpes_ain_commune_divonne_les_bains_01220-c1143/type_de_maison.html');return false;\">\n\n\n\n                                        <td class=\"td_A\">\n\n>>> doc = html5parser.fromstring(response.body)\n>>> doc.xpath('//x:tr[@id=\"carteNum_12\"]', namespaces={'x': 'http://www.w3.org/1999/xhtml'})\n[<Element {http://www.w3.org/1999/xhtml}tr at 0x7f948ccda7a0>]\n\n>>> doc.xpath('//x:tr[@id=\"carteNum_12\"]//text()', namespaces={'x': 'http://www.w3.org/1999/xhtml'})\n['\\n                                       \\n                                                                                                                     \\n                                    \\t                                    \\t                                    \\t                                                                                \\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n                                        ', '\\n                                        ', u' Densit\\xe9 de logements', '\\n                                        ', '\\n                                        ', '1 log./ha', '\\n                                        ', '\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t                                            \\t                                                ', '\\n                                                                                                                                    ', '\\n                                        ', '<1 log./ha', '\\n                                    ']\n\n>>> doc.xpath(u'''\n...     //x:tr[normalize-space(x:td[1])=\"Densit\u00e9 de logements\"]\n...         //x:td''', namespaces={'x': 'http://www.w3.org/1999/xhtml'})\n[<Element {http://www.w3.org/1999/xhtml}td at 0x7f948ccda7e8>, <Element {http://www.w3.org/1999/xhtml}td at 0x7f948ccdac20>, <Element {http://www.w3.org/1999/xhtml}td at 0x7f948ccdae18>, <Element {http://www.w3.org/1999/xhtml}td at 0x7f948ccde098>]\n>>> for cell in doc.xpath(u'''\n...     //x:tr[normalize-space(x:td[1])=\"Densit\u00e9 de logements\"]\n...         //x:td''', namespaces={'x': 'http://www.w3.org/1999/xhtml'}):\n...     print(cell.xpath('normalize-space()'))\n... \nDensit\u00e9 de logements\n1 log./ha\n\n<1 log./ha\n>>> \n```\n"
    },
    {
        "Issue ID": 2154,
        "Issue State": "closed",
        "Issue Title": "response encoding is not utf-8 and impossible to change it",
        "Comment ID": 241839147,
        "Author": "nyov",
        "Created At": "2016-08-23T18:55:10Z",
        "Comment Body": "@redapple, the virtualenv version works as you posted it. So I would assume at this point that debian's libxml2 version is responsible.\n"
    },
    {
        "Issue ID": 2154,
        "Issue State": "closed",
        "Issue Title": "response encoding is not utf-8 and impossible to change it",
        "Comment ID": 244031628,
        "Author": "redapple",
        "Created At": "2016-09-01T09:52:35Z",
        "Comment Body": "@mayouf , did you manage to get it to work in the end?\nDid you try the html5lib way?\n"
    },
    {
        "Issue ID": 2154,
        "Issue State": "closed",
        "Issue Title": "response encoding is not utf-8 and impossible to change it",
        "Comment ID": 247275523,
        "Author": "redapple",
        "Created At": "2016-09-15T09:05:02Z",
        "Comment Body": "hi @mayouf , do you have any updates?\n"
    },
    {
        "Issue ID": 2154,
        "Issue State": "closed",
        "Issue Title": "response encoding is not utf-8 and impossible to change it",
        "Comment ID": 247815852,
        "Author": "nyov",
        "Created At": "2016-09-18T00:16:52Z",
        "Comment Body": "Just close it already :p\nThey can always reopen when it suits them.\n"
    },
    {
        "Issue ID": 2154,
        "Issue State": "closed",
        "Issue Title": "response encoding is not utf-8 and impossible to change it",
        "Comment ID": 247930764,
        "Author": "redapple",
        "Created At": "2016-09-19T07:55:34Z",
        "Comment Body": "Correct.\n"
    },
    {
        "Issue ID": 6650,
        "Issue State": "closed",
        "Issue Title": "Remove a duplicate test.",
        "Comment ID": 2629405889,
        "Author": "codecov[bot]",
        "Created At": "2025-02-02T13:52:56Z",
        "Comment Body": "## [Codecov](https://app.codecov.io/gh/scrapy/scrapy/pull/6650?dropdown=coverage&src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scrapy) Report\nAll modified and coverable lines are covered by tests :white_check_mark:\n> Project coverage is 91.36%. Comparing base [(`340819e`)](https://app.codecov.io/gh/scrapy/scrapy/commit/340819eff095a76ca824874dbf1ea9f841716d54?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scrapy) to head [(`df68891`)](https://app.codecov.io/gh/scrapy/scrapy/commit/df688910e0499b0a874d220fe00ed7355a70fce0?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scrapy).\n> Report is 12 commits behind head on master.\n\n<details><summary>Additional details and impacted files</summary>\n\n\n```diff\n@@           Coverage Diff           @@\n##           master    #6650   +/-   ##\n=======================================\n  Coverage   91.35%   91.36%           \n=======================================\n  Files         162      162           \n  Lines       12052    12052           \n  Branches     1550     1550           \n=======================================\n+ Hits        11010    11011    +1     \n  Misses        735      735           \n+ Partials      307      306    -1     \n```\n\n[see 1 file with indirect coverage changes](https://app.codecov.io/gh/scrapy/scrapy/pull/6650/indirect-changes?src=pr&el=tree-more&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scrapy)\n\n</details>"
    },
    {
        "Issue ID": 5923,
        "Issue State": "closed",
        "Issue Title": "ValueError: Cannot use xpath on a Selector of type 'json'",
        "Comment ID": 1536191304,
        "Author": "Gallaecio",
        "Created At": "2023-05-05T12:30:04Z",
        "Comment Body": "I find it interesting that you were able to use XPath to parse HTML within a JSON structure before.\r\n\r\nIf you are looking for a workaround, downgrade parsel to <1.8.\r\n\r\nAs for a long-term solution, I am inclined to say that this is how things should work.\r\n\r\nIf you have `{\"html\": \"<html><title>foo</title></html>\"}`, you do not use `response.xpath(\"//title\")`, you use `response.selector.jmespath(\"html\").xpath(\"//tittle\")` (or, starting with the upcoming Scrapy 2.9, `response.jmespath(\"html\").xpath(\"//tittle\")`)."
    },
    {
        "Issue ID": 5923,
        "Issue State": "closed",
        "Issue Title": "ValueError: Cannot use xpath on a Selector of type 'json'",
        "Comment ID": 1536210143,
        "Author": "damodharheadrun",
        "Created At": "2023-05-05T12:46:18Z",
        "Comment Body": "Thanks for the quick update @Gallaecio \n\nIs this same for 2.5.0 also?"
    },
    {
        "Issue ID": 5923,
        "Issue State": "closed",
        "Issue Title": "ValueError: Cannot use xpath on a Selector of type 'json'",
        "Comment ID": 1536223212,
        "Author": "Gallaecio",
        "Created At": "2023-05-05T12:57:00Z",
        "Comment Body": "It affects any version of Scrapy if your version of Parsel is 1.8.0 or later."
    },
    {
        "Issue ID": 5923,
        "Issue State": "closed",
        "Issue Title": "ValueError: Cannot use xpath on a Selector of type 'json'",
        "Comment ID": 1536534244,
        "Author": "damodharheadrun",
        "Created At": "2023-05-05T17:00:01Z",
        "Comment Body": "ohh ok @Gallaecio  i will try above mentioned method and update here"
    },
    {
        "Issue ID": 5923,
        "Issue State": "closed",
        "Issue Title": "ValueError: Cannot use xpath on a Selector of type 'json'",
        "Comment ID": 1536589309,
        "Author": "GeorgeA92",
        "Created At": "2023-05-05T17:55:59Z",
        "Comment Body": "> I find it interesting that you were able to use XPath to parse HTML within a JSON structure before.\r\n\r\nOn parsel 1.6.0 and older - application add extra `<html>><body><p>`  tags that \"make\" it.. possible to use css xpath selectors (attempt to \"get\" valid html as browser does)\r\n\r\n```python\r\n\r\nfrom parsel import Selector\r\nselector_json = Selector(text='{\"a\":\"1\"}')\r\n\r\nprint(selector_json.getall())\r\n# parsel 1.6: ['<html><body><p>{\"a\":\"1\"}</p></body></html>']\r\n# parsel 1.8: [{'a': '1'}] # <-converted to dict, we see ' instead of original \", most likely expected to receive str here\"\r\n\r\n#print(selector_json.css('*::text').getall())\r\nprint(selector_json.css('p::text').getall())\r\n\r\n# parsel 1.6: ['{\"a\":\"1\"}']\r\n\r\n# parsel 1.8:\r\n'''\r\n    print(selector_json.css('p::text').getall())\r\n  File \"<redacted>\\parsel\\parsel\\selector.py\", line 680, in css\r\n    raise ValueError(\r\nValueError: Cannot use css on a Selector of type 'json'\r\n\r\nProcess finished with exit code 1\r\n\r\n'''\r\n```\r\n\r\n> If you have {\"html\": \"<html><title>foo</title></html>\"}, you do not use response.xpath(\"//title\"), you use response.selector.jmespath(\"html\").xpath(\"//tittle\") (or, starting with the upcoming Scrapy 2.9, response.jmespath(\"html\").xpath(\"//tittle\")).\r\n\r\nIn case if server return json response with.. html inside it's variables - Selector <strike>will be</strike> was able to parse it's html content by xpath/css selectors without any additional data transformations:\r\n\r\n```python\r\n\r\ntext2 = '''{\r\n\"prod_1\": \"<div class=product><div class=price>1$</div></div>\",\r\n\"prod_2\": \"<div class=product><div class=price>2$</div></div>\",\r\n\"prod_3\": \"<div class=product><div class=price>3$</div></div>\"}\r\n'''\r\n\r\nselector_html = Selector(text=text2)\r\nprint(selector_html.css('div.price::text').getall())\r\n# parsel 1.6: ['1$', '2$', '3$']\r\n# parsel 1.8:\r\n'''\r\n    print(selector_html.css('div.price::text').getall())\r\n  File \"<redacted>\\parsel\\parsel\\selector.py\", line 680, in css\r\n    raise ValueError(\r\nValueError: Cannot use css on a Selector of type 'json'\r\n'''\r\n```"
    },
    {
        "Issue ID": 5923,
        "Issue State": "closed",
        "Issue Title": "ValueError: Cannot use xpath on a Selector of type 'json'",
        "Comment ID": 1536631380,
        "Author": "damodharheadrun",
        "Created At": "2023-05-05T18:37:36Z",
        "Comment Body": "> \r\n\r\nThanks for the detailed information @GeorgeA92 "
    },
    {
        "Issue ID": 5923,
        "Issue State": "closed",
        "Issue Title": "ValueError: Cannot use xpath on a Selector of type 'json'",
        "Comment ID": 1619522675,
        "Author": "damodharheadrun",
        "Created At": "2023-07-04T05:34:06Z",
        "Comment Body": "from scrapy.selector import Selector\r\nresponse = Selector(text=str(data))\r\nresponse.xpath(\"//title\")\r\n\r\nabove soltion also work to avoid the Value error\r\n\r\n>>> data = '{\"test\": \"verify_xpath\", \"data\": \"<html>test</html>\"}'\r\n>>> from scrapy.selector import Selector\r\n>>> sel = Selector(text=data)\r\n>>> sel.xpath('.//text()').extract()\r\n['{\"test\": \"verify_xpath\", \"data\": \"test\"}']\r\n>>> \r\n"
    },
    {
        "Issue ID": 6647,
        "Issue State": "closed",
        "Issue Title": "Refactor downloader tests",
        "Comment ID": 2629107315,
        "Author": "codecov[bot]",
        "Created At": "2025-02-01T20:47:22Z",
        "Comment Body": "## [Codecov](https://app.codecov.io/gh/scrapy/scrapy/pull/6647?dropdown=coverage&src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scrapy) Report\nAll modified and coverable lines are covered by tests :white_check_mark:\n> Project coverage is 91.35%. Comparing base [(`340819e`)](https://app.codecov.io/gh/scrapy/scrapy/commit/340819eff095a76ca824874dbf1ea9f841716d54?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scrapy) to head [(`1066e37`)](https://app.codecov.io/gh/scrapy/scrapy/commit/1066e37e52c6fa32d1afa5c6b967e7f310b65c48?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scrapy).\n> Report is 10 commits behind head on master.\n\n<details><summary>Additional details and impacted files</summary>\n\n\n```diff\n@@           Coverage Diff           @@\n##           master    #6647   +/-   ##\n=======================================\n  Coverage   91.35%   91.35%           \n=======================================\n  Files         162      162           \n  Lines       12052    12052           \n  Branches     1550     1550           \n=======================================\n  Hits        11010    11010           \n  Misses        735      735           \n  Partials      307      307           \n```\n\n</details>"
    }
]

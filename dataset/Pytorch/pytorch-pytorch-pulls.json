[
    [
        146764,
        "Fix standalone runner for CUTLASS auto-tuning backend",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146764\n* #146755\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov",
        "open",
        "2025-02-08T17:23:45Z",
        null,
        null,
        "open source, topic: not user facing, module: inductor, ciflow/inductor",
        "gh/alexsamardzic/26/base",
        "gh/alexsamardzic/26/head",
        1,
        64,
        8,
        2,
        2,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146764"
    ],
    [
        146763,
        "[Break XPU] Align meta calculation for fft_r2c with _fft_r2c_mkl",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146763\n* #145248\n* #146762\n\nFix #146761",
        "open",
        "2025-02-08T16:20:39Z",
        null,
        null,
        "open source, ciflow/trunk, topic: not user facing, ciflow/inductor, ciflow/xpu",
        "gh/etaf/98/base",
        "gh/etaf/98/head",
        1,
        3,
        1,
        1,
        1,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146763"
    ],
    [
        146762,
        "[Break XPU][Inductor UT] Fix XPU Inductor UT introduced from community.",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #146763\n* #145248\n* __->__ #146762\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov",
        "open",
        "2025-02-08T16:20:35Z",
        null,
        null,
        "open source, topic: not user facing, module: inductor, ciflow/inductor",
        "gh/etaf/97/base",
        "gh/etaf/97/head",
        4,
        10,
        2,
        1,
        1,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146762"
    ],
    [
        146756,
        "[Inductor][CPU] Add GEMM tamplates for _weight_int4pack_mm_for_cpu with AVX512",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146756\n* #145250\n* #145245\n\r\n**Summary**\r\nIt's part of the task to enable max-autotune with GEMM template for WoQ INT4 GEMM on CPU. Early work: #133846\r\n\r\nThis PR adds GEMM templates for `torch.ops.aten_weight_int4pack_mm_for_cpu`. The micro kernel used for the templates is based on AVX512 and it's a copy of the ATen implementation of `torch.ops.aten_weight_int4pack_mm_for_cpu` with minor changes.\r\n\r\nDue to better blocking and loop schedule, the GEMM template based implementation outperforms the ATen implementation in all cases we tested.\r\n\r\n**Test plan**\r\n```\r\npython test/inductor/test_cpu_select_algorithm.py -k test_int4_woq_mm_avx512\r\n```\r\n\r\ncc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @voznesenskym @penguinwu @EikanWang @Guobing-Chen @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov",
        "open",
        "2025-02-08T13:51:09Z",
        null,
        null,
        "open source, ciflow/trunk, topic: not user facing, intel, module: inductor, ciflow/inductor",
        "gh/Xia-Weiwen/30/base",
        "gh/Xia-Weiwen/30/head",
        8,
        468,
        21,
        2,
        1,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146756"
    ],
    [
        146755,
        "Fix CUTLASS 2.x kernels for auto-tuning",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #146764\n* __->__ #146755\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov",
        "open",
        "2025-02-08T12:01:14Z",
        null,
        null,
        "open source, topic: not user facing, module: inductor, ciflow/inductor",
        "gh/alexsamardzic/25/base",
        "gh/alexsamardzic/25/head",
        3,
        12,
        30,
        1,
        1,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146755"
    ],
    [
        146754,
        "[MPS] fix inverse bug for N>1024",
        "Fixes #138200 \r\n",
        "open",
        "2025-02-08T10:24:29Z",
        null,
        null,
        "open source, release notes: mps",
        "main",
        "mps-inverse-bugfix",
        4,
        26,
        27,
        1,
        1,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146754"
    ],
    [
        146753,
        "[MPS] fix lu factor for large tensors with bs>1",
        "Try this:\r\n```\r\nimport torch\r\n\r\nbatch_size = 2\r\nA = torch.eye(256, device=\"mps\")[None, :, :].expand(batch_size, -1, -1) + 0.1 * torch.randn((batch_size, 256, 256), device=\"mps\")\r\nA_cpu = A.cpu()\r\nLU_cpu, pivots_cpu = torch.linalg.lu_factor(A_cpu)\r\nLU, pivots = torch.linalg.lu_factor(A)\r\ntorch.testing.assert_close(LU.cpu(), LU_cpu)\r\n```\r\nYou'll get huge difference in LU tensors\r\n<img width=\"706\" alt=\"Screenshot 2025-02-08 at 12 14 39\" src=\"https://github.com/user-attachments/assets/b45f2b3c-e0a5-49c8-aa07-42792150b781\" />\r\n",
        "open",
        "2025-02-08T08:15:29Z",
        null,
        null,
        "open source, release notes: mps",
        "main",
        "lu-factor-fix-batches",
        2,
        7,
        5,
        1,
        1,
        0,
        "Isalia20",
        "COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146753"
    ],
    [
        146752,
        "realize stride symbols in estimate_runtime",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\r\n* __->__ #146752\r\n\r\nUnfortuanlty could not create a local repo, or unit test.\r\nfix https://github.com/pytorch/pytorch/issues/146686\r\n\r\n",
        "open",
        "2025-02-08T07:30:03Z",
        null,
        null,
        "topic: not user facing, ciflow/inductor",
        "gh/laithsakka/107/base",
        "gh/laithsakka/107/head",
        1,
        2,
        1,
        1,
        1,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146752"
    ],
    [
        146751,
        "[MTIA] (4/n) Implement PyTorch APIs to query/reset device peak memory usage",
        "Summary: Public summary (shared with Github): This diff updates the unit test for the PyTorch API \"reset_peak_memory_stats\".\n\nTest Plan:\n```\nbuck2 test //mtia/host_runtime/torch_mtia/tests:test_torch_mtia_api -- -r test_reset_peak_memory_stats\n```\n\nhttps://www.internalfb.com/intern/testinfra/testrun/9007199321947161\n\nReviewed By: yuhc\n\nDifferential Revision: D68989900\n\n\n",
        "open",
        "2025-02-08T07:16:36Z",
        null,
        null,
        "fb-exported",
        "main",
        "export-D68989900",
        2,
        17,
        0,
        1,
        3,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146751"
    ],
    [
        146750,
        "Update instructions about faster linker",
        "This PR adds instructions to specify linker via cmake env `CMAKE_LINKER_TYPE` and also adds `mold` as a linker alternative.\r\n\r\nSince 3.29, cmake introduced [`CMAKE_LINKER_TYPE`](https://cmake.org/cmake/help/latest/variable/CMAKE_LINKER_TYPE.html) that can specify linker without overwriting `ld` file or changing build script.\r\n\r\n`mold` is already stable and **the fastest** (afaict) linker out there, and also easier to install compared with `lld`. So I added it here. After switching to `mold`, the time of linking `libtorch_cuda.so` has been reduced from ~7s to ~0.6s locally.\r\n\r\nAlso note `gold` has been marked deprecated recently[1].\r\n\r\n[1] https://lwn.net/Articles/1007541/",
        "open",
        "2025-02-08T06:30:17Z",
        null,
        null,
        "open source, topic: not user facing",
        "main",
        "update-linker",
        1,
        6,
        8,
        2,
        1,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146750"
    ],
    [
        146748,
        "Update strided test to float32",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\r\n* __->__ #146748\r\n\r\nFixes #146377\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov",
        "open",
        "2025-02-08T04:48:51Z",
        null,
        null,
        "ciflow/trunk, topic: not user facing, module: inductor, ciflow/inductor",
        "gh/drisspg/122/base",
        "gh/drisspg/122/head",
        1,
        3,
        3,
        1,
        6,
        1,
        "BoyuanFeng, leijurv",
        "APPROVED, APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146748"
    ],
    [
        146747,
        "Add hint message for `pack_padded_sequence`",
        "Fixes #144207\r\n\r\nAdd truncate hint message in docs [torch.nn.utils.rnn.pack_padded_sequence](https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pack_padded_sequence.html)\r\n\r\n## Test Result\r\n\r\n![image](https://github.com/user-attachments/assets/46258f36-f6c7-4f11-9213-8513e52a9001)\r\n\r\n",
        "open",
        "2025-02-08T04:28:15Z",
        null,
        null,
        "open source, topic: not user facing",
        "main",
        "opt/nn/rnn",
        1,
        5,
        1,
        1,
        2,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146747"
    ],
    [
        146746,
        "[Inductor] Fix the lowering of squeeze when input is not contiguous",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146746\n\r\n**Summary**\r\nFix issue https://github.com/pytorch/pytorch/issues/143498. The issue happens when we lowering `select = torch.ops.aten.select.int(cat, 1, 0)`. \r\n\r\nFor example, when `cat` is contiguous with size[2, 2] stride[2,1]\r\n\r\n- for eager, it returns a view of size[2,] stride[2,]\r\n- for Inductor lowering, it returns wrong stride 1 instead of 2\r\n```\r\nTensorBox(\r\n  ReinterpretView(\r\n    StorageBox(\r\n      ConcatKernel(name='buf10', layout=FixedLayout('cpu', torch.int64, size=[u0, 2], stride=[2, 1]), inputs=[ComputedBuffer(name='buf8', layout=NonOwningLayout('cpu', torch.int64, size=[u0, 1], stride=[2, 1]), data=Pointwise(device=device(type='cpu'), dtype=torch.int64, inner_fn=<function ReinterpretView.make_loader.<locals>.loader at 0x7f6b856449d0>, ranges=[u0, 1])), ComputedBuffer(name='buf9', layout=NonOwningLayout('cpu', torch.int64, size=[u0, 1], stride=[2, 1]), data=Pointwise(device=device(type='cpu'), dtype=torch.int64, inner_fn=<function ReinterpretView.make_loader.<locals>.loader at 0x7f6b85644790>, ranges=[u0, 1]))])\r\n    ),\r\n    FixedLayout('cpu', torch.int64, size=[u0], stride=[**1**]),\r\n    origins=OrderedSet([select])\r\n  )\r\n)\r\n```\r\n\r\nTo fix this issue, we give the right stride when lowering of `squeeze`.\r\n\r\n**Test Plan**\r\n```\r\npython -u -m pytest -s -v test/inductor/test_unbacked_symints.py -k test_issue_143498\r\n```\r\n\r\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov",
        "open",
        "2025-02-08T03:50:25Z",
        null,
        null,
        "open source, ciflow/trunk, topic: not user facing, module: inductor, ciflow/inductor",
        "gh/leslie-fang-intel/180/base",
        "gh/leslie-fang-intel/180/head",
        2,
        16,
        4,
        2,
        1,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146746"
    ],
    [
        146743,
        "[cutlass backend] refactor tests to remove duplicate logic",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146743\n* #146356\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov",
        "open",
        "2025-02-08T01:36:44Z",
        null,
        null,
        "topic: not user facing, module: inductor, ciflow/inductor",
        "gh/henrylhtsang/3/base",
        "gh/henrylhtsang/3/head",
        1,
        57,
        81,
        1,
        1,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146743"
    ],
    [
        146742,
        "[Dynamo][autograd.Function] Relax backward speculation strict mode: support .requires_grad",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146742\n* #146741\n* #146571\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-02-08T01:30:53Z",
        null,
        null,
        "topic: not user facing, module: dynamo, ciflow/inductor",
        "gh/yanboliang/65/base",
        "gh/yanboliang/65/head",
        2,
        27,
        1,
        1,
        1,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146742"
    ],
    [
        146741,
        "[Dynamo][autograd.Function] Relax backward speculation strict mode: support .data",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #146742\n* __->__ #146741\n* #146571\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-02-08T01:30:49Z",
        null,
        null,
        "topic: not user facing, module: dynamo, ciflow/inductor",
        "gh/yanboliang/64/base",
        "gh/yanboliang/64/head",
        2,
        29,
        1,
        1,
        1,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146741"
    ],
    [
        146739,
        "Testing",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146739\n* #145748\n\nThis reverts commit 5cd5b4d2d54c0220b92ee488dd36d789c9b60af3.",
        "open",
        "2025-02-08T00:30:57Z",
        null,
        null,
        "release notes: releng, ciflow/binaries_wheel",
        "gh/mikaylagawarecki/312/base",
        "gh/mikaylagawarecki/312/head",
        8,
        25,
        80,
        2,
        1,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146739"
    ],
    [
        146738,
        "[audio hash update] update the pinned audio hash",
        "This PR is auto-generated nightly by [this action](https://github.com/pytorch/pytorch/blob/main/.github/workflows/nightly.yml).\nUpdate the pinned audio hash.",
        "open",
        "2025-02-08T00:23:09Z",
        null,
        null,
        "open source, ciflow/trunk, topic: not user facing, ciflow/inductor, merging",
        "main",
        "update-audio-commit-hash/13210264744-1454-1",
        1,
        1,
        1,
        1,
        4,
        0,
        "pytorchbot",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146738"
    ],
    [
        146737,
        "[dynamo][user-defined] Unify standard and non-standard __new__ codebase",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #146678\n* __->__ #146737\n* #146677\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-02-08T00:21:25Z",
        null,
        null,
        "ciflow/trunk, topic: not user facing, module: dynamo, ciflow/inductor, keep-going",
        "gh/anijain2305/676/base",
        "gh/anijain2305/676/head",
        1,
        17,
        28,
        2,
        1,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146737"
    ],
    [
        146736,
        "Document dynamo",
        "Many files in dynamo are currently lacking file/module-level documentation, which makes it hard to know what they do at a glance and without digging into the code. This fixes that.\r\n\r\nNote: documentation was AI-generated and could be incorrect, please review carefully.\r\n\r\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @xmfan @svekars @brycebortree @sekyondaMeta @AlannaBurke",
        "open",
        "2025-02-08T00:15:41Z",
        null,
        null,
        "better-engineering, ciflow/trunk, topic: not user facing, module: inductor, module: dynamo, ciflow/inductor, module: compiled autograd",
        "main",
        "gh/raymo/document-dynamo",
        30,
        601,
        45,
        1,
        4,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146736"
    ],
    [
        146735,
        "[ca] log graph before reodering passes",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146735\n* #146720\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @yf225",
        "open",
        "2025-02-07T23:39:50Z",
        null,
        null,
        "module: dynamo, ciflow/inductor, module: compiled autograd",
        "gh/xmfan/177/base",
        "gh/xmfan/177/head",
        1,
        12,
        0,
        1,
        2,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146735"
    ],
    [
        146734,
        "[CUDA][CUDNN][SDPA] Pass dropout seed and offset to cuDNN in `int64`",
        "Workaround for limitation in cuDNN that does not accept dropout seed/offset in `int32` for SM 10.0 kernels.\n\ncc @csarofeen @ptrblck @xwang233 @msaroufim",
        "open",
        "2025-02-07T23:30:33Z",
        null,
        null,
        "module: cudnn, module: cuda, triaged, open source, topic: not user facing, module: sdpa",
        "main",
        "cudnnsm100dropoutworkaround",
        1,
        31,
        12,
        1,
        1,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146734"
    ],
    [
        146733,
        "[CUDA][SDPA] Don't dispatch to mem eff attn for batch_size >= 65536",
        "#146704\n\ncc @ptrblck @msaroufim",
        "open",
        "2025-02-07T23:29:27Z",
        null,
        null,
        "module: cuda, open source, topic: not user facing, module: sdpa",
        "main",
        "memeff65536",
        2,
        23,
        0,
        1,
        2,
        0,
        "drisspg, drisspg, nikitaved",
        "COMMENTED, APPROVED, COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146733"
    ],
    [
        146731,
        "dont specialize symints when testing truthiness",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #133044\n* #146729\n* #146642\n* __->__ #146731\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-02-07T22:48:49Z",
        null,
        null,
        "module: dynamo, ciflow/inductor",
        "gh/bdhirsh/641/base",
        "gh/bdhirsh/641/head",
        2,
        20,
        1,
        1,
        3,
        1,
        "bobrenjc93",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146731"
    ],
    [
        146730,
        "[BaseHOP] change hop(subgraph, operands) to hop(subgraph, *operands)",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146730\n* #146727\n\nOur three main users are OK with this, with two of them (foreach_map,\ninvoke_quant) prefering it like this.\n\nI was originally worried about BC issues (this now means you cannot add\nany positional args) but I think that's not a concern -- one can always\nadd kwonly args.\n\nTest Plan\n- tests\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov",
        "open",
        "2025-02-07T22:42:43Z",
        null,
        null,
        "release notes: foreach_frontend, module: inductor, module: dynamo, ciflow/inductor",
        "gh/zou3519/1130/base",
        "gh/zou3519/1130/head",
        6,
        35,
        48,
        1,
        1,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146730"
    ],
    [
        146729,
        "support meta_tensor.to(device='cpu') under fake_mode",
        "Fixing this is actually a bit annoying:\r\n\r\n(1) FakeTensorMode sees a function where all of its inputs are real tensors, so it tries to run the real compute before converting the output to a FakeTensor\r\n\r\n(2) we don't actually want this, because the \"real compute\" is support to error normally, when you do `meta_tensor.to(device='cpu')`. Instead, we want FakeTensor to actually skip constant prop and run the normal FakeTensor implementation, which will not error\r\n\r\nStack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #133044\n* __->__ #146729\n* #146642\n* #146731\n\r\n",
        "open",
        "2025-02-07T22:19:32Z",
        null,
        null,
        "ciflow/inductor",
        "gh/bdhirsh/640/base",
        "gh/bdhirsh/640/head",
        2,
        12,
        0,
        2,
        2,
        0,
        "zou3519, zou3519, bdhirsh",
        "COMMENTED, APPROVED, COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146729"
    ],
    [
        146728,
        "[StaticRuntime] Fix a bug that memory planner ignores subblocks",
        "Summary: When Static Runtime graph node has sub-blocks, the memory planner does not consider sub-blocks' inputs as a node's input in memory planner. As the result, such nodes' inputs' lifetime is incorrect and corresponding tensor memory is released earlier than required and causes errors.\n\nDifferential Revision: D69195886\n\n\n\n\ncc @EikanWang @jgong5 @wenzhe-nrv @sanchitintel",
        "open",
        "2025-02-07T22:13:28Z",
        null,
        null,
        "oncall: jit, fb-exported, release notes: jit",
        "main",
        "export-D69195886",
        3,
        75,
        8,
        1,
        7,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146728"
    ],
    [
        146727,
        "Rename PrimHOPBase to BaseHOP + minor changes",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #146730\n* __->__ #146727\n\nThis PR:\n- renames PrimHOPBase to BaseHOP\n- changes the backward pass to always return a tuple (to match the\n  forward pass).\n\nTest Plan:\n- tests\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-02-07T22:00:22Z",
        null,
        null,
        "release notes: foreach_frontend, module: dynamo, ciflow/inductor",
        "gh/zou3519/1129/base",
        "gh/zou3519/1129/head",
        6,
        18,
        18,
        1,
        1,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146727"
    ],
    [
        146726,
        "[ez][BE] get rid of the extra printf('\\n')",
        "Summary: as title\n\nTest Plan:\n```\nAOT_INDUCTOR_DEBUG_INTERMEDIATE_VALUE_PRINTER=3  TORCHINDUCTOR_FORCE_DISABLE_CACHES=1 TORCHINDUCTOR_ABI_COMPATIBLE=1 TORCH_COMPILE_DEBUG=1 TORCH_LOGS=\"+graph, inductor, +schedule, output_code\" buck2 run -c fbcode.enable_gpu_sections=true -c fbcode.nvcc_arch=h100a @//mode/opt fbcode//caffe2/test/inductor:test_aot_inductor -- -r test_addmm_cuda\n```\n\nDifferential Revision: D69328701\n\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov",
        "open",
        "2025-02-07T21:57:49Z",
        null,
        null,
        "fb-exported, ciflow/trunk, topic: not user facing, module: inductor, ciflow/inductor, merging",
        "main",
        "export-D69328701",
        1,
        1,
        2,
        1,
        5,
        0,
        "ColinPeppler",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146726"
    ],
    [
        146723,
        "torch: Log a unified waitcounter for torch.compile and triton.autotune",
        "Summary: Add a second more generic waitcounter to torch.compile. We'll keep expanding this as new generic pytorch compilation sites show up.\n\nTest Plan: Waitcounter only change, relying on existing tests.\n\nDifferential Revision: D69215401\n\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov",
        "open",
        "2025-02-07T20:59:32Z",
        null,
        null,
        "fb-exported, ciflow/trunk, topic: not user facing, module: inductor, ciflow/inductor",
        "main",
        "export-D69215401",
        1,
        3,
        1,
        1,
        3,
        0,
        "davidberard98",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146723"
    ],
    [
        146722,
        "Test on in-graph constructed NJTs",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146722\n* #146721\n\r\nA recent set of bugs has been cropping up related to NJTs that constructed in-graph within a compiled function. This exercises different paths related to symbolic nested ints, etc. Some examples:\r\n* #145874\r\n* #146644\r\n\r\nTo get ahead of these, we should do NJT testing for this case as well.\r\n\r\nThis PR parametrizes the OpInfo tests for compile + forward to cover both in-graph constructed NJT and normal input cases. TBD what fails..\r\n\r\nTODO:\r\n* Do this for compile + backward tests also (?)",
        "open",
        "2025-02-07T20:09:15Z",
        null,
        null,
        "topic: not user facing",
        "gh/jbschlosser/229/base",
        "gh/jbschlosser/229/head",
        2,
        44,
        6,
        2,
        1,
        0,
        "jbschlosser, cpuhrsch, soulitzer",
        "COMMENTED, APPROVED, APPROVED",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146722"
    ],
    [
        146721,
        "Use inductor backend for NJT compile tests",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #146722\n* __->__ #146721\n\r\nWe've been using `backend=\"aot_eager_decomp_partition\"` for NJT compile testing, but this can let inductor bugs slip through. This PR switches the compile tests to use `backend=\"inductor\"`; let's see if test runtime is an issue after this.",
        "open",
        "2025-02-07T20:09:11Z",
        null,
        null,
        "topic: not user facing",
        "gh/jbschlosser/228/base",
        "gh/jbschlosser/228/head",
        1,
        3,
        7,
        2,
        1,
        0,
        "soulitzer",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146721"
    ],
    [
        146720,
        "[ca] remove private API: _compiled_autograd_should_lift",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #146735\n* __->__ #146720\n\r\nSince the functional autograd + compiled autograd migration, we don't trace into nodes anymore, and everything is lifted. We can't support this flag which tries to inline make_fx style in CA initial pass. There's no more usage internally.",
        "open",
        "2025-02-07T20:06:08Z",
        null,
        null,
        "ciflow/trunk, ciflow/inductor, release notes: dynamo",
        "gh/xmfan/176/base",
        "gh/xmfan/176/head",
        4,
        0,
        15,
        1,
        7,
        0,
        "zou3519",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146720"
    ],
    [
        146718,
        "[CUDAGraph] add skip message for unbacked symint",
        "Add explicit skip message for unbacked symint in cudagraph, as suggested by @bdhirsh.\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov",
        "open",
        "2025-02-07T19:29:37Z",
        null,
        null,
        "ciflow/trunk, topic: not user facing, module: inductor, module: dynamo, ciflow/inductor",
        "main",
        "bf/cg-skip-unbacked-symint-msg",
        4,
        31,
        14,
        1,
        1,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146718"
    ],
    [
        146717,
        "[BE][cuDNN] cuDNN to 9.7.1.26 for CUDA 12.8",
        "cuDNN 9.7.1 is out now and is expected to be the longer-lived branch with more potential backports vs. 9.7.0\r\n\r\nCC @nWEIdia @tinglvv \n\ncc @malfet @seemethere @csarofeen @ptrblck @xwang233",
        "open",
        "2025-02-07T19:25:00Z",
        null,
        null,
        "module: build, module: cudnn, triaged, open source, topic: not user facing, topic: build",
        "main",
        "cudnn971",
        6,
        14,
        12,
        1,
        1,
        0,
        "drisspg",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146717"
    ],
    [
        146716,
        "[BE] Remove outdated RPC benchmark",
        "We have lots of outdated unused + uncalled code in our codebase, namely in our benchmarks and examples folders among others. The last change to this directory was 4 years ago and this code looks dead. cc @albanD @H-Huang for feedback\r\n\r\nStack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\r\n* __->__ #146716\r\n\r\n",
        "open",
        "2025-02-07T18:52:53Z",
        null,
        null,
        "release notes: distributed (rpc), skip-pr-sanity-checks",
        "gh/janeyx99/223/base",
        "gh/janeyx99/223/head",
        29,
        0,
        2535,
        1,
        1,
        0,
        "Skylion007, H-Huang",
        "APPROVED, APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146716"
    ],
    [
        146715,
        "[export][ez] Allow math.trunc for serialization.",
        "Summary: as title.\n\nTest Plan: CI\n\nDifferential Revision: D69317084\n\n\n",
        "open",
        "2025-02-07T18:24:55Z",
        null,
        null,
        "fb-exported, ciflow/trunk, ciflow/inductor, release notes: export",
        "main",
        "export-D69317084",
        1,
        1,
        0,
        1,
        2,
        0,
        "angelayi",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146715"
    ],
    [
        146714,
        "[hop] Support more output types for `flat_apply`",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #146367\n* __->__ #146714\n* #146713\n\nThis patch enables `flat_apply` to support certain non-Tensor output\ntypes like containers and graphable types. This will in turn enable the\nupcoming `mark_traceable` to support more output types.\n\nThe patch also exposes a `func_to_graphable` rather than having the\nusers calling the lower level `pytree.flatten(ConstantFunction(...))`.\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-02-07T17:43:34Z",
        null,
        null,
        "module: dynamo, ciflow/inductor",
        "gh/StrongerXi/83/base",
        "gh/StrongerXi/83/head",
        2,
        54,
        21,
        1,
        2,
        0,
        "zou3519",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146714"
    ],
    [
        146713,
        "[dynamo][fx] Support dataclass whose fields have `init=False`",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* (to be filled)\n\nPreviously Dynamo and FX have code paths that reconstruct a dataclass\ninstance based on its type and fields; however they weren't taking\n`init=False` into account (which is supposed to exclude the field from\nconstructor).\n\nThis patch fixes that, and also updates `pytree.LeafSpec` so that its\n`__init__` conforms with the `init` attribute of its fields. Without\nthis change, the aforementioned reconstruction logic would fail.\n\ncc @ezyang @SherlockNoMad @EikanWang @jgong5 @wenzhe-nrv @voznesenskym @penguinwu @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-02-07T17:43:29Z",
        null,
        null,
        "release notes: fx, fx, module: dynamo, ciflow/inductor",
        "gh/StrongerXi/82/base",
        "gh/StrongerXi/82/head",
        4,
        61,
        9,
        1,
        1,
        0,
        "Skylion007",
        "COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146713"
    ],
    [
        146710,
        "[MTIA] (3/n) Implement PyTorch APIs to query/reset device peak memory usage",
        "Summary: Public summary (shared with Github): This diff implements a C++-Python binding to enable `reset_peak_memory_stats`.\n\nTest Plan: The test is implemented in the following diff.\n\nReviewed By: yuhc\n\nDifferential Revision: D68988673\n\n\n",
        "open",
        "2025-02-07T16:52:00Z",
        null,
        null,
        "fb-exported, ciflow/trunk, topic: not user facing, merging",
        "main",
        "export-D68988673",
        3,
        9,
        1,
        1,
        6,
        0,
        "nautsimon",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146710"
    ],
    [
        146709,
        "FSDP: avoid resetting version counter of all_gather_output in inference_mode",
        "Summary:\nFSDP needs to hide VC bumps on its allgather buffer, but it does not need to do this is the allgather buffer was generated under inference mode.\n\nmore details here: https://www.internalfb.com/diff/D69115649?dst_version_fbid=1316814572779281&transaction_fbid=849120230625711\n\nTest Plan: CI\n\nDifferential Revision: D69311496\n\n\n\n\ncc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o",
        "open",
        "2025-02-07T16:39:54Z",
        null,
        null,
        "oncall: distributed, fb-exported, ciflow/trunk, release notes: distributed (fsdp), ciflow/inductor, merging",
        "main",
        "export-D69311496",
        1,
        9,
        1,
        1,
        6,
        0,
        "awgu",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146709"
    ],
    [
        146706,
        "cpp_wrapper: persist autotune example tensors until last use",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\r\n* #146452\r\n* __->__ #146706\r\n* #146424\r\n* #146109\r\n* #146449\r\n* #144349\r\n* #144293\r\n* #144002\r\n\r\nPatches over an issue where randomly generated example tensors can cause kernel autotuning to fail, when those tensors would not be possible outputs from previous kernels in the sequence. This fixes a failure in `test_torchinductor_opinfo.py` when run with compile-time autotuning, `test_comprehensive_nanquantile_cuda_float64`.\r\n\r\nFor clarity, the situation triggering this PR looks like kernels `A -> BCDE -> F` (`BCDE` is fused), where one of the outputs from `A` is a boolean tensor describing some of the input data. Previously, we randomly regenerated that boolean tensor and the input data before passing them to `BCDE`, so that they no longer matched. This caused a `tl.device_assert` call in `BCDE` to fail. With this PR, we reuse the random data input to `A` and the output Boolean tensor, such that they match and pass the device assertion in `BCDE`.\r\n\r\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @desertfire @chauhang @aakhundov",
        "open",
        "2025-02-07T16:08:21Z",
        null,
        null,
        "open source, topic: not user facing, module: inductor, ciflow/inductor",
        "gh/benjaminglass1/67/base",
        "gh/benjaminglass1/67/head",
        1,
        28,
        8,
        2,
        2,
        0,
        "benjaminglass1",
        "COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146706"
    ],
    [
        146705,
        "Remove NO_MULTIPROCESSING_SPAWN checks",
        "py 3.9 has spawn.\r\n\r\ncc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o",
        "open",
        "2025-02-07T15:19:32Z",
        null,
        null,
        "oncall: distributed, triaged, open source, release notes: distributed (torchelastic)",
        "main",
        "NO_MULTIPROCESSING_SPAWN",
        11,
        23,
        156,
        1,
        1,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146705"
    ],
    [
        146696,
        "Add full_like.default to list of ops with kwargs",
        "The _maybe_insert_input_observers_for_node function expects ops, except a few exceptions, to have zero kwargs. full_like.default seems to be one of these cases and should therefore be added to the list.\r\n\r\nAddresses https://github.com/pytorch/pytorch/issues/146621\r\n\r\nFixes #146621 \r\n",
        "open",
        "2025-02-07T11:34:31Z",
        null,
        null,
        "triaged, open source, release notes: quantization, release notes: AO frontend",
        "main",
        "main",
        2,
        3,
        2,
        1,
        2,
        1,
        "Xia-Weiwen",
        "COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146696"
    ],
    [
        146695,
        "Enable Windows tests",
        "Fixes #ISSUE_NUMBER\r\n\n\ncc @peterjc123 @mszhanyi @skyline75489 @nbcsm @iremyux @Blackhex",
        "open",
        "2025-02-07T11:34:26Z",
        null,
        null,
        "module: windows, triaged, open source, topic: not user facing",
        "main",
        "win_test294",
        3,
        1,
        23,
        2,
        1,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146695"
    ],
    [
        146692,
        "[clang-tidy] Add suppression clang-diagnostic-shadow",
        "Summary:\r\nReviewed By: varun2784\r\n\r\nDifferential Revision: D69182465\r\n\r\n\r\n",
        "open",
        "2025-02-07T10:06:04Z",
        null,
        null,
        "fb-exported, topic: not user facing",
        "main",
        "export-D69182465",
        1,
        2,
        1,
        1,
        8,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146692"
    ],
    [
        146690,
        "Enable pt2e quantization path for arm",
        "**Title**: Enable PyTorch 2 Export Quantization path for ARM CPUs.\r\n\r\n**Description:**\r\n - This PR extends the PyTorch 2 Export Quantization (PT2E Quantization) workflow\u2014originally available only on x86 CPUs\u2014to support ARM platforms. PT2E Quantization is an automated, full-graph quantization solution in PyTorch that improves on Eager Mode Quantization by adding support for functionals and automating the overall process. It is part of the torch.ao module and fully supports quantization when using the compile mode.\r\n\r\n**Key Changes:**\r\n\r\n - Introduces ARM-specific support by leveraging oneDNN kernels for matmuls and convolution.\r\n\r\n - Integrates pre-defined configuration selection to automatically choose the best quantization settings based on the selected quantization method.\r\n\r\n**Provides customization options via two flags:**\r\n\r\n - **qat_state:** Indicates whether to use Quantization Aware Training (if set to True) or Post Training Quantization (if set to False). The default remains False.\r\n - **dynamic_state:** Selects between dynamic quantization (if True) and static quantization (if False). The default is also set to False.\r\n![Screenshot 2025-01-22 105543](https://github.com/user-attachments/assets/c611a1ce-9274-4b70-9c58-cae96000d06d)\r\n\r\nThese options allow users to tailor the quantization process for their specific workload requirements (e.g., using QAT for fine-tuning or PTQ for calibration-based quantization).\r\n\r\nTesting and Validation:\r\n\r\nThe new ARM flow has been thoroughly tested across a range of models with all combinations:\r\n**NLP**: Models such as BERT and T5.\r\n**Vision**: Models like ResNet and ViT.\r\n**Custom Models**: user defined models with various operators.\r\n\r\nexample script:\r\n```\r\nimport torch\r\nimport torchvision.models as models\r\nfrom torch.ao.quantization.quantize_pt2e import prepare_pt2e, convert_pt2e\r\nimport torch.ao.quantization.quantizer.arm_inductor_quantizer as armiq\r\nfrom torch.ao.quantization.quantizer.arm_inductor_quantizer import ArmInductorQuantizer\r\nfrom torch.profiler import profile, record_function, ProfilerActivity\r\n\r\nmodel_name = \"resnet50\"\r\nmodel = models.__dict__[model_name](pretrained=True)\r\n\r\n# Set the model to eval mode\r\nmodel = model.eval()\r\n\r\n# Create the data, using the dummy data here as an example\r\ntraced_bs = 500\r\nx = torch.randn(traced_bs, 3, 224, 224).contiguous(memory_format=torch.channels_last)\r\nexample_inputs = (x,)\r\n\r\nwith torch.no_grad():\r\n    exported_model = torch.export.export_for_training(model, example_inputs).module()\r\n    quantizer = armiq.ArmInductorQuantizer()\r\n    quantizer.set_global(armiq.get_default_arm_inductor_quantization_config(is_dynamic=False))\r\n    prepared_model = prepare_pt2e(exported_model, quantizer)\r\n    converted_model = convert_pt2e(prepared_model)\r\n\r\n    with torch.set_grad_enabled(False):\r\n        for _ in range(50):\r\n            converted_model(*example_inputs) #Warmup\r\n        print(\"Warmup over\")\r\n        with profile(activities=[ProfilerActivity.CPU], record_shapes=True) as prof:\r\n            with record_function(\"model_inference\"):\r\n                for _ in range(100):\r\n                    converted_model(*example_inputs)\r\n\r\n    print(prof.key_averages(group_by_input_shape=True).table(sort_by=\"self_cpu_time_total\"))\r\n\r\n```\n\ncc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @malfet @snadampal @milpuz01",
        "open",
        "2025-02-07T09:23:48Z",
        null,
        null,
        "module: cpu, triaged, open source, module: arm, release notes: quantization, release notes: AO frontend",
        "main",
        "devang/pt2e_quantization_arm",
        2,
        1592,
        0,
        1,
        7,
        1,
        "jerryzh168",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146690"
    ],
    [
        146689,
        "Update addbmm, addmm, addmv and baddbmm description",
        "Fixes #146611, following #146482\r\n\r\n## Test Result\r\n\r\n![image](https://github.com/user-attachments/assets/5c1749be-1f10-4e80-a284-b1929ca340eb)\r\n",
        "open",
        "2025-02-07T08:57:51Z",
        null,
        null,
        "triaged, open source, release notes: python_frontend",
        "main",
        "opt/docs/add",
        1,
        4,
        4,
        1,
        7,
        0,
        "mikaylagawarecki",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146689"
    ],
    [
        146687,
        "Make GetCPUAllocatorMaybePinned to be Device-Agnostic",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146687\n\n----\n\n- Keep cuda first to perserve BC\n- Remove cuda first if it is possible to have only one accelerator at a time in the future",
        "open",
        "2025-02-07T08:54:31Z",
        null,
        null,
        "open source, topic: not user facing",
        "gh/fffrog/38/base",
        "gh/fffrog/38/head",
        2,
        18,
        15,
        1,
        2,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146687"
    ],
    [
        146684,
        "Optimize LRScheduler docs",
        "Fixes #120735\r\n\r\nAdd more description about [`LRScheduler`](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.LRScheduler.html#torch.optim.lr_scheduler.LRScheduler)\r\n\r\n## Test Result\r\n\r\n### Before\r\n\r\n![image](https://github.com/user-attachments/assets/619c3ea8-5652-4e61-936f-0cb5aa5a326b)\r\n\r\n### After\r\n\r\n![image](https://github.com/user-attachments/assets/174a6ffc-5da2-4837-bf49-2f09f6c7b6ee)\r\n\r\n![image](https://github.com/user-attachments/assets/ae1bc984-49cc-4d5b-8d81-08f460b71361)\r\n\r\ncc @janeyx99\r\n",
        "open",
        "2025-02-07T08:42:57Z",
        null,
        null,
        "triaged, open source, release notes: optim",
        "main",
        "opt/docs/LRScheduler",
        1,
        21,
        2,
        1,
        1,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146684"
    ],
    [
        146678,
        "[dynamo][not ready] polyfill infra for classes",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146678\n* #146737\n* #146677\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-02-07T06:41:35Z",
        null,
        null,
        "module: dynamo, ciflow/inductor",
        "gh/anijain2305/675/base",
        "gh/anijain2305/675/head",
        7,
        247,
        41,
        3,
        2,
        0,
        "XuehaiPan",
        "COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146678"
    ],
    [
        146677,
        "[dynamo][user-defined] User class.__new__ instead of special casing",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #146678\n* #146737\n* __->__ #146677\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-02-07T06:41:32Z",
        null,
        null,
        "ciflow/trunk, topic: not user facing, module: dynamo, ciflow/inductor, keep-going",
        "gh/anijain2305/674/base",
        "gh/anijain2305/674/head",
        6,
        168,
        104,
        3,
        1,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146677"
    ],
    [
        146675,
        "[ROCm] Move ROCm unstable MI300 jobs back to stable",
        "Fixes #145790\r\n\r\nThis PR moves rocm unstable MI300 back to stable. The change to unstable was introduced through this [PR](https://github.com/pytorch/pytorch/pull/145790). This was because the MI300s were failing with a [docker daemon](https://github.com/pytorch/pytorch/actions/runs/13015957622/job/36306779536) issue which has been resolved.\r\n\r\ncc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd @ZainRizvi \r\n\r\ncc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd",
        "open",
        "2025-02-07T05:22:51Z",
        null,
        null,
        "module: rocm, open source, topic: not user facing, ciflow/inductor, ciflow/rocm, ciflow/inductor-rocm",
        "main",
        "patch-8",
        3,
        80,
        70,
        8,
        1,
        0,
        "jithunnair-amd, jithunnair-amd",
        "COMMENTED, COMMENTED",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146675"
    ],
    [
        146671,
        "Update torch-xpu-ops commit pin",
        "Update the torch-xpu-ops commit to [662837e722cbbc0701fbcf6ea9ad6383158cc44e](https://github.com/intel/torch-xpu-ops/commit/662837e722cbbc0701fbcf6ea9ad6383158cc44e), includes:\r\n\r\n- Aten operator coverage improvement\r\n- SYCL kernel optimization\r\n- Nested Tensor OPs support\r\n",
        "open",
        "2025-02-07T03:28:45Z",
        null,
        null,
        "triaged, open source, topic: not user facing, keep-going, ciflow/xpu",
        "main",
        "xyt/xpu_pin_662837e722cbbc0701fbcf6ea9ad6383158cc44e",
        1,
        1,
        1,
        1,
        2,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146671"
    ],
    [
        146669,
        "Optimize inductor `Self` typing",
        "Replace method return type with `Self` typing\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @desertfire @chauhang @aakhundov",
        "open",
        "2025-02-07T01:59:49Z",
        null,
        null,
        "triaged, open source, topic: not user facing, module: inductor",
        "main",
        "opt/inductor/typing",
        3,
        8,
        5,
        1,
        3,
        0,
        "jansel",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146669"
    ],
    [
        146668,
        "Make sure cutlass kernel .cu file has configuration name and nvcc compile command",
        "I think its good to have everything in the .cu file. Especially the nvcc compile command.\r\n\r\nTechnically, the configuration name can be found in the template already. So let me know if you think its not needed. \r\n\r\nDifferential Revision: D69281295\r\n\r\n\r\n\r\n\r\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @desertfire @chauhang @aakhundov",
        "open",
        "2025-02-07T01:54:45Z",
        null,
        null,
        "fb-exported, ciflow/trunk, topic: not user facing, module: inductor, ciflow/inductor, merging",
        "main",
        "export-D69281295",
        2,
        6,
        0,
        1,
        8,
        0,
        "chenyang78",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146668"
    ],
    [
        146664,
        "[Docs] Fix description of `input` in `torch.addbmm()`",
        "Fixes #146613\r\n",
        "open",
        "2025-02-07T01:27:52Z",
        null,
        null,
        "open source, ciflow/trunk, release notes: python_frontend, topic: docs, merging",
        "main",
        "docs/addbmm",
        1,
        1,
        1,
        1,
        10,
        1,
        "mikaylagawarecki",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146664"
    ],
    [
        146662,
        "[Optimus] Bug fix in the select cat aten pass",
        "Summary: Thanks to Shuai for reporting the bug in the pattern. We found there's a typo in the pass, where we should make sure all the selects will go to the cat node.\n\nTest Plan:\nbuck2 test 'fbcode//mode/dev-nosan' fbcode//caffe2/test/inductor:split_cat_fx_aten_passes -- test_select_cat_post_grad\n\n\nBuck UI: https://www.internalfb.com/buck2/2cd0888e-d803-43a8-8530-d97e6bc281b3\nTest UI: https://www.internalfb.com/intern/testinfra/testrun/6192449699305108\nNetwork: Up: 110KiB  Down: 35KiB  (reSessionID-687be0fa-031a-47a0-8780-5ab4cf4bbd94)\nExecuting actions. Remaining     0/4                                                                              6.6s exec time total\nCommand: test.     Finished 2 local\nTime elapsed: 2:12.0s\nTests finished: Pass 2. Fail 0. Fatal 0. Skip 0. Build failure 0\n\nDifferential Revision: D69278487\n\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @desertfire @chauhang @aakhundov",
        "open",
        "2025-02-07T00:50:59Z",
        null,
        null,
        "fb-exported, module: inductor, ciflow/inductor, release notes: inductor, inductor_pattern_match",
        "main",
        "export-D69278487",
        2,
        53,
        35,
        1,
        4,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146662"
    ],
    [
        146661,
        "[cond] Refactor cond_op's signature to take *operands.",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146661\n* #146660\n\r\n\r\nThis is a BC-breaking change for hop's IR schema. Previously, \r\n```python\r\ntorch.cond(pred, true_fn, false_fn, (a, b))\r\n# Old representation\r\ntorch.ops.higher_order.cond(pred, true_gm, false_gm, (a, b))\r\n# New representation:\r\ntorch.ops.higher_order.cond(pred, true_gm, false_gm, a, b)\r\n```\r\nThe benefits of this change is that it's much easier to construct the schema since the tuple is flattened. What's particularly troublesome about previous node is that it's hard to represent the mutation and alias information inside the tuple: we have to change the legacy schema parser and verify (maybe re-purpose) the aliasInfo to supports nested aliasInfo inside tuple/list.\r\n\r\nWe'll also refactor other control flow operators.\r\n\r\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @desertfire @chauhang @aakhundov\r\n\r\nDifferential Revision: [D69279033](https://our.internmc.facebook.com/intern/diff/D69279033)",
        "open",
        "2025-02-07T00:44:09Z",
        null,
        null,
        "ciflow/trunk, topic: not user facing, module: inductor, module: dynamo, ciflow/inductor, keep-going, release notes: export",
        "gh/ydwu4/208/base",
        "gh/ydwu4/208/head",
        13,
        119,
        181,
        3,
        3,
        0,
        "zou3519",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146661"
    ],
    [
        146660,
        "[hop][inductor] don't promote arg type for cond and while_loop",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #146661\n* __->__ #146660\n\r\nHop subgraph codegen assumes arguments's type are not promoted. Otherwise, we might generate wrong kernel.\r\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @desertfire @chauhang @aakhundov\n\nDifferential Revision: [D69279031](https://our.internmc.facebook.com/intern/diff/D69279031)",
        "open",
        "2025-02-07T00:44:03Z",
        null,
        null,
        "ciflow/trunk, topic: not user facing, module: inductor, ciflow/inductor, merging",
        "gh/ydwu4/207/base",
        "gh/ydwu4/207/head",
        1,
        2,
        2,
        2,
        6,
        1,
        "zou3519, zou3519, ydwu4",
        "COMMENTED, APPROVED, COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146660"
    ],
    [
        146658,
        "[HOP] Mutation and alias rework",
        "This PR reworks the way the input mutations and various aliases are checked\r\n\r\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @ydwu4 \r\n",
        "open",
        "2025-02-07T00:28:18Z",
        null,
        null,
        "open source, topic: not user facing, module: dynamo",
        "main",
        "mutation_alias_rework",
        12,
        116,
        100,
        5,
        2,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146658"
    ],
    [
        146656,
        "Optimize isclose() for CPU and GPU by adding specific implementations",
        "`isclose()` is currently quite slow, so this PR adds specific implementations for both CPU and cuda.\r\n\r\nCUDA implementation seeing ~4.9x improvement at 100m elements and ~18.7x improvement at 400m elements\r\n\r\nCPU implementation seeing ~5.7x improvements at 100m elements and ~5.9x improvements at 400m elements \r\n\r\nsimple benchmark used(adapt with CPU as needed):\r\n```python\r\nimport time\r\nimport numpy as np\r\nimport torch\r\n\r\ndef benchmark_isclose(shape1, shape2, num_runs=5):\r\n    tensor1 = torch.randn(shape1, device=\"cuda\")\r\n    tensor2 = tensor1.clone()\r\n    tensor2 += torch.randn_like(tensor2) * 0.001\r\n\r\n    # warn up\r\n    _ = torch.isclose(tensor1, tensor2)\r\n    torch.cuda.synchronize()\r\n\r\n    times = []\r\n    for _ in range(num_runs):\r\n        start_time = time.perf_counter()\r\n\r\n        _ = torch.isclose(tensor1, tensor2)\r\n        torch.cuda.synchronize()\r\n        end_time = time.perf_counter()\r\n        times.append(end_time - start_time)\r\n\r\n    mean_time = np.mean(times)\r\n    std_time = np.std(times)\r\n\r\n    return mean_time, std_time\r\n\r\n\r\ntest_shapes = [\r\n    (10000, 10000),  # 100M elements\r\n    (20000, 20000),  # 400M elements\r\n]\r\n\r\nprint(\"\\nBenchmarking torch.isclose():\")\r\nprint(\"-\" * 50)\r\n\r\nfor shape in test_shapes:\r\n    total_elements = np.prod(shape)\r\n    print(f\"\\nTensor shape: {shape} ({total_elements:,} elements)\")\r\n\r\n    mean_time, std_time = benchmark_isclose(shape, shape)\r\n    print(f\"Mean time: {mean_time*1000:.2f} ms +/- {std_time*1000:.2f} ms\")\r\n    print(f\"Elements per second: {total_elements/mean_time:,.0f}\")\r\n```\r\n\r\n```\r\n(optimized)\r\nTensor shape: (10000, 10000) (100,000,000 elements)\r\nMean time: 2.73 ms \u00b1 0.26 ms\r\nElements per second: 36,611,905,024\r\n\r\nTensor shape: (20000, 20000) (400,000,000 elements)\r\nMean time: 8.98 ms \u00b1 0.28 ms\r\nElements per second: 44,546,604,660\r\n\r\n(unoptimized)\r\nTensor shape: (10000, 10000) (100,000,000 elements)\r\nMean time: 13.48 ms \u00b1 0.28 ms\r\nElements per second: 7,420,814,236\r\n\r\nTensor shape: (20000, 20000) (400,000,000 elements)\r\nMean time: 166.90 ms \u00b1 4.71 ms\r\nElements per second: 2,396,711,992\r\n```\r\ncc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @albanD \r\n\r\n",
        "open",
        "2025-02-07T00:09:02Z",
        null,
        null,
        "module: cpu, triaged, open source",
        "main",
        "feature/isclose-kernels",
        4,
        171,
        57,
        3,
        2,
        1,
        "albanD",
        "COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146656"
    ],
    [
        146655,
        "torch._scaled_mm with MX dtypes",
        "making https://github.com/pytorch/pytorch/pull/145562 work with in-core dtypes\r\n\r\nnot ready for review yet",
        "open",
        "2025-02-07T00:02:24Z",
        null,
        null,
        "ciflow/inductor",
        "gh/vkuzo/2/head",
        "gh/vkuzo/3/head",
        8,
        291,
        79,
        3,
        2,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146655"
    ],
    [
        146654,
        "[DCP] Introduce modules metadata in the storage_meta",
        "Summary: Introduce the list of modules in the storage_meta which is shared between the planner and the storage writer. We will use it to let the storage writer know about the modules in the state dict and create module directories in the checkpoint.\n\nTest Plan: UTs\n\nDifferential Revision: D69154628\n\n\n\n\ncc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o @LucasLLC @MeetVadakkanchery @mhorowitz @pradeepfn @ekr0",
        "open",
        "2025-02-06T23:43:44Z",
        null,
        null,
        "oncall: distributed, fb-exported, module: distributed_checkpoint",
        "main",
        "export-D69154628",
        1,
        2,
        1,
        1,
        3,
        0,
        "mhorowitz",
        "COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146654"
    ],
    [
        146653,
        "windows Magma build for cu128",
        "https://github.com/pytorch/pytorch/issues/145570\r\n\r\nremoving `.ci/pytorch/windows/internal/cuda_install.bat` as it is a duplicate with` .github/scripts/windows/cuda_install.bat`. The later one is the one in use - https://github.com/pytorch/pytorch/pull/146653/files#diff-613791f266f2f7b81148ca8f447b0cd6c6544f824f5f46a78a2794006c78957bR8\r\n\r\ncc @atalman @ptrblck @nWEIdia ",
        "open",
        "2025-02-06T23:33:34Z",
        null,
        null,
        "open source, Merged, Reverted, release notes: releng, ciflow/binaries_wheel, ci-no-td",
        "main",
        "cu128-win-magma",
        5,
        95,
        223,
        4,
        6,
        1,
        "atalman",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146653"
    ],
    [
        146642,
        "[poc] force UntypedStorage.from_buffer(buf) to return meta storage under FakeTensorMode",
        "context here: https://fb.workplace.com/groups/326136610199609/permalink/495389539940981/\r\n\r\nThis PR is an attempt to make it such that if you create a tensor from an external buffer (using `UntypedStorage.from_buffer(buf)`, we can generate a proper fake tensor for you out of the box.\r\n\r\nThe annoying bit is that there are not any dispatcher ops to interpose on and change behavior. So instead, I took the manual C binding and tweaked the storage device to be \"meta' if we see an active fake mode.\r\n\r\nPut \"poc\" in the title since I... think this is hopefully reasonable, but I can be convinced that it's not :)\r\n\r\n```\r\nfrom torch._subclasses.fake_tensor import FakeTensorMode\r\nimport pickle\r\nimport io\r\nimport torch\r\nfrom contextlib import nullcontext\r\n\r\n\r\nuse_fake_tensor = True\r\nwith FakeTensorMode() if use_fake_tensor else nullcontext():\r\n    obj = [1, 2]\r\n    f = io.BytesIO()\r\n    pickle.Pickler(f).dump(obj)\r\n    byte_storage = torch.ByteStorage._from_buffer(f.getvalue())  # type: ignore[attr-defined]\r\n    \r\n    t = torch.ByteTensor(byte_storage)\r\n```\r\n\r\nStack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #133044\n* #146729\n* __->__ #146642\n* #146731\n\r\n",
        "open",
        "2025-02-06T21:50:27Z",
        null,
        null,
        "release notes: composability",
        "gh/bdhirsh/639/base",
        "gh/bdhirsh/639/head",
        2,
        27,
        8,
        4,
        2,
        1,
        "zou3519",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146642"
    ],
    [
        146641,
        "[dim order]  solve broken doc",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146641\n\nDifferential Revision: [D69265340](https://our.internmc.facebook.com/intern/diff/D69265340/)",
        "open",
        "2025-02-06T21:49:03Z",
        null,
        null,
        "fb-exported, ciflow/trunk, topic: not user facing, merging",
        "gh/gasoonjia/2/base",
        "gh/gasoonjia/2/head",
        1,
        3,
        0,
        3,
        8,
        0,
        "svekars, svekars, Jack-Khuu",
        "COMMENTED, APPROVED, APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146641"
    ],
    [
        146640,
        "POC for mixed prec optim frontend",
        "This PR is a prototype for what a frontend for asking for mixed precision can look like torch.optim through set_dtype_policy in optimizer.py.\r\n\r\nThis is not meant to be landable but to start some discussions on what people want/would like to see and to ask if there are things I haven't considered yet.\r\n\r\nThis currently only works with Adam(W)!\r\n\r\nA toy script for how to use:\r\n```\r\nimport torch\r\n\r\nmodel = torch.nn.Sequential(\r\n    torch.nn.Linear(2, 3),\r\n    torch.nn.Sigmoid(),\r\n    torch.nn.Linear(3, 1),\r\n    torch.nn.Sigmoid(),\r\n)\r\nmodel.to(\"cuda\")\r\n\r\noptim = torch.optim.AdamW(model.named_parameters(), foreach=False)\r\nmp_policy = {\r\n    \"exp_avg\": lambda _: torch.bfloat16,\r\n    \"exp_avg_sq\": lambda _: torch.bfloat16,\r\n    \"max_exp_avg_sq\": lambda _: torch.bfloat16,\r\n}\r\noptim.set_dtype_policy(mp_policy)\r\n\r\ni = torch.tensor([0.1, 0.2, 0.3, 0.4, 0.5, 0.6], device=\"cuda\").reshape(3, 2)\r\nl = model(i).sum()\r\nl.backward()\r\n\r\noptim.step()\r\n```\r\n\r\nStack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146640\n\r\n",
        "open",
        "2025-02-06T21:31:50Z",
        null,
        null,
        "release notes: optim",
        "gh/janeyx99/222/base",
        "gh/janeyx99/222/head",
        3,
        113,
        11,
        2,
        1,
        0,
        "janeyx99",
        "COMMENTED",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146640"
    ],
    [
        146638,
        "use None to slice when list has one element only",
        "When autotune_num_choices_displayed is None and the list of choices has length 1, slicing with `[:-1]` means getting all elements except the last one, which resulted in an empty list.\r\n\r\nSlicing with `[:None]` works. \r\n\r\nDifferential Revision: D69265168\r\n\r\n\r\n\r\n\r\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @desertfire @chauhang @aakhundov",
        "open",
        "2025-02-06T21:08:40Z",
        null,
        null,
        "fb-exported, ciflow/trunk, topic: not user facing, module: inductor, ciflow/inductor, merging",
        "main",
        "export-D69265168",
        1,
        2,
        5,
        1,
        8,
        0,
        "drisspg",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146638"
    ],
    [
        146637,
        "gloo: fix building system gloo with CUDA/HIP",
        "Fix incorrect linking of Gloo's libraries when building with system Gloo. Previously, either Gloo's native library or Gloo's CUDA library were linked. However, Gloo had changed such that all users of Gloo must link the native library, and can optionally link the CUDA or HIP library for Gloo + CUDA/HIP support.\r\nThis had been updated when building/linking with vendored Gloo, but not when using system Gloo.\r\n\r\nFixes: #146239\r\n\r\nReported-by: Adam J Stewart <ajstewart426@gmail.com>\r\n\r\n\n\ncc @malfet @seemethere @ptrblck @msaroufim @eqy",
        "open",
        "2025-02-06T21:04:17Z",
        null,
        null,
        "module: build, module: cuda, triaged, open source, topic: not user facing",
        "main",
        "gloo_cuda",
        2,
        25,
        25,
        1,
        2,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146637"
    ],
    [
        146636,
        "example repro failure",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146636\n\nSummary:\n\nTest Plan:\n\nReviewers:\n\nSubscribers:\n\nTasks:\n\nTags:\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-02-06T21:00:47Z",
        null,
        null,
        "module: dynamo, ciflow/inductor",
        "gh/c00w/37/base",
        "gh/c00w/37/head",
        2,
        5,
        0,
        1,
        2,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146636"
    ],
    [
        146634,
        "Add Structured Tracing for Traced Graph Edge Details for AC Debugging",
        "Summary:\nUpdating the structured trace infrastructure so that we are able to output to Zoomer and have an E2E solution.\n\nContext Doc: https://docs.google.com/document/d/1T6omIBEWVhbOiwDLSLffgQwjxiT2rQv8QvvQwXkw4fY/edit?usp=sharing\n\nTest Plan:\n### Testing Structured Log + tlparse locally\n\nCommand:\n```\nTORCH_TRACE=/data/users/basilwong/fbsource/fbcode/log_torch_trace buck2 run mode/opt //aps_models/ads/icvr:icvr_launcher -- mode=local_fb_fm_v4 launcher.num_workers=2\n```\n\nTorch Trace Logs (local then sent to paste): P1686419449\n```\ncat log_torch_trace/dedicated_log_torch_trace_rank_0_2lg012xo.log | pastry\nP1686419449\n```\n\ntlparse output: https://manifold.edge.x2p.facebook.net/v0/read/tree/logs/.tmpyiv5wj/rank_1/index.html?bucketName=tlparse_reports&apiKey=tlparse_reports-key&withPayload=1&timeoutMsec=100\n\ntlparse graph edge details output: https://manifold.edge.x2p.facebook.net/v0/read/tree/logs/.tmpyiv5wj/rank_1/9_0_0/joint_graph_information_397.txt?bucketName=tlparse_reports&apiKey=tlparse_reports-key&withPayload=1&timeoutMsec=100\n\nDifferential Revision: D61557220\n\n\n",
        "open",
        "2025-02-06T20:29:48Z",
        null,
        null,
        "fb-exported, topic: not user facing, ciflow/inductor",
        "main",
        "export-D61557220",
        2,
        137,
        38,
        1,
        4,
        1,
        "jansel",
        "CHANGES_REQUESTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146634"
    ],
    [
        146633,
        "[NJT] Fix inference mode for composite implicit ops without nested-specific kernel",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146633\n\n",
        "open",
        "2025-02-06T20:02:09Z",
        null,
        null,
        "ciflow/trunk, topic: bug fixes, release notes: nested tensor, merging",
        "gh/soulitzer/352/base",
        "gh/soulitzer/352/head",
        2,
        27,
        4,
        3,
        10,
        1,
        "jbschlosser",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146633"
    ],
    [
        146632,
        "[ROCm] OCP FP8 Support for new GPUs",
        "TLDR: Follow up/ Build on top of https://github.com/pytorch/pytorch/pull/144476. add OCP FP8 support for gfx950\r\nrefer to https://github.com/pytorch/ao/pull/1677\r\n\r\nThis pull request includes several changes to improve compatibility and support for new GPU architectures and data types, particularly for ROCm. The key updates involve adding support for new ROCm versions and GPU architectures, updating data type handling, and removing outdated checks.\r\n\r\n### Improvements to GPU Architecture and ROCm Version Support:\r\n* [`aten/src/ATen/Context.cpp`](diffhunk://#diff-33de472d304acbe57d693c8567370c638068bedc1aa0ce8e9dc115dad05a7810L323-R326): Added support for new GPU architectures `gfx1200`, `gfx1201`, and `gfx950` based on ROCm version checks.\r\n* [`aten/src/ATen/native/cuda/Blas.cpp`](diffhunk://#diff-e8a569efee1e650172f120a0fdcda024fe3e4703a4ee3336425c8f685af6b3abL196-R199): Updated architecture support in multiple functions to include `gfx1200`, `gfx1201`, and `gfx950` based on ROCm version checks. [[1]](diffhunk://#diff-e8a569efee1e650172f120a0fdcda024fe3e4703a4ee3336425c8f685af6b3abL196-R199) [[2]](diffhunk://#diff-e8a569efee1e650172f120a0fdcda024fe3e4703a4ee3336425c8f685af6b3abL865-R876)\r\n\r\n### Updates to Data Type Handling:\r\n* [`aten/src/ATen/cuda/CUDADataType.h`](diffhunk://#diff-9188bb13b1a49f459141f5f9b875593d1c5ce2beb5ad711fdbaf5bc7089ec015L81-L98): Enhanced data type conversion to include new float8 types for both CUDA and ROCm environments.\r\n* [`aten/src/ATen/cuda/tunable/GemmHipblaslt.h`](diffhunk://#diff-bfa1a3b5d4bef1892bf50338775f3b0fd8cd31fc1868148f3968b98aefb68e3fL29-R80): Updated `HipDataTypeFor` template to handle new float8 types and added hard-coded enum values for ROCm versions prior to 6.3.\r\n\r\n### Removal of Outdated Checks:\r\n* [`cmake/public/LoadHIP.cmake`](diffhunk://#diff-b98e27b9a5f196a6965a99ee5a7bb15b3fc633d6375b767635b1b04ccb2fd3d5L169-L197): Removed the check for `HIP_NEW_TYPE_ENUMS` as it is no longer necessary with the updated ROCm versions. [[1]](diffhunk://#diff-b98e27b9a5f196a6965a99ee5a7bb15b3fc633d6375b767635b1b04ccb2fd3d5L169-L197) [[2]](diffhunk://#diff-b98e27b9a5f196a6965a99ee5a7bb15b3fc633d6375b767635b1b04ccb2fd3d5L211-R182)\r\n\r\nThese changes ensure better compatibility and performance on newer hardware and software environments, particularly for users leveraging ROCm and CUDA for deep learning and scientific computing tasks.\r\n\n\ncc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd",
        "open",
        "2025-02-06T18:57:21Z",
        null,
        null,
        "module: rocm, open source, release notes: linalg_frontend",
        "main",
        "ocp_gfx950",
        11,
        98,
        25,
        8,
        1,
        0,
        "jeffdaily",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146632"
    ],
    [
        146631,
        "Support ignoring parameters in FSDP2",
        "Differential Revision: D69153051\n\n\n\n\ncc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o",
        "open",
        "2025-02-06T18:46:40Z",
        null,
        null,
        "oncall: distributed, fb-exported, release notes: distributed (fsdp), ciflow/inductor",
        "main",
        "export-D69153051",
        3,
        392,
        5,
        1,
        17,
        2,
        "awgu, weifengpy, weifengpy, weifengpy",
        "COMMENTED, COMMENTED, COMMENTED, COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146631"
    ],
    [
        146626,
        " [inductor] Improve type annotations in _inductor/pattern_matcher.py",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146626\n* #146248\n\n\n\ncc @ezyang @SherlockNoMad @EikanWang @jgong5 @wenzhe-nrv @voznesenskym @penguinwu @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @desertfire @chauhang @aakhundov",
        "open",
        "2025-02-06T17:59:04Z",
        null,
        null,
        "open source, release notes: fx, topic: not user facing, fx, module: inductor, ciflow/inductor, suppress-api-compatibility-check, suppress-bc-linter",
        "gh/rec/132/base",
        "gh/rec/132/head",
        2,
        41,
        33,
        6,
        2,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146626"
    ],
    [
        146625,
        "Move capture_provenance to make_node_impl",
        "Previously we were only logging `make_user_impl` implementations, which only gets triggered for operations done on python SymInts, not cpp SymInts. Instead `make_node_impl` will get triggered for both python and cpp SymInt operations.\r\n\r\n\r\ncc @ezyang @SherlockNoMad @EikanWang @jgong5 @wenzhe-nrv",
        "open",
        "2025-02-06T17:53:35Z",
        null,
        null,
        "ciflow/trunk, release notes: fx, fx, ciflow/inductor, merging",
        "main",
        "angelayi/test_expression_created",
        1,
        89,
        90,
        1,
        13,
        2,
        "bobrenjc93, bobrenjc93",
        "APPROVED, APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146625"
    ],
    [
        146623,
        "bug fix: ensure 4d input in _scaled_dot_product_attention_math_mps",
        "This pr addresses the issue in the MPS backend for `_scaled_dot_product_attention_math_mps` where a 3d input like (num_heads, seq_len, query_dim) cannot be automatically treated as (1, num_heads, seq_len, query_dim), which can be inferred on cpu or cuda, which can be circumvented by adding a util function to ensure a 4d shape.\r\n\r\nThe issue was found in https://github.com/hiyouga/LLaMA-Factory/issues/6835, in [transformers qwen2_vl](https://github.com/huggingface/transformers/blob/1590c664306766f32ba68c50e67f14d61b16925d/src/transformers/models/qwen2_vl/modeling_qwen2_vl.py#L373C14-L373C93), 3d q/k/v were passed into sdpa function, which lead to an error.\r\n\r\nConsidering consistency, since this pattern might pop up elsewhere in the transformers codebase, I think it makes more sense to maintain the same intuition across all platforms.\r\n\r\n---\r\nreproduce code:\r\n```\r\nimport torch\r\nimport torch.nn.functional as F\r\n\r\nhead_num, seq_len, embed_dim = 16, 16, 80\r\nbsz = 1\r\n\r\nq = torch.randn(head_num, seq_len, embed_dim)\r\nk = torch.randn(head_num, seq_len, embed_dim)\r\nv = torch.randn(head_num, seq_len, embed_dim)\r\nattention_mask = torch.ones(1, seq_len, seq_len)\r\n\r\noo_cpu = F.scaled_dot_product_attention(\r\n    q.to(\"cpu\"),\r\n    k.to(\"cpu\"),\r\n    v.to(\"cpu\"),\r\n    attention_mask.to(\"cpu\"),\r\n    dropout_p=0.0\r\n)\r\n\r\nif torch.backends.mps.is_available():\r\n    oo_mps = F.scaled_dot_product_attention(\r\n        q.to(\"mps\"),\r\n        k.to(\"mps\"),\r\n        v.to(\"mps\"),\r\n        attention_mask.to(\"mps\"),\r\n        dropout_p=0.0\r\n    )\r\n    assert torch.allclose(oo_cpu, oo_mps.to(\"cpu\"), atol=1e-5)\r\n```\r\n\r\nerror outputs:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/opt/homebrew/Caskroom/miniconda/base/envs/torch-dev/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-2-5169b8d2c5dd>\", line 21, in <module>\r\n    oo_mps = F.scaled_dot_product_attention(\r\nIndexError: Dimension out of range (expected to be in range of [-3, 2], but got 3)\r\n```\r\n\r\nhardware and envs:\r\n```\r\ntorch               2.6.0\r\napple m3 max\r\n```\r\n\r\n---\r\n\r\n",
        "open",
        "2025-02-06T17:15:59Z",
        null,
        null,
        "triaged, open source, topic: bug fixes, release notes: mps, ciflow/mps",
        "main",
        "main",
        2,
        66,
        14,
        6,
        5,
        0,
        "Skylion007, Skylion007, malfet, malfet",
        "COMMENTED, COMMENTED, COMMENTED, COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146623"
    ],
    [
        146622,
        "Fix inductor non-stable argsort/sort test",
        "- Prevent the inductor test for argsort/sort from wrongly failing when the argsort/sort output with stable=False differs from pytorch but is still a valid argsort output.\r\n- Add functionality to allow alternative assert_equal functions in inductor tests for future cases.\r\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @desertfire @chauhang @aakhundov",
        "open",
        "2025-02-06T17:10:12Z",
        null,
        null,
        "triaged, open source, topic: not user facing, module: inductor",
        "main",
        "PYT-466",
        2,
        169,
        19,
        1,
        1,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146622"
    ],
    [
        146620,
        "Enable qint8 and quint8 add for AArch64 using ACL directly",
        "This enables qint8 and quint8 add for AArch64 through Arm Compute Library (ACL) directly.\r\nIt\u2019s based on changes in PR #145942 which enables the use of ACL directly in ATen.\r\nRelative performance improvement using OMP_NUM_THREADS=1 is ~15x, using OMP_NUM_THREADS=32 it\u2019s ~5.4x.\r\n\r\nScript to benchmark quantised add performance:\r\n```\r\nimport torch\r\nimport torch.profiler as profiler\r\n\r\na_f32 = torch.rand((400, 3456),dtype=torch.float)\r\nb_f32 = torch.rand((400, 3456),dtype=torch.float)\r\na_q = torch.quantize_per_tensor(a_f32, 1.2, 0, torch.qint8)\r\nb_q = torch.quantize_per_tensor(b_f32, 1.7, 5, torch.qint8)\r\n\r\nwith profiler.profile(with_stack=True, profile_memory=False, record_shapes=True) as prof:\r\n    for i in range(1000):     \r\n        _ = torch.ops.quantized.add(a_q, b_q, 1.3, 2)\r\nprint(prof.key_averages(group_by_input_shape=True).table(sort_by='self_cpu_time_total', row_limit=50))\r\n```\n\ncc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10",
        "open",
        "2025-02-06T16:54:08Z",
        null,
        null,
        "module: cpu, triaged, open source, release notes: quantization, release notes: releng",
        "main",
        "acl_qadd",
        10,
        577,
        15,
        3,
        2,
        1,
        "malfet, malfet",
        "COMMENTED, COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146620"
    ],
    [
        146617,
        "Generate test reports for pytest when option is given",
        "The argument needs to be appended when test reports should be generated. `IS_CI` is not necessarily set, so rather check `TEST_SAVE_XML` instead as in other places where test reports are conditionally enabled.\r\n\r\nSee also https://github.com/pytorch/pytorch/issues/126523",
        "open",
        "2025-02-06T16:12:06Z",
        null,
        null,
        "triaged, open source, topic: not user facing",
        "main",
        "Flamefire-patch-1",
        1,
        2,
        1,
        1,
        1,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146617"
    ],
    [
        146616,
        "[don't merge] test baseline",
        "Fixes #ISSUE_NUMBER\r\n",
        "open",
        "2025-02-06T16:04:22Z",
        null,
        null,
        "open source, topic: not user facing, ciflow/binaries_wheel, ciflow/xpu",
        "main",
        "test_main",
        1,
        1,
        0,
        1,
        7,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146616"
    ],
    [
        146615,
        "[BE][Ez]: Enable specific ruff rules to prevent antipatterns and bugs",
        "Enables a few ruff rules\r\n* Ban print statements within asserts (likely bugs)\r\n* ~Use string for Decimal literal to prevent loss of precision~ \r\n* ~Do not use default args for __post__init__ in dataclasses, they likely were meant to go into the factory method, the __init__, or somewhere else. The default values are useless here.~\r\n\r\nWait until ruff upgrade for the last 2\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-02-06T16:04:20Z",
        null,
        null,
        "triaged, open source, better-engineering, topic: not user facing, module: dynamo, ciflow/inductor",
        "main",
        "skylion007/enable-RUF-2025-02-06",
        2,
        4,
        3,
        3,
        1,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146615"
    ],
    [
        146614,
        "[CD] Add python 3.13t build for xpu",
        "Fixes #146451\r\n",
        "open",
        "2025-02-06T15:55:52Z",
        null,
        null,
        "open source, ciflow/trunk, topic: not user facing, ciflow/binaries_wheel",
        "main",
        "xpu_py_3_13t",
        3,
        339,
        2,
        1,
        5,
        1,
        "atalman",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146614"
    ],
    [
        146612,
        "[WIP] BaseSubclass",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146612\n\n",
        "open",
        "2025-02-06T15:33:23Z",
        null,
        null,
        "",
        "gh/IvanKobzarev/100/base",
        "gh/IvanKobzarev/100/head",
        3,
        98,
        17,
        1,
        2,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146612"
    ],
    [
        146604,
        "[Profiler] Enable CUPTI teardown to reduce profiler overhead",
        "The problem is that the profiler slowed down\r\ntraining by roughly 10-20% even after completion\r\nbecause cuptiFinalize was not called in Kineto due to TEARDOWN_CUPTI=0. Disabling CUPTI teardown was a workaround for crashes which occured when CUDA graphs were used. This issue was fixed in CUDA 12.6. Also there is no point in disabling CUPTI teardown if CUDA Graphs are not used.\r\n\r\nFixes #144455 \r\n\n\ncc @robieta @chaekit @guotuofeng @guyang3532 @dzhulgakov @davidberard98 @briancoutinho @sraikund16 @sanrise",
        "open",
        "2025-02-06T13:43:37Z",
        null,
        null,
        "triaged, open source, oncall: profiler, topic: not user facing",
        "main",
        "fix/144455_teardown_cupti",
        2,
        22,
        4,
        4,
        8,
        1,
        "sraikund16, sraikund16, davidberard98, mgmtea, mgmtea, mgmtea, mgmtea",
        "APPROVED, APPROVED, CHANGES_REQUESTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146604"
    ],
    [
        146597,
        "Fix bug in _ref_test_helper in test_ops and fix failing test on Aarch64",
        "We have a failing unit test on Aarch64\r\n\r\n```\r\nException: Caused by reference input at index 34: SampleInput(input=Tensor[size=(5, 5, 4), device=\"cpu\", dtype=torch.complex64, contiguous=False], args=(), kwargs={}, broadcasts_input=False, name='')\r\n\r\nTo execute this test, run the following from the base repo dir:\r\n    PYTORCH_OPINFO_SAMPLE_INPUT_INDEX=34 python test/test_ops.py TestCommonCPU.test_python_ref__refs_square_cpu_complex64\r\n\r\nThis message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0\r\n```\r\n\r\nAfter debugging it I found that `ex` variable is not being reset to None on each loop inside _ref_test_helper. Which after fixing, highlighted another expectedFailure to reenable - `nn.functional.hinge_embedding_loss` which was incorrectly being skipped due to the same problem.\r\n\r\nhttps://github.com/pytorch/pytorch/blob/4a545eb85d6ba06079787a83f8ab1a8c8f67c76f/test/test_ops.py#L546\r\nex variable is not reset after this for next loop iteration",
        "open",
        "2025-02-06T11:07:07Z",
        null,
        null,
        "triaged, open source, topic: not user facing",
        "main",
        "test_fix",
        3,
        3,
        11,
        1,
        3,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146597"
    ],
    [
        146596,
        "separate f16 vectorized class from bf16",
        "This refactoring is required as part of https://github.com/pytorch/pytorch/pull/143666\n\ncc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @malfet @snadampal @milpuz01",
        "open",
        "2025-02-06T10:58:15Z",
        null,
        null,
        "module: cpu, open source, module: arm, topic: not user facing",
        "main",
        "refactor",
        4,
        1031,
        404,
        1,
        1,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146596"
    ],
    [
        146595,
        "skip test_torch_dynamo_codegen_pow if CPU backend is not cpp",
        "The test asserts that `aten.pow` is not present in the generated kernel code. When using a CPU backend other than cpp, the kernel contains comments referencing the aten ops that produced the kernel in this case `aten.pow`. \r\n\r\nThis PR skips that test case if the CPU backend is not cpp.\r\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-02-06T10:46:32Z",
        null,
        null,
        "triaged, open source, topic: not user facing, module: dynamo",
        "main",
        "skip-if-cpu-backend-not-cpp",
        1,
        4,
        0,
        1,
        1,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146595"
    ],
    [
        146593,
        "[NOT FOR LANDING] experimental NVSHMEM integration",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146593\n* #146592\n\n\n\ncc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o",
        "open",
        "2025-02-06T10:26:18Z",
        null,
        null,
        "oncall: distributed, release notes: distributed (c10d)",
        "gh/yifuwang/195/base",
        "gh/yifuwang/195/head",
        7,
        471,
        4,
        2,
        1,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146593"
    ],
    [
        146592,
        "clang-format CUDASymmetricMemory.cu",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #146593\n* __->__ #146592\n\n\n\ncc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o",
        "open",
        "2025-02-06T10:26:14Z",
        null,
        null,
        "oncall: distributed, release notes: distributed (c10d)",
        "gh/yifuwang/194/base",
        "gh/yifuwang/194/head",
        1,
        20,
        10,
        1,
        2,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146592"
    ],
    [
        146589,
        "[DDP] Use NCCL allocated memory for gradient bucket",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146589\n\r\nSo that NVLink SHARP comes with zero-copy on H100+ platforms, for DDP applications.\r\nLess SM usage, less memory contention between NCCL kernel and compute kernels.\r\n\r\nAdded env `DDP_DISABLE_COMM_MEM` as a back-out option:\r\n```\r\nAn environment variable to disable comm-optimized memory pool.\r\nDefault is 0, which means comm-optimized memory pool is enabled.\r\nUsers can set it to 1 in case of seeing regression or OOM (because this\r\ncomm MemPool may not share space with regular compute MemPool).\r\n```\r\n\r\ncc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o\n\nDifferential Revision: [D69297766](https://our.internmc.facebook.com/intern/diff/D69297766)",
        "open",
        "2025-02-06T09:01:24Z",
        null,
        null,
        "oncall: distributed, ciflow/trunk, release notes: distributed (c10d), release notes: distributed (ddp), merging",
        "gh/kwen2501/123/base",
        "gh/kwen2501/123/head",
        7,
        127,
        13,
        6,
        14,
        2,
        "Skylion007, Skylion007, Skylion007, Skylion007, kwen2501, Skylion007, kwen2501, syed-ahmed, syed-ahmed, syed-ahmed, syed-ahmed, syed-ahmed, syed-ahmed, yifuwang, kwen2501, yifuwang, fegin, syed-ahmed, kwen2501, c-p-i-o, c-p-i-o, c-p-i-o, c-p-i-o, fduwjj, fduwjj",
        "COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, CHANGES_REQUESTED, APPROVED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, APPROVED, COMMENTED, APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146589"
    ],
    [
        146587,
        "[Dynamo] Allow dynamo to handle `str.xxx()`",
        "\r\nFixes #146350\r\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-02-06T08:47:14Z",
        null,
        null,
        "open source, topic: not user facing, module: dynamo",
        "main",
        "fix/dynamo/str",
        2,
        13,
        0,
        2,
        10,
        1,
        "zou3519, shink, shink, zou3519",
        "COMMENTED, COMMENTED, COMMENTED, APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146587"
    ],
    [
        146583,
        "[symbolic shapes] Log symnode id",
        "We want to log the symnode id which will help us with provenance tracking between expressions created.\r\n\r\n\r\ncc @ezyang @SherlockNoMad @EikanWang @jgong5 @wenzhe-nrv",
        "open",
        "2025-02-06T07:45:13Z",
        null,
        null,
        "ciflow/trunk, release notes: fx, fx, ciflow/inductor, merging",
        "main",
        "angelayi/test",
        1,
        26,
        7,
        1,
        7,
        1,
        "bobrenjc93",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146583"
    ],
    [
        146582,
        "[Partitioner] Reduce time consuming of partitions merger",
        "This patch optimize maybe_merge_partition func through 3-ways:\r\n\r\nRemove unnecessary copy https://github.com/pytorch/pytorch/blob/main/torch/fx/passes/infra/partitioner.py#L99. The number of copied nodes is large if we can merge all of the nodes of graph into one partition.\r\nRecord users of each partition to avoid duplicate iteration over nodes https://github.com/pytorch/pytorch/blob/main/torch/fx/passes/infra/partitioner.py#L133. The trip count of this loop maybe very large.\r\nThe nodes number of each partitions maybe not balance https://github.com/pytorch/pytorch/blob/main/torch/fx/passes/infra/partitioner.py#L145. We always encounter one issue: one partition has n nodes, but the other has one node. Merge the smaller partition into the larger can help to reduce time consuming.\n\ncc @ezyang @SherlockNoMad @EikanWang @jgong5 @wenzhe-nrv",
        "open",
        "2025-02-06T07:35:55Z",
        null,
        null,
        "triaged, open source, release notes: fx, topic: not user facing, fx",
        "main",
        "lingzhiz/optimize_partition_merger",
        1,
        39,
        26,
        3,
        2,
        0,
        "Skylion007",
        "COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146582"
    ],
    [
        146581,
        "Clarify that compile(module) only affects the forward method",
        "Fixes #141616\r\n\r\n## Changes\r\n\r\n- Add `Note` to Clarify how compile works with `nn.Module`\r\n- Optimize plain url address with clickable description\r\n\r\n## Test Result\r\n\r\n### Before\r\n\r\n![image](https://github.com/user-attachments/assets/15ff9985-7e91-4d71-be7d-cdd38eacd3f9)\r\n![image](https://github.com/user-attachments/assets/26e27ba4-52da-4336-b72d-a0f9d0ebe839)\r\n\r\n\r\n### After\r\n\r\n![image](https://github.com/user-attachments/assets/5eaa8421-19e8-4186-af3d-dab4323d2c95)\r\n![image](https://github.com/user-attachments/assets/a96a8a79-6320-4748-931f-33f4dbc640eb)\r\n\r\n",
        "open",
        "2025-02-06T07:34:49Z",
        null,
        null,
        "triaged, open source, topic: not user facing",
        "main",
        "opt/docs/compile",
        1,
        8,
        2,
        1,
        2,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146581"
    ],
    [
        146580,
        "[Partitioner] Remove unnecessary upstream nodes in dependency viewer",
        "We iterate upstream nodes to update partition map. But actually did nothing due to we iterate nodes with reversed topological order https://github.com/pytorch/pytorch/pull/136608/files#diff-f2f9dd3903fd99955732eb694941fea0cb7301a58d59554787f3311d417e5615L193 so that there exists no upstream nodes in assignment. Remove it to reduce for-loop overhead which up to O(N * N) complexity.\r\n\n\ncc @ezyang @SherlockNoMad @EikanWang @jgong5 @wenzhe-nrv",
        "open",
        "2025-02-06T07:32:01Z",
        null,
        null,
        "triaged, open source, release notes: fx, topic: not user facing, fx",
        "main",
        "lingzhiz/remove_upstream_nodes",
        1,
        0,
        19,
        1,
        2,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146580"
    ],
    [
        146578,
        "add `torch.float4_e2m1fn_x2` to PyTorch",
        "Summary:\r\n\r\nAdds the `torch.float4_e2m1fn_x2` dtype to PyTorch, as detailed in\r\nhttps://github.com/pytorch/pytorch/issues/146414 . Please see the issue for a detailed definition of the format.  \r\n\r\nNote that I decided to keep the casts out of this to significantly simplify the code, as defining casting between packed and unpacked formats will be tricky using the existing casting machinery.  \r\n\r\nExample of basic functionality:\r\n\r\n```python\r\nimport torch\r\n\r\n# creation with empty\r\nx0 = torch.empty(4, 4, dtype=torch.float4_e2m1fn_x2)\r\n\r\n# printing, prints the uint8 representation of the stored values\r\nprint(x0)\r\n\r\n# view as other dtype\r\nx0.view(torch.uint8).view(torch.float4_e2m1fn_x2)\r\n```\r\n\r\nDone in this PR:\r\n* tensor creation and tensor printing works (no other ops defined)\r\n\r\nFor future PRs:\r\n* torch._scaled_mm\r\n* PT2\r\n* various cleanups (detailed in comments with issue numbers)\r\n\r\nTest Plan:\r\n\r\n```\r\npytest test/quantization/core/experimental/test_floatx.py -s\r\n```\r\n\r\nReviewers:\r\n\r\nSubscribers:\r\n\r\nTasks:\r\n\r\nTags:\r\n\r\ncc @yanbing-j @albanD @kadeng @penguinwu @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10",
        "open",
        "2025-02-06T07:08:42Z",
        null,
        null,
        "release notes: quantization",
        "gh/vkuzo/1/head",
        "gh/vkuzo/2/head",
        7,
        70,
        5,
        3,
        2,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146578"
    ],
    [
        146574,
        "[ROCm][TunableOp] Offline results are saved to file when offline tuning is disabled.",
        "This PR is to fix UT breakage that has been reported internally and is considered high priority. When `tunable.record_untuned_enable(False)` is invoked, we flush the results of the untuned gemm file.\r\n\r\nOffline tuning I/O currently doesn't have a set untuned results filename member function or untuned results write to file member function. When performing back-to-back unit tests, the same ofstream ends up getting reused between UTs. Due to the way the UT are executed, this can lead to unexpected failures.\r\n\r\ncc: @jfactory07 \r\n\r\n\r\n\r\ncc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang",
        "open",
        "2025-02-06T06:04:28Z",
        null,
        null,
        "module: rocm, open source, topic: not user facing, ciflow/rocm",
        "main",
        "fix_tunableop_untuned_fileio",
        1,
        2,
        0,
        1,
        4,
        0,
        "jeffdaily",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146574"
    ],
    [
        146573,
        "add python root bin to windows load path.",
        "This PR is extend python root bin path to dll load list. \r\nIt makes PyTorch robust and compatible to more dependency libraries, such as `intel-pti`.\r\n\r\ncc @peterjc123 @mszhanyi @skyline75489 @nbcsm @iremyux @Blackhex @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10",
        "open",
        "2025-02-06T05:58:55Z",
        null,
        null,
        "module: windows, open source, ciflow/trunk, topic: not user facing, intel",
        "main",
        "xu_add_init_path",
        1,
        8,
        1,
        1,
        1,
        0,
        "EikanWang",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146573"
    ]
][
    [
        146764,
        "Fix standalone runner for CUTLASS auto-tuning backend",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146764\n* #146755\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov",
        "open",
        "2025-02-08T17:23:45Z",
        null,
        null,
        "open source, topic: not user facing, module: inductor, ciflow/inductor",
        "gh/alexsamardzic/26/base",
        "gh/alexsamardzic/26/head",
        1,
        64,
        8,
        2,
        2,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146764"
    ],
    [
        146763,
        "[Break XPU] Align meta calculation for fft_r2c with _fft_r2c_mkl",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146763\n* #145248\n* #146762\n\nFix #146761",
        "open",
        "2025-02-08T16:20:39Z",
        null,
        null,
        "open source, ciflow/trunk, topic: not user facing, ciflow/inductor, ciflow/xpu",
        "gh/etaf/98/base",
        "gh/etaf/98/head",
        1,
        3,
        1,
        1,
        1,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146763"
    ],
    [
        146762,
        "[Break XPU][Inductor UT] Fix XPU Inductor UT introduced from community.",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #146763\n* #145248\n* __->__ #146762\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov",
        "open",
        "2025-02-08T16:20:35Z",
        null,
        null,
        "open source, topic: not user facing, module: inductor, ciflow/inductor",
        "gh/etaf/97/base",
        "gh/etaf/97/head",
        4,
        10,
        2,
        1,
        1,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146762"
    ],
    [
        146756,
        "[Inductor][CPU] Add GEMM tamplates for _weight_int4pack_mm_for_cpu with AVX512",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146756\n* #145250\n* #145245\n\r\n**Summary**\r\nIt's part of the task to enable max-autotune with GEMM template for WoQ INT4 GEMM on CPU. Early work: #133846\r\n\r\nThis PR adds GEMM templates for `torch.ops.aten_weight_int4pack_mm_for_cpu`. The micro kernel used for the templates is based on AVX512 and it's a copy of the ATen implementation of `torch.ops.aten_weight_int4pack_mm_for_cpu` with minor changes.\r\n\r\nDue to better blocking and loop schedule, the GEMM template based implementation outperforms the ATen implementation in all cases we tested.\r\n\r\n**Test plan**\r\n```\r\npython test/inductor/test_cpu_select_algorithm.py -k test_int4_woq_mm_avx512\r\n```\r\n\r\ncc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @voznesenskym @penguinwu @EikanWang @Guobing-Chen @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov",
        "open",
        "2025-02-08T13:51:09Z",
        null,
        null,
        "open source, ciflow/trunk, topic: not user facing, intel, module: inductor, ciflow/inductor",
        "gh/Xia-Weiwen/30/base",
        "gh/Xia-Weiwen/30/head",
        8,
        468,
        21,
        2,
        1,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146756"
    ],
    [
        146755,
        "Fix CUTLASS 2.x kernels for auto-tuning",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #146764\n* __->__ #146755\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov",
        "open",
        "2025-02-08T12:01:14Z",
        null,
        null,
        "open source, topic: not user facing, module: inductor, ciflow/inductor",
        "gh/alexsamardzic/25/base",
        "gh/alexsamardzic/25/head",
        3,
        12,
        30,
        1,
        1,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146755"
    ],
    [
        146754,
        "[MPS] fix inverse bug for N>1024",
        "Fixes #138200 \r\n",
        "open",
        "2025-02-08T10:24:29Z",
        null,
        null,
        "open source, release notes: mps",
        "main",
        "mps-inverse-bugfix",
        4,
        26,
        27,
        1,
        1,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146754"
    ],
    [
        146753,
        "[MPS] fix lu factor for large tensors with bs>1",
        "Try this:\r\n```\r\nimport torch\r\n\r\nbatch_size = 2\r\nA = torch.eye(256, device=\"mps\")[None, :, :].expand(batch_size, -1, -1) + 0.1 * torch.randn((batch_size, 256, 256), device=\"mps\")\r\nA_cpu = A.cpu()\r\nLU_cpu, pivots_cpu = torch.linalg.lu_factor(A_cpu)\r\nLU, pivots = torch.linalg.lu_factor(A)\r\ntorch.testing.assert_close(LU.cpu(), LU_cpu)\r\n```\r\nYou'll get huge difference in LU tensors\r\n<img width=\"706\" alt=\"Screenshot 2025-02-08 at 12 14 39\" src=\"https://github.com/user-attachments/assets/b45f2b3c-e0a5-49c8-aa07-42792150b781\" />\r\n",
        "open",
        "2025-02-08T08:15:29Z",
        null,
        null,
        "open source, release notes: mps",
        "main",
        "lu-factor-fix-batches",
        2,
        7,
        5,
        1,
        1,
        0,
        "Isalia20",
        "COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146753"
    ],
    [
        146752,
        "realize stride symbols in estimate_runtime",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\r\n* __->__ #146752\r\n\r\nUnfortuanlty could not create a local repo, or unit test.\r\nfix https://github.com/pytorch/pytorch/issues/146686\r\n\r\n",
        "open",
        "2025-02-08T07:30:03Z",
        null,
        null,
        "topic: not user facing, ciflow/inductor",
        "gh/laithsakka/107/base",
        "gh/laithsakka/107/head",
        1,
        2,
        1,
        1,
        1,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146752"
    ],
    [
        146751,
        "[MTIA] (4/n) Implement PyTorch APIs to query/reset device peak memory usage",
        "Summary: Public summary (shared with Github): This diff updates the unit test for the PyTorch API \"reset_peak_memory_stats\".\n\nTest Plan:\n```\nbuck2 test //mtia/host_runtime/torch_mtia/tests:test_torch_mtia_api -- -r test_reset_peak_memory_stats\n```\n\nhttps://www.internalfb.com/intern/testinfra/testrun/9007199321947161\n\nReviewed By: yuhc\n\nDifferential Revision: D68989900\n\n\n",
        "open",
        "2025-02-08T07:16:36Z",
        null,
        null,
        "fb-exported",
        "main",
        "export-D68989900",
        2,
        17,
        0,
        1,
        3,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146751"
    ],
    [
        146750,
        "Update instructions about faster linker",
        "This PR adds instructions to specify linker via cmake env `CMAKE_LINKER_TYPE` and also adds `mold` as a linker alternative.\r\n\r\nSince 3.29, cmake introduced [`CMAKE_LINKER_TYPE`](https://cmake.org/cmake/help/latest/variable/CMAKE_LINKER_TYPE.html) that can specify linker without overwriting `ld` file or changing build script.\r\n\r\n`mold` is already stable and **the fastest** (afaict) linker out there, and also easier to install compared with `lld`. So I added it here. After switching to `mold`, the time of linking `libtorch_cuda.so` has been reduced from ~7s to ~0.6s locally.\r\n\r\nAlso note `gold` has been marked deprecated recently[1].\r\n\r\n[1] https://lwn.net/Articles/1007541/",
        "open",
        "2025-02-08T06:30:17Z",
        null,
        null,
        "open source, topic: not user facing",
        "main",
        "update-linker",
        1,
        6,
        8,
        2,
        1,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146750"
    ],
    [
        146748,
        "Update strided test to float32",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\r\n* __->__ #146748\r\n\r\nFixes #146377\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov",
        "open",
        "2025-02-08T04:48:51Z",
        null,
        null,
        "ciflow/trunk, topic: not user facing, module: inductor, ciflow/inductor",
        "gh/drisspg/122/base",
        "gh/drisspg/122/head",
        1,
        3,
        3,
        1,
        6,
        1,
        "BoyuanFeng, leijurv",
        "APPROVED, APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146748"
    ],
    [
        146747,
        "Add hint message for `pack_padded_sequence`",
        "Fixes #144207\r\n\r\nAdd truncate hint message in docs [torch.nn.utils.rnn.pack_padded_sequence](https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pack_padded_sequence.html)\r\n\r\n## Test Result\r\n\r\n![image](https://github.com/user-attachments/assets/46258f36-f6c7-4f11-9213-8513e52a9001)\r\n\r\n",
        "open",
        "2025-02-08T04:28:15Z",
        null,
        null,
        "open source, topic: not user facing",
        "main",
        "opt/nn/rnn",
        1,
        5,
        1,
        1,
        2,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146747"
    ],
    [
        146746,
        "[Inductor] Fix the lowering of squeeze when input is not contiguous",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146746\n\r\n**Summary**\r\nFix issue https://github.com/pytorch/pytorch/issues/143498. The issue happens when we lowering `select = torch.ops.aten.select.int(cat, 1, 0)`. \r\n\r\nFor example, when `cat` is contiguous with size[2, 2] stride[2,1]\r\n\r\n- for eager, it returns a view of size[2,] stride[2,]\r\n- for Inductor lowering, it returns wrong stride 1 instead of 2\r\n```\r\nTensorBox(\r\n  ReinterpretView(\r\n    StorageBox(\r\n      ConcatKernel(name='buf10', layout=FixedLayout('cpu', torch.int64, size=[u0, 2], stride=[2, 1]), inputs=[ComputedBuffer(name='buf8', layout=NonOwningLayout('cpu', torch.int64, size=[u0, 1], stride=[2, 1]), data=Pointwise(device=device(type='cpu'), dtype=torch.int64, inner_fn=<function ReinterpretView.make_loader.<locals>.loader at 0x7f6b856449d0>, ranges=[u0, 1])), ComputedBuffer(name='buf9', layout=NonOwningLayout('cpu', torch.int64, size=[u0, 1], stride=[2, 1]), data=Pointwise(device=device(type='cpu'), dtype=torch.int64, inner_fn=<function ReinterpretView.make_loader.<locals>.loader at 0x7f6b85644790>, ranges=[u0, 1]))])\r\n    ),\r\n    FixedLayout('cpu', torch.int64, size=[u0], stride=[**1**]),\r\n    origins=OrderedSet([select])\r\n  )\r\n)\r\n```\r\n\r\nTo fix this issue, we give the right stride when lowering of `squeeze`.\r\n\r\n**Test Plan**\r\n```\r\npython -u -m pytest -s -v test/inductor/test_unbacked_symints.py -k test_issue_143498\r\n```\r\n\r\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov",
        "open",
        "2025-02-08T03:50:25Z",
        null,
        null,
        "open source, ciflow/trunk, topic: not user facing, module: inductor, ciflow/inductor",
        "gh/leslie-fang-intel/180/base",
        "gh/leslie-fang-intel/180/head",
        2,
        16,
        4,
        2,
        1,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146746"
    ],
    [
        146743,
        "[cutlass backend] refactor tests to remove duplicate logic",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146743\n* #146356\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov",
        "open",
        "2025-02-08T01:36:44Z",
        null,
        null,
        "topic: not user facing, module: inductor, ciflow/inductor",
        "gh/henrylhtsang/3/base",
        "gh/henrylhtsang/3/head",
        1,
        57,
        81,
        1,
        1,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146743"
    ],
    [
        146742,
        "[Dynamo][autograd.Function] Relax backward speculation strict mode: support .requires_grad",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146742\n* #146741\n* #146571\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-02-08T01:30:53Z",
        null,
        null,
        "topic: not user facing, module: dynamo, ciflow/inductor",
        "gh/yanboliang/65/base",
        "gh/yanboliang/65/head",
        2,
        27,
        1,
        1,
        1,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146742"
    ],
    [
        146741,
        "[Dynamo][autograd.Function] Relax backward speculation strict mode: support .data",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #146742\n* __->__ #146741\n* #146571\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-02-08T01:30:49Z",
        null,
        null,
        "topic: not user facing, module: dynamo, ciflow/inductor",
        "gh/yanboliang/64/base",
        "gh/yanboliang/64/head",
        2,
        29,
        1,
        1,
        1,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146741"
    ],
    [
        146739,
        "Testing",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146739\n* #145748\n\nThis reverts commit 5cd5b4d2d54c0220b92ee488dd36d789c9b60af3.",
        "open",
        "2025-02-08T00:30:57Z",
        null,
        null,
        "release notes: releng, ciflow/binaries_wheel",
        "gh/mikaylagawarecki/312/base",
        "gh/mikaylagawarecki/312/head",
        8,
        25,
        80,
        2,
        1,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146739"
    ],
    [
        146738,
        "[audio hash update] update the pinned audio hash",
        "This PR is auto-generated nightly by [this action](https://github.com/pytorch/pytorch/blob/main/.github/workflows/nightly.yml).\nUpdate the pinned audio hash.",
        "open",
        "2025-02-08T00:23:09Z",
        null,
        null,
        "open source, ciflow/trunk, topic: not user facing, ciflow/inductor, merging",
        "main",
        "update-audio-commit-hash/13210264744-1454-1",
        1,
        1,
        1,
        1,
        4,
        0,
        "pytorchbot",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146738"
    ],
    [
        146737,
        "[dynamo][user-defined] Unify standard and non-standard __new__ codebase",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #146678\n* __->__ #146737\n* #146677\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-02-08T00:21:25Z",
        null,
        null,
        "ciflow/trunk, topic: not user facing, module: dynamo, ciflow/inductor, keep-going",
        "gh/anijain2305/676/base",
        "gh/anijain2305/676/head",
        1,
        17,
        28,
        2,
        1,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146737"
    ],
    [
        146736,
        "Document dynamo",
        "Many files in dynamo are currently lacking file/module-level documentation, which makes it hard to know what they do at a glance and without digging into the code. This fixes that.\r\n\r\nNote: documentation was AI-generated and could be incorrect, please review carefully.\r\n\r\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @xmfan @svekars @brycebortree @sekyondaMeta @AlannaBurke",
        "open",
        "2025-02-08T00:15:41Z",
        null,
        null,
        "better-engineering, ciflow/trunk, topic: not user facing, module: inductor, module: dynamo, ciflow/inductor, module: compiled autograd",
        "main",
        "gh/raymo/document-dynamo",
        30,
        601,
        45,
        1,
        4,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146736"
    ],
    [
        146735,
        "[ca] log graph before reodering passes",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146735\n* #146720\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @yf225",
        "open",
        "2025-02-07T23:39:50Z",
        null,
        null,
        "module: dynamo, ciflow/inductor, module: compiled autograd",
        "gh/xmfan/177/base",
        "gh/xmfan/177/head",
        1,
        12,
        0,
        1,
        2,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146735"
    ],
    [
        146734,
        "[CUDA][CUDNN][SDPA] Pass dropout seed and offset to cuDNN in `int64`",
        "Workaround for limitation in cuDNN that does not accept dropout seed/offset in `int32` for SM 10.0 kernels.\n\ncc @csarofeen @ptrblck @xwang233 @msaroufim",
        "open",
        "2025-02-07T23:30:33Z",
        null,
        null,
        "module: cudnn, module: cuda, triaged, open source, topic: not user facing, module: sdpa",
        "main",
        "cudnnsm100dropoutworkaround",
        1,
        31,
        12,
        1,
        1,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146734"
    ],
    [
        146733,
        "[CUDA][SDPA] Don't dispatch to mem eff attn for batch_size >= 65536",
        "#146704\n\ncc @ptrblck @msaroufim",
        "open",
        "2025-02-07T23:29:27Z",
        null,
        null,
        "module: cuda, open source, topic: not user facing, module: sdpa",
        "main",
        "memeff65536",
        2,
        23,
        0,
        1,
        2,
        0,
        "drisspg, drisspg, nikitaved",
        "COMMENTED, APPROVED, COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146733"
    ],
    [
        146731,
        "dont specialize symints when testing truthiness",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #133044\n* #146729\n* #146642\n* __->__ #146731\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-02-07T22:48:49Z",
        null,
        null,
        "module: dynamo, ciflow/inductor",
        "gh/bdhirsh/641/base",
        "gh/bdhirsh/641/head",
        2,
        20,
        1,
        1,
        3,
        1,
        "bobrenjc93",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146731"
    ],
    [
        146730,
        "[BaseHOP] change hop(subgraph, operands) to hop(subgraph, *operands)",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146730\n* #146727\n\nOur three main users are OK with this, with two of them (foreach_map,\ninvoke_quant) prefering it like this.\n\nI was originally worried about BC issues (this now means you cannot add\nany positional args) but I think that's not a concern -- one can always\nadd kwonly args.\n\nTest Plan\n- tests\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov",
        "open",
        "2025-02-07T22:42:43Z",
        null,
        null,
        "release notes: foreach_frontend, module: inductor, module: dynamo, ciflow/inductor",
        "gh/zou3519/1130/base",
        "gh/zou3519/1130/head",
        6,
        35,
        48,
        1,
        1,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146730"
    ],
    [
        146729,
        "support meta_tensor.to(device='cpu') under fake_mode",
        "Fixing this is actually a bit annoying:\r\n\r\n(1) FakeTensorMode sees a function where all of its inputs are real tensors, so it tries to run the real compute before converting the output to a FakeTensor\r\n\r\n(2) we don't actually want this, because the \"real compute\" is support to error normally, when you do `meta_tensor.to(device='cpu')`. Instead, we want FakeTensor to actually skip constant prop and run the normal FakeTensor implementation, which will not error\r\n\r\nStack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #133044\n* __->__ #146729\n* #146642\n* #146731\n\r\n",
        "open",
        "2025-02-07T22:19:32Z",
        null,
        null,
        "ciflow/inductor",
        "gh/bdhirsh/640/base",
        "gh/bdhirsh/640/head",
        2,
        12,
        0,
        2,
        2,
        0,
        "zou3519, zou3519, bdhirsh",
        "COMMENTED, APPROVED, COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146729"
    ],
    [
        146728,
        "[StaticRuntime] Fix a bug that memory planner ignores subblocks",
        "Summary: When Static Runtime graph node has sub-blocks, the memory planner does not consider sub-blocks' inputs as a node's input in memory planner. As the result, such nodes' inputs' lifetime is incorrect and corresponding tensor memory is released earlier than required and causes errors.\n\nDifferential Revision: D69195886\n\n\n\n\ncc @EikanWang @jgong5 @wenzhe-nrv @sanchitintel",
        "open",
        "2025-02-07T22:13:28Z",
        null,
        null,
        "oncall: jit, fb-exported, release notes: jit",
        "main",
        "export-D69195886",
        3,
        75,
        8,
        1,
        7,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146728"
    ],
    [
        146727,
        "Rename PrimHOPBase to BaseHOP + minor changes",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #146730\n* __->__ #146727\n\nThis PR:\n- renames PrimHOPBase to BaseHOP\n- changes the backward pass to always return a tuple (to match the\n  forward pass).\n\nTest Plan:\n- tests\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-02-07T22:00:22Z",
        null,
        null,
        "release notes: foreach_frontend, module: dynamo, ciflow/inductor",
        "gh/zou3519/1129/base",
        "gh/zou3519/1129/head",
        6,
        18,
        18,
        1,
        1,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146727"
    ],
    [
        146726,
        "[ez][BE] get rid of the extra printf('\\n')",
        "Summary: as title\n\nTest Plan:\n```\nAOT_INDUCTOR_DEBUG_INTERMEDIATE_VALUE_PRINTER=3  TORCHINDUCTOR_FORCE_DISABLE_CACHES=1 TORCHINDUCTOR_ABI_COMPATIBLE=1 TORCH_COMPILE_DEBUG=1 TORCH_LOGS=\"+graph, inductor, +schedule, output_code\" buck2 run -c fbcode.enable_gpu_sections=true -c fbcode.nvcc_arch=h100a @//mode/opt fbcode//caffe2/test/inductor:test_aot_inductor -- -r test_addmm_cuda\n```\n\nDifferential Revision: D69328701\n\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov",
        "open",
        "2025-02-07T21:57:49Z",
        null,
        null,
        "fb-exported, ciflow/trunk, topic: not user facing, module: inductor, ciflow/inductor, merging",
        "main",
        "export-D69328701",
        1,
        1,
        2,
        1,
        5,
        0,
        "ColinPeppler",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146726"
    ],
    [
        146723,
        "torch: Log a unified waitcounter for torch.compile and triton.autotune",
        "Summary: Add a second more generic waitcounter to torch.compile. We'll keep expanding this as new generic pytorch compilation sites show up.\n\nTest Plan: Waitcounter only change, relying on existing tests.\n\nDifferential Revision: D69215401\n\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov",
        "open",
        "2025-02-07T20:59:32Z",
        null,
        null,
        "fb-exported, ciflow/trunk, topic: not user facing, module: inductor, ciflow/inductor",
        "main",
        "export-D69215401",
        1,
        3,
        1,
        1,
        3,
        0,
        "davidberard98",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146723"
    ],
    [
        146722,
        "Test on in-graph constructed NJTs",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146722\n* #146721\n\r\nA recent set of bugs has been cropping up related to NJTs that constructed in-graph within a compiled function. This exercises different paths related to symbolic nested ints, etc. Some examples:\r\n* #145874\r\n* #146644\r\n\r\nTo get ahead of these, we should do NJT testing for this case as well.\r\n\r\nThis PR parametrizes the OpInfo tests for compile + forward to cover both in-graph constructed NJT and normal input cases. TBD what fails..\r\n\r\nTODO:\r\n* Do this for compile + backward tests also (?)",
        "open",
        "2025-02-07T20:09:15Z",
        null,
        null,
        "topic: not user facing",
        "gh/jbschlosser/229/base",
        "gh/jbschlosser/229/head",
        2,
        44,
        6,
        2,
        1,
        0,
        "jbschlosser, cpuhrsch, soulitzer",
        "COMMENTED, APPROVED, APPROVED",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146722"
    ],
    [
        146721,
        "Use inductor backend for NJT compile tests",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #146722\n* __->__ #146721\n\r\nWe've been using `backend=\"aot_eager_decomp_partition\"` for NJT compile testing, but this can let inductor bugs slip through. This PR switches the compile tests to use `backend=\"inductor\"`; let's see if test runtime is an issue after this.",
        "open",
        "2025-02-07T20:09:11Z",
        null,
        null,
        "topic: not user facing",
        "gh/jbschlosser/228/base",
        "gh/jbschlosser/228/head",
        1,
        3,
        7,
        2,
        1,
        0,
        "soulitzer",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146721"
    ],
    [
        146720,
        "[ca] remove private API: _compiled_autograd_should_lift",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #146735\n* __->__ #146720\n\r\nSince the functional autograd + compiled autograd migration, we don't trace into nodes anymore, and everything is lifted. We can't support this flag which tries to inline make_fx style in CA initial pass. There's no more usage internally.",
        "open",
        "2025-02-07T20:06:08Z",
        null,
        null,
        "ciflow/trunk, ciflow/inductor, release notes: dynamo",
        "gh/xmfan/176/base",
        "gh/xmfan/176/head",
        4,
        0,
        15,
        1,
        7,
        0,
        "zou3519",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146720"
    ],
    [
        146718,
        "[CUDAGraph] add skip message for unbacked symint",
        "Add explicit skip message for unbacked symint in cudagraph, as suggested by @bdhirsh.\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov",
        "open",
        "2025-02-07T19:29:37Z",
        null,
        null,
        "ciflow/trunk, topic: not user facing, module: inductor, module: dynamo, ciflow/inductor",
        "main",
        "bf/cg-skip-unbacked-symint-msg",
        4,
        31,
        14,
        1,
        1,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146718"
    ],
    [
        146717,
        "[BE][cuDNN] cuDNN to 9.7.1.26 for CUDA 12.8",
        "cuDNN 9.7.1 is out now and is expected to be the longer-lived branch with more potential backports vs. 9.7.0\r\n\r\nCC @nWEIdia @tinglvv \n\ncc @malfet @seemethere @csarofeen @ptrblck @xwang233",
        "open",
        "2025-02-07T19:25:00Z",
        null,
        null,
        "module: build, module: cudnn, triaged, open source, topic: not user facing, topic: build",
        "main",
        "cudnn971",
        6,
        14,
        12,
        1,
        1,
        0,
        "drisspg",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146717"
    ],
    [
        146716,
        "[BE] Remove outdated RPC benchmark",
        "We have lots of outdated unused + uncalled code in our codebase, namely in our benchmarks and examples folders among others. The last change to this directory was 4 years ago and this code looks dead. cc @albanD @H-Huang for feedback\r\n\r\nStack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\r\n* __->__ #146716\r\n\r\n",
        "open",
        "2025-02-07T18:52:53Z",
        null,
        null,
        "release notes: distributed (rpc), skip-pr-sanity-checks",
        "gh/janeyx99/223/base",
        "gh/janeyx99/223/head",
        29,
        0,
        2535,
        1,
        1,
        0,
        "Skylion007, H-Huang",
        "APPROVED, APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146716"
    ],
    [
        146715,
        "[export][ez] Allow math.trunc for serialization.",
        "Summary: as title.\n\nTest Plan: CI\n\nDifferential Revision: D69317084\n\n\n",
        "open",
        "2025-02-07T18:24:55Z",
        null,
        null,
        "fb-exported, ciflow/trunk, ciflow/inductor, release notes: export",
        "main",
        "export-D69317084",
        1,
        1,
        0,
        1,
        2,
        0,
        "angelayi",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146715"
    ],
    [
        146714,
        "[hop] Support more output types for `flat_apply`",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #146367\n* __->__ #146714\n* #146713\n\nThis patch enables `flat_apply` to support certain non-Tensor output\ntypes like containers and graphable types. This will in turn enable the\nupcoming `mark_traceable` to support more output types.\n\nThe patch also exposes a `func_to_graphable` rather than having the\nusers calling the lower level `pytree.flatten(ConstantFunction(...))`.\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-02-07T17:43:34Z",
        null,
        null,
        "module: dynamo, ciflow/inductor",
        "gh/StrongerXi/83/base",
        "gh/StrongerXi/83/head",
        2,
        54,
        21,
        1,
        2,
        0,
        "zou3519",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146714"
    ],
    [
        146713,
        "[dynamo][fx] Support dataclass whose fields have `init=False`",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* (to be filled)\n\nPreviously Dynamo and FX have code paths that reconstruct a dataclass\ninstance based on its type and fields; however they weren't taking\n`init=False` into account (which is supposed to exclude the field from\nconstructor).\n\nThis patch fixes that, and also updates `pytree.LeafSpec` so that its\n`__init__` conforms with the `init` attribute of its fields. Without\nthis change, the aforementioned reconstruction logic would fail.\n\ncc @ezyang @SherlockNoMad @EikanWang @jgong5 @wenzhe-nrv @voznesenskym @penguinwu @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-02-07T17:43:29Z",
        null,
        null,
        "release notes: fx, fx, module: dynamo, ciflow/inductor",
        "gh/StrongerXi/82/base",
        "gh/StrongerXi/82/head",
        4,
        61,
        9,
        1,
        1,
        0,
        "Skylion007",
        "COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146713"
    ],
    [
        146710,
        "[MTIA] (3/n) Implement PyTorch APIs to query/reset device peak memory usage",
        "Summary: Public summary (shared with Github): This diff implements a C++-Python binding to enable `reset_peak_memory_stats`.\n\nTest Plan: The test is implemented in the following diff.\n\nReviewed By: yuhc\n\nDifferential Revision: D68988673\n\n\n",
        "open",
        "2025-02-07T16:52:00Z",
        null,
        null,
        "fb-exported, ciflow/trunk, topic: not user facing, merging",
        "main",
        "export-D68988673",
        3,
        9,
        1,
        1,
        6,
        0,
        "nautsimon",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146710"
    ],
    [
        146709,
        "FSDP: avoid resetting version counter of all_gather_output in inference_mode",
        "Summary:\nFSDP needs to hide VC bumps on its allgather buffer, but it does not need to do this is the allgather buffer was generated under inference mode.\n\nmore details here: https://www.internalfb.com/diff/D69115649?dst_version_fbid=1316814572779281&transaction_fbid=849120230625711\n\nTest Plan: CI\n\nDifferential Revision: D69311496\n\n\n\n\ncc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o",
        "open",
        "2025-02-07T16:39:54Z",
        null,
        null,
        "oncall: distributed, fb-exported, ciflow/trunk, release notes: distributed (fsdp), ciflow/inductor, merging",
        "main",
        "export-D69311496",
        1,
        9,
        1,
        1,
        6,
        0,
        "awgu",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146709"
    ],
    [
        146706,
        "cpp_wrapper: persist autotune example tensors until last use",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\r\n* #146452\r\n* __->__ #146706\r\n* #146424\r\n* #146109\r\n* #146449\r\n* #144349\r\n* #144293\r\n* #144002\r\n\r\nPatches over an issue where randomly generated example tensors can cause kernel autotuning to fail, when those tensors would not be possible outputs from previous kernels in the sequence. This fixes a failure in `test_torchinductor_opinfo.py` when run with compile-time autotuning, `test_comprehensive_nanquantile_cuda_float64`.\r\n\r\nFor clarity, the situation triggering this PR looks like kernels `A -> BCDE -> F` (`BCDE` is fused), where one of the outputs from `A` is a boolean tensor describing some of the input data. Previously, we randomly regenerated that boolean tensor and the input data before passing them to `BCDE`, so that they no longer matched. This caused a `tl.device_assert` call in `BCDE` to fail. With this PR, we reuse the random data input to `A` and the output Boolean tensor, such that they match and pass the device assertion in `BCDE`.\r\n\r\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @desertfire @chauhang @aakhundov",
        "open",
        "2025-02-07T16:08:21Z",
        null,
        null,
        "open source, topic: not user facing, module: inductor, ciflow/inductor",
        "gh/benjaminglass1/67/base",
        "gh/benjaminglass1/67/head",
        1,
        28,
        8,
        2,
        2,
        0,
        "benjaminglass1",
        "COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146706"
    ],
    [
        146705,
        "Remove NO_MULTIPROCESSING_SPAWN checks",
        "py 3.9 has spawn.\r\n\r\ncc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o",
        "open",
        "2025-02-07T15:19:32Z",
        null,
        null,
        "oncall: distributed, triaged, open source, release notes: distributed (torchelastic)",
        "main",
        "NO_MULTIPROCESSING_SPAWN",
        11,
        23,
        156,
        1,
        1,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146705"
    ],
    [
        146696,
        "Add full_like.default to list of ops with kwargs",
        "The _maybe_insert_input_observers_for_node function expects ops, except a few exceptions, to have zero kwargs. full_like.default seems to be one of these cases and should therefore be added to the list.\r\n\r\nAddresses https://github.com/pytorch/pytorch/issues/146621\r\n\r\nFixes #146621 \r\n",
        "open",
        "2025-02-07T11:34:31Z",
        null,
        null,
        "triaged, open source, release notes: quantization, release notes: AO frontend",
        "main",
        "main",
        2,
        3,
        2,
        1,
        2,
        1,
        "Xia-Weiwen",
        "COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146696"
    ],
    [
        146695,
        "Enable Windows tests",
        "Fixes #ISSUE_NUMBER\r\n\n\ncc @peterjc123 @mszhanyi @skyline75489 @nbcsm @iremyux @Blackhex",
        "open",
        "2025-02-07T11:34:26Z",
        null,
        null,
        "module: windows, triaged, open source, topic: not user facing",
        "main",
        "win_test294",
        3,
        1,
        23,
        2,
        1,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146695"
    ],
    [
        146692,
        "[clang-tidy] Add suppression clang-diagnostic-shadow",
        "Summary:\r\nReviewed By: varun2784\r\n\r\nDifferential Revision: D69182465\r\n\r\n\r\n",
        "open",
        "2025-02-07T10:06:04Z",
        null,
        null,
        "fb-exported, topic: not user facing",
        "main",
        "export-D69182465",
        1,
        2,
        1,
        1,
        8,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146692"
    ],
    [
        146690,
        "Enable pt2e quantization path for arm",
        "**Title**: Enable PyTorch 2 Export Quantization path for ARM CPUs.\r\n\r\n**Description:**\r\n - This PR extends the PyTorch 2 Export Quantization (PT2E Quantization) workflow\u2014originally available only on x86 CPUs\u2014to support ARM platforms. PT2E Quantization is an automated, full-graph quantization solution in PyTorch that improves on Eager Mode Quantization by adding support for functionals and automating the overall process. It is part of the torch.ao module and fully supports quantization when using the compile mode.\r\n\r\n**Key Changes:**\r\n\r\n - Introduces ARM-specific support by leveraging oneDNN kernels for matmuls and convolution.\r\n\r\n - Integrates pre-defined configuration selection to automatically choose the best quantization settings based on the selected quantization method.\r\n\r\n**Provides customization options via two flags:**\r\n\r\n - **qat_state:** Indicates whether to use Quantization Aware Training (if set to True) or Post Training Quantization (if set to False). The default remains False.\r\n - **dynamic_state:** Selects between dynamic quantization (if True) and static quantization (if False). The default is also set to False.\r\n![Screenshot 2025-01-22 105543](https://github.com/user-attachments/assets/c611a1ce-9274-4b70-9c58-cae96000d06d)\r\n\r\nThese options allow users to tailor the quantization process for their specific workload requirements (e.g., using QAT for fine-tuning or PTQ for calibration-based quantization).\r\n\r\nTesting and Validation:\r\n\r\nThe new ARM flow has been thoroughly tested across a range of models with all combinations:\r\n**NLP**: Models such as BERT and T5.\r\n**Vision**: Models like ResNet and ViT.\r\n**Custom Models**: user defined models with various operators.\r\n\r\nexample script:\r\n```\r\nimport torch\r\nimport torchvision.models as models\r\nfrom torch.ao.quantization.quantize_pt2e import prepare_pt2e, convert_pt2e\r\nimport torch.ao.quantization.quantizer.arm_inductor_quantizer as armiq\r\nfrom torch.ao.quantization.quantizer.arm_inductor_quantizer import ArmInductorQuantizer\r\nfrom torch.profiler import profile, record_function, ProfilerActivity\r\n\r\nmodel_name = \"resnet50\"\r\nmodel = models.__dict__[model_name](pretrained=True)\r\n\r\n# Set the model to eval mode\r\nmodel = model.eval()\r\n\r\n# Create the data, using the dummy data here as an example\r\ntraced_bs = 500\r\nx = torch.randn(traced_bs, 3, 224, 224).contiguous(memory_format=torch.channels_last)\r\nexample_inputs = (x,)\r\n\r\nwith torch.no_grad():\r\n    exported_model = torch.export.export_for_training(model, example_inputs).module()\r\n    quantizer = armiq.ArmInductorQuantizer()\r\n    quantizer.set_global(armiq.get_default_arm_inductor_quantization_config(is_dynamic=False))\r\n    prepared_model = prepare_pt2e(exported_model, quantizer)\r\n    converted_model = convert_pt2e(prepared_model)\r\n\r\n    with torch.set_grad_enabled(False):\r\n        for _ in range(50):\r\n            converted_model(*example_inputs) #Warmup\r\n        print(\"Warmup over\")\r\n        with profile(activities=[ProfilerActivity.CPU], record_shapes=True) as prof:\r\n            with record_function(\"model_inference\"):\r\n                for _ in range(100):\r\n                    converted_model(*example_inputs)\r\n\r\n    print(prof.key_averages(group_by_input_shape=True).table(sort_by=\"self_cpu_time_total\"))\r\n\r\n```\n\ncc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @malfet @snadampal @milpuz01",
        "open",
        "2025-02-07T09:23:48Z",
        null,
        null,
        "module: cpu, triaged, open source, module: arm, release notes: quantization, release notes: AO frontend",
        "main",
        "devang/pt2e_quantization_arm",
        2,
        1592,
        0,
        1,
        7,
        1,
        "jerryzh168",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146690"
    ],
    [
        146689,
        "Update addbmm, addmm, addmv and baddbmm description",
        "Fixes #146611, following #146482\r\n\r\n## Test Result\r\n\r\n![image](https://github.com/user-attachments/assets/5c1749be-1f10-4e80-a284-b1929ca340eb)\r\n",
        "open",
        "2025-02-07T08:57:51Z",
        null,
        null,
        "triaged, open source, release notes: python_frontend",
        "main",
        "opt/docs/add",
        1,
        4,
        4,
        1,
        7,
        0,
        "mikaylagawarecki",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146689"
    ],
    [
        146687,
        "Make GetCPUAllocatorMaybePinned to be Device-Agnostic",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146687\n\n----\n\n- Keep cuda first to perserve BC\n- Remove cuda first if it is possible to have only one accelerator at a time in the future",
        "open",
        "2025-02-07T08:54:31Z",
        null,
        null,
        "open source, topic: not user facing",
        "gh/fffrog/38/base",
        "gh/fffrog/38/head",
        2,
        18,
        15,
        1,
        2,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146687"
    ],
    [
        146684,
        "Optimize LRScheduler docs",
        "Fixes #120735\r\n\r\nAdd more description about [`LRScheduler`](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.LRScheduler.html#torch.optim.lr_scheduler.LRScheduler)\r\n\r\n## Test Result\r\n\r\n### Before\r\n\r\n![image](https://github.com/user-attachments/assets/619c3ea8-5652-4e61-936f-0cb5aa5a326b)\r\n\r\n### After\r\n\r\n![image](https://github.com/user-attachments/assets/174a6ffc-5da2-4837-bf49-2f09f6c7b6ee)\r\n\r\n![image](https://github.com/user-attachments/assets/ae1bc984-49cc-4d5b-8d81-08f460b71361)\r\n\r\ncc @janeyx99\r\n",
        "open",
        "2025-02-07T08:42:57Z",
        null,
        null,
        "triaged, open source, release notes: optim",
        "main",
        "opt/docs/LRScheduler",
        1,
        21,
        2,
        1,
        1,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146684"
    ],
    [
        146678,
        "[dynamo][not ready] polyfill infra for classes",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146678\n* #146737\n* #146677\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-02-07T06:41:35Z",
        null,
        null,
        "module: dynamo, ciflow/inductor",
        "gh/anijain2305/675/base",
        "gh/anijain2305/675/head",
        7,
        247,
        41,
        3,
        2,
        0,
        "XuehaiPan",
        "COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146678"
    ],
    [
        146677,
        "[dynamo][user-defined] User class.__new__ instead of special casing",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #146678\n* #146737\n* __->__ #146677\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-02-07T06:41:32Z",
        null,
        null,
        "ciflow/trunk, topic: not user facing, module: dynamo, ciflow/inductor, keep-going",
        "gh/anijain2305/674/base",
        "gh/anijain2305/674/head",
        6,
        168,
        104,
        3,
        1,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146677"
    ],
    [
        146675,
        "[ROCm] Move ROCm unstable MI300 jobs back to stable",
        "Fixes #145790\r\n\r\nThis PR moves rocm unstable MI300 back to stable. The change to unstable was introduced through this [PR](https://github.com/pytorch/pytorch/pull/145790). This was because the MI300s were failing with a [docker daemon](https://github.com/pytorch/pytorch/actions/runs/13015957622/job/36306779536) issue which has been resolved.\r\n\r\ncc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd @ZainRizvi \r\n\r\ncc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd",
        "open",
        "2025-02-07T05:22:51Z",
        null,
        null,
        "module: rocm, open source, topic: not user facing, ciflow/inductor, ciflow/rocm, ciflow/inductor-rocm",
        "main",
        "patch-8",
        3,
        80,
        70,
        8,
        1,
        0,
        "jithunnair-amd, jithunnair-amd",
        "COMMENTED, COMMENTED",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146675"
    ],
    [
        146671,
        "Update torch-xpu-ops commit pin",
        "Update the torch-xpu-ops commit to [662837e722cbbc0701fbcf6ea9ad6383158cc44e](https://github.com/intel/torch-xpu-ops/commit/662837e722cbbc0701fbcf6ea9ad6383158cc44e), includes:\r\n\r\n- Aten operator coverage improvement\r\n- SYCL kernel optimization\r\n- Nested Tensor OPs support\r\n",
        "open",
        "2025-02-07T03:28:45Z",
        null,
        null,
        "triaged, open source, topic: not user facing, keep-going, ciflow/xpu",
        "main",
        "xyt/xpu_pin_662837e722cbbc0701fbcf6ea9ad6383158cc44e",
        1,
        1,
        1,
        1,
        2,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146671"
    ],
    [
        146669,
        "Optimize inductor `Self` typing",
        "Replace method return type with `Self` typing\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @desertfire @chauhang @aakhundov",
        "open",
        "2025-02-07T01:59:49Z",
        null,
        null,
        "triaged, open source, topic: not user facing, module: inductor",
        "main",
        "opt/inductor/typing",
        3,
        8,
        5,
        1,
        3,
        0,
        "jansel",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146669"
    ],
    [
        146668,
        "Make sure cutlass kernel .cu file has configuration name and nvcc compile command",
        "I think its good to have everything in the .cu file. Especially the nvcc compile command.\r\n\r\nTechnically, the configuration name can be found in the template already. So let me know if you think its not needed. \r\n\r\nDifferential Revision: D69281295\r\n\r\n\r\n\r\n\r\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @desertfire @chauhang @aakhundov",
        "open",
        "2025-02-07T01:54:45Z",
        null,
        null,
        "fb-exported, ciflow/trunk, topic: not user facing, module: inductor, ciflow/inductor, merging",
        "main",
        "export-D69281295",
        2,
        6,
        0,
        1,
        8,
        0,
        "chenyang78",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146668"
    ],
    [
        146664,
        "[Docs] Fix description of `input` in `torch.addbmm()`",
        "Fixes #146613\r\n",
        "open",
        "2025-02-07T01:27:52Z",
        null,
        null,
        "open source, ciflow/trunk, release notes: python_frontend, topic: docs, merging",
        "main",
        "docs/addbmm",
        1,
        1,
        1,
        1,
        10,
        1,
        "mikaylagawarecki",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146664"
    ],
    [
        146662,
        "[Optimus] Bug fix in the select cat aten pass",
        "Summary: Thanks to Shuai for reporting the bug in the pattern. We found there's a typo in the pass, where we should make sure all the selects will go to the cat node.\n\nTest Plan:\nbuck2 test 'fbcode//mode/dev-nosan' fbcode//caffe2/test/inductor:split_cat_fx_aten_passes -- test_select_cat_post_grad\n\n\nBuck UI: https://www.internalfb.com/buck2/2cd0888e-d803-43a8-8530-d97e6bc281b3\nTest UI: https://www.internalfb.com/intern/testinfra/testrun/6192449699305108\nNetwork: Up: 110KiB  Down: 35KiB  (reSessionID-687be0fa-031a-47a0-8780-5ab4cf4bbd94)\nExecuting actions. Remaining     0/4                                                                              6.6s exec time total\nCommand: test.     Finished 2 local\nTime elapsed: 2:12.0s\nTests finished: Pass 2. Fail 0. Fatal 0. Skip 0. Build failure 0\n\nDifferential Revision: D69278487\n\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @desertfire @chauhang @aakhundov",
        "open",
        "2025-02-07T00:50:59Z",
        null,
        null,
        "fb-exported, module: inductor, ciflow/inductor, release notes: inductor, inductor_pattern_match",
        "main",
        "export-D69278487",
        2,
        53,
        35,
        1,
        4,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146662"
    ],
    [
        146661,
        "[cond] Refactor cond_op's signature to take *operands.",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146661\n* #146660\n\r\n\r\nThis is a BC-breaking change for hop's IR schema. Previously, \r\n```python\r\ntorch.cond(pred, true_fn, false_fn, (a, b))\r\n# Old representation\r\ntorch.ops.higher_order.cond(pred, true_gm, false_gm, (a, b))\r\n# New representation:\r\ntorch.ops.higher_order.cond(pred, true_gm, false_gm, a, b)\r\n```\r\nThe benefits of this change is that it's much easier to construct the schema since the tuple is flattened. What's particularly troublesome about previous node is that it's hard to represent the mutation and alias information inside the tuple: we have to change the legacy schema parser and verify (maybe re-purpose) the aliasInfo to supports nested aliasInfo inside tuple/list.\r\n\r\nWe'll also refactor other control flow operators.\r\n\r\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @desertfire @chauhang @aakhundov\r\n\r\nDifferential Revision: [D69279033](https://our.internmc.facebook.com/intern/diff/D69279033)",
        "open",
        "2025-02-07T00:44:09Z",
        null,
        null,
        "ciflow/trunk, topic: not user facing, module: inductor, module: dynamo, ciflow/inductor, keep-going, release notes: export",
        "gh/ydwu4/208/base",
        "gh/ydwu4/208/head",
        13,
        119,
        181,
        3,
        3,
        0,
        "zou3519",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146661"
    ],
    [
        146660,
        "[hop][inductor] don't promote arg type for cond and while_loop",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #146661\n* __->__ #146660\n\r\nHop subgraph codegen assumes arguments's type are not promoted. Otherwise, we might generate wrong kernel.\r\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @desertfire @chauhang @aakhundov\n\nDifferential Revision: [D69279031](https://our.internmc.facebook.com/intern/diff/D69279031)",
        "open",
        "2025-02-07T00:44:03Z",
        null,
        null,
        "ciflow/trunk, topic: not user facing, module: inductor, ciflow/inductor, merging",
        "gh/ydwu4/207/base",
        "gh/ydwu4/207/head",
        1,
        2,
        2,
        2,
        6,
        1,
        "zou3519, zou3519, ydwu4",
        "COMMENTED, APPROVED, COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146660"
    ],
    [
        146658,
        "[HOP] Mutation and alias rework",
        "This PR reworks the way the input mutations and various aliases are checked\r\n\r\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @ydwu4 \r\n",
        "open",
        "2025-02-07T00:28:18Z",
        null,
        null,
        "open source, topic: not user facing, module: dynamo",
        "main",
        "mutation_alias_rework",
        12,
        116,
        100,
        5,
        2,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146658"
    ],
    [
        146656,
        "Optimize isclose() for CPU and GPU by adding specific implementations",
        "`isclose()` is currently quite slow, so this PR adds specific implementations for both CPU and cuda.\r\n\r\nCUDA implementation seeing ~4.9x improvement at 100m elements and ~18.7x improvement at 400m elements\r\n\r\nCPU implementation seeing ~5.7x improvements at 100m elements and ~5.9x improvements at 400m elements \r\n\r\nsimple benchmark used(adapt with CPU as needed):\r\n```python\r\nimport time\r\nimport numpy as np\r\nimport torch\r\n\r\ndef benchmark_isclose(shape1, shape2, num_runs=5):\r\n    tensor1 = torch.randn(shape1, device=\"cuda\")\r\n    tensor2 = tensor1.clone()\r\n    tensor2 += torch.randn_like(tensor2) * 0.001\r\n\r\n    # warn up\r\n    _ = torch.isclose(tensor1, tensor2)\r\n    torch.cuda.synchronize()\r\n\r\n    times = []\r\n    for _ in range(num_runs):\r\n        start_time = time.perf_counter()\r\n\r\n        _ = torch.isclose(tensor1, tensor2)\r\n        torch.cuda.synchronize()\r\n        end_time = time.perf_counter()\r\n        times.append(end_time - start_time)\r\n\r\n    mean_time = np.mean(times)\r\n    std_time = np.std(times)\r\n\r\n    return mean_time, std_time\r\n\r\n\r\ntest_shapes = [\r\n    (10000, 10000),  # 100M elements\r\n    (20000, 20000),  # 400M elements\r\n]\r\n\r\nprint(\"\\nBenchmarking torch.isclose():\")\r\nprint(\"-\" * 50)\r\n\r\nfor shape in test_shapes:\r\n    total_elements = np.prod(shape)\r\n    print(f\"\\nTensor shape: {shape} ({total_elements:,} elements)\")\r\n\r\n    mean_time, std_time = benchmark_isclose(shape, shape)\r\n    print(f\"Mean time: {mean_time*1000:.2f} ms +/- {std_time*1000:.2f} ms\")\r\n    print(f\"Elements per second: {total_elements/mean_time:,.0f}\")\r\n```\r\n\r\n```\r\n(optimized)\r\nTensor shape: (10000, 10000) (100,000,000 elements)\r\nMean time: 2.73 ms \u00b1 0.26 ms\r\nElements per second: 36,611,905,024\r\n\r\nTensor shape: (20000, 20000) (400,000,000 elements)\r\nMean time: 8.98 ms \u00b1 0.28 ms\r\nElements per second: 44,546,604,660\r\n\r\n(unoptimized)\r\nTensor shape: (10000, 10000) (100,000,000 elements)\r\nMean time: 13.48 ms \u00b1 0.28 ms\r\nElements per second: 7,420,814,236\r\n\r\nTensor shape: (20000, 20000) (400,000,000 elements)\r\nMean time: 166.90 ms \u00b1 4.71 ms\r\nElements per second: 2,396,711,992\r\n```\r\ncc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @albanD \r\n\r\n",
        "open",
        "2025-02-07T00:09:02Z",
        null,
        null,
        "module: cpu, triaged, open source",
        "main",
        "feature/isclose-kernels",
        4,
        171,
        57,
        3,
        2,
        1,
        "albanD",
        "COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146656"
    ],
    [
        146655,
        "torch._scaled_mm with MX dtypes",
        "making https://github.com/pytorch/pytorch/pull/145562 work with in-core dtypes\r\n\r\nnot ready for review yet",
        "open",
        "2025-02-07T00:02:24Z",
        null,
        null,
        "ciflow/inductor",
        "gh/vkuzo/2/head",
        "gh/vkuzo/3/head",
        8,
        291,
        79,
        3,
        2,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146655"
    ],
    [
        146654,
        "[DCP] Introduce modules metadata in the storage_meta",
        "Summary: Introduce the list of modules in the storage_meta which is shared between the planner and the storage writer. We will use it to let the storage writer know about the modules in the state dict and create module directories in the checkpoint.\n\nTest Plan: UTs\n\nDifferential Revision: D69154628\n\n\n\n\ncc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o @LucasLLC @MeetVadakkanchery @mhorowitz @pradeepfn @ekr0",
        "open",
        "2025-02-06T23:43:44Z",
        null,
        null,
        "oncall: distributed, fb-exported, module: distributed_checkpoint",
        "main",
        "export-D69154628",
        1,
        2,
        1,
        1,
        3,
        0,
        "mhorowitz",
        "COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146654"
    ],
    [
        146653,
        "windows Magma build for cu128",
        "https://github.com/pytorch/pytorch/issues/145570\r\n\r\nremoving `.ci/pytorch/windows/internal/cuda_install.bat` as it is a duplicate with` .github/scripts/windows/cuda_install.bat`. The later one is the one in use - https://github.com/pytorch/pytorch/pull/146653/files#diff-613791f266f2f7b81148ca8f447b0cd6c6544f824f5f46a78a2794006c78957bR8\r\n\r\ncc @atalman @ptrblck @nWEIdia ",
        "open",
        "2025-02-06T23:33:34Z",
        null,
        null,
        "open source, Merged, Reverted, release notes: releng, ciflow/binaries_wheel, ci-no-td",
        "main",
        "cu128-win-magma",
        5,
        95,
        223,
        4,
        6,
        1,
        "atalman",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146653"
    ],
    [
        146642,
        "[poc] force UntypedStorage.from_buffer(buf) to return meta storage under FakeTensorMode",
        "context here: https://fb.workplace.com/groups/326136610199609/permalink/495389539940981/\r\n\r\nThis PR is an attempt to make it such that if you create a tensor from an external buffer (using `UntypedStorage.from_buffer(buf)`, we can generate a proper fake tensor for you out of the box.\r\n\r\nThe annoying bit is that there are not any dispatcher ops to interpose on and change behavior. So instead, I took the manual C binding and tweaked the storage device to be \"meta' if we see an active fake mode.\r\n\r\nPut \"poc\" in the title since I... think this is hopefully reasonable, but I can be convinced that it's not :)\r\n\r\n```\r\nfrom torch._subclasses.fake_tensor import FakeTensorMode\r\nimport pickle\r\nimport io\r\nimport torch\r\nfrom contextlib import nullcontext\r\n\r\n\r\nuse_fake_tensor = True\r\nwith FakeTensorMode() if use_fake_tensor else nullcontext():\r\n    obj = [1, 2]\r\n    f = io.BytesIO()\r\n    pickle.Pickler(f).dump(obj)\r\n    byte_storage = torch.ByteStorage._from_buffer(f.getvalue())  # type: ignore[attr-defined]\r\n    \r\n    t = torch.ByteTensor(byte_storage)\r\n```\r\n\r\nStack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #133044\n* #146729\n* __->__ #146642\n* #146731\n\r\n",
        "open",
        "2025-02-06T21:50:27Z",
        null,
        null,
        "release notes: composability",
        "gh/bdhirsh/639/base",
        "gh/bdhirsh/639/head",
        2,
        27,
        8,
        4,
        2,
        1,
        "zou3519",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146642"
    ],
    [
        146641,
        "[dim order]  solve broken doc",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146641\n\nDifferential Revision: [D69265340](https://our.internmc.facebook.com/intern/diff/D69265340/)",
        "open",
        "2025-02-06T21:49:03Z",
        null,
        null,
        "fb-exported, ciflow/trunk, topic: not user facing, merging",
        "gh/gasoonjia/2/base",
        "gh/gasoonjia/2/head",
        1,
        3,
        0,
        3,
        8,
        0,
        "svekars, svekars, Jack-Khuu",
        "COMMENTED, APPROVED, APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146641"
    ],
    [
        146640,
        "POC for mixed prec optim frontend",
        "This PR is a prototype for what a frontend for asking for mixed precision can look like torch.optim through set_dtype_policy in optimizer.py.\r\n\r\nThis is not meant to be landable but to start some discussions on what people want/would like to see and to ask if there are things I haven't considered yet.\r\n\r\nThis currently only works with Adam(W)!\r\n\r\nA toy script for how to use:\r\n```\r\nimport torch\r\n\r\nmodel = torch.nn.Sequential(\r\n    torch.nn.Linear(2, 3),\r\n    torch.nn.Sigmoid(),\r\n    torch.nn.Linear(3, 1),\r\n    torch.nn.Sigmoid(),\r\n)\r\nmodel.to(\"cuda\")\r\n\r\noptim = torch.optim.AdamW(model.named_parameters(), foreach=False)\r\nmp_policy = {\r\n    \"exp_avg\": lambda _: torch.bfloat16,\r\n    \"exp_avg_sq\": lambda _: torch.bfloat16,\r\n    \"max_exp_avg_sq\": lambda _: torch.bfloat16,\r\n}\r\noptim.set_dtype_policy(mp_policy)\r\n\r\ni = torch.tensor([0.1, 0.2, 0.3, 0.4, 0.5, 0.6], device=\"cuda\").reshape(3, 2)\r\nl = model(i).sum()\r\nl.backward()\r\n\r\noptim.step()\r\n```\r\n\r\nStack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146640\n\r\n",
        "open",
        "2025-02-06T21:31:50Z",
        null,
        null,
        "release notes: optim",
        "gh/janeyx99/222/base",
        "gh/janeyx99/222/head",
        3,
        113,
        11,
        2,
        1,
        0,
        "janeyx99",
        "COMMENTED",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146640"
    ],
    [
        146638,
        "use None to slice when list has one element only",
        "When autotune_num_choices_displayed is None and the list of choices has length 1, slicing with `[:-1]` means getting all elements except the last one, which resulted in an empty list.\r\n\r\nSlicing with `[:None]` works. \r\n\r\nDifferential Revision: D69265168\r\n\r\n\r\n\r\n\r\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @desertfire @chauhang @aakhundov",
        "open",
        "2025-02-06T21:08:40Z",
        null,
        null,
        "fb-exported, ciflow/trunk, topic: not user facing, module: inductor, ciflow/inductor, merging",
        "main",
        "export-D69265168",
        1,
        2,
        5,
        1,
        8,
        0,
        "drisspg",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146638"
    ],
    [
        146637,
        "gloo: fix building system gloo with CUDA/HIP",
        "Fix incorrect linking of Gloo's libraries when building with system Gloo. Previously, either Gloo's native library or Gloo's CUDA library were linked. However, Gloo had changed such that all users of Gloo must link the native library, and can optionally link the CUDA or HIP library for Gloo + CUDA/HIP support.\r\nThis had been updated when building/linking with vendored Gloo, but not when using system Gloo.\r\n\r\nFixes: #146239\r\n\r\nReported-by: Adam J Stewart <ajstewart426@gmail.com>\r\n\r\n\n\ncc @malfet @seemethere @ptrblck @msaroufim @eqy",
        "open",
        "2025-02-06T21:04:17Z",
        null,
        null,
        "module: build, module: cuda, triaged, open source, topic: not user facing",
        "main",
        "gloo_cuda",
        2,
        25,
        25,
        1,
        2,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146637"
    ],
    [
        146636,
        "example repro failure",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146636\n\nSummary:\n\nTest Plan:\n\nReviewers:\n\nSubscribers:\n\nTasks:\n\nTags:\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-02-06T21:00:47Z",
        null,
        null,
        "module: dynamo, ciflow/inductor",
        "gh/c00w/37/base",
        "gh/c00w/37/head",
        2,
        5,
        0,
        1,
        2,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146636"
    ],
    [
        146634,
        "Add Structured Tracing for Traced Graph Edge Details for AC Debugging",
        "Summary:\nUpdating the structured trace infrastructure so that we are able to output to Zoomer and have an E2E solution.\n\nContext Doc: https://docs.google.com/document/d/1T6omIBEWVhbOiwDLSLffgQwjxiT2rQv8QvvQwXkw4fY/edit?usp=sharing\n\nTest Plan:\n### Testing Structured Log + tlparse locally\n\nCommand:\n```\nTORCH_TRACE=/data/users/basilwong/fbsource/fbcode/log_torch_trace buck2 run mode/opt //aps_models/ads/icvr:icvr_launcher -- mode=local_fb_fm_v4 launcher.num_workers=2\n```\n\nTorch Trace Logs (local then sent to paste): P1686419449\n```\ncat log_torch_trace/dedicated_log_torch_trace_rank_0_2lg012xo.log | pastry\nP1686419449\n```\n\ntlparse output: https://manifold.edge.x2p.facebook.net/v0/read/tree/logs/.tmpyiv5wj/rank_1/index.html?bucketName=tlparse_reports&apiKey=tlparse_reports-key&withPayload=1&timeoutMsec=100\n\ntlparse graph edge details output: https://manifold.edge.x2p.facebook.net/v0/read/tree/logs/.tmpyiv5wj/rank_1/9_0_0/joint_graph_information_397.txt?bucketName=tlparse_reports&apiKey=tlparse_reports-key&withPayload=1&timeoutMsec=100\n\nDifferential Revision: D61557220\n\n\n",
        "open",
        "2025-02-06T20:29:48Z",
        null,
        null,
        "fb-exported, topic: not user facing, ciflow/inductor",
        "main",
        "export-D61557220",
        2,
        137,
        38,
        1,
        4,
        1,
        "jansel",
        "CHANGES_REQUESTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146634"
    ],
    [
        146633,
        "[NJT] Fix inference mode for composite implicit ops without nested-specific kernel",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146633\n\n",
        "open",
        "2025-02-06T20:02:09Z",
        null,
        null,
        "ciflow/trunk, topic: bug fixes, release notes: nested tensor, merging",
        "gh/soulitzer/352/base",
        "gh/soulitzer/352/head",
        2,
        27,
        4,
        3,
        10,
        1,
        "jbschlosser",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146633"
    ],
    [
        146632,
        "[ROCm] OCP FP8 Support for new GPUs",
        "TLDR: Follow up/ Build on top of https://github.com/pytorch/pytorch/pull/144476. add OCP FP8 support for gfx950\r\nrefer to https://github.com/pytorch/ao/pull/1677\r\n\r\nThis pull request includes several changes to improve compatibility and support for new GPU architectures and data types, particularly for ROCm. The key updates involve adding support for new ROCm versions and GPU architectures, updating data type handling, and removing outdated checks.\r\n\r\n### Improvements to GPU Architecture and ROCm Version Support:\r\n* [`aten/src/ATen/Context.cpp`](diffhunk://#diff-33de472d304acbe57d693c8567370c638068bedc1aa0ce8e9dc115dad05a7810L323-R326): Added support for new GPU architectures `gfx1200`, `gfx1201`, and `gfx950` based on ROCm version checks.\r\n* [`aten/src/ATen/native/cuda/Blas.cpp`](diffhunk://#diff-e8a569efee1e650172f120a0fdcda024fe3e4703a4ee3336425c8f685af6b3abL196-R199): Updated architecture support in multiple functions to include `gfx1200`, `gfx1201`, and `gfx950` based on ROCm version checks. [[1]](diffhunk://#diff-e8a569efee1e650172f120a0fdcda024fe3e4703a4ee3336425c8f685af6b3abL196-R199) [[2]](diffhunk://#diff-e8a569efee1e650172f120a0fdcda024fe3e4703a4ee3336425c8f685af6b3abL865-R876)\r\n\r\n### Updates to Data Type Handling:\r\n* [`aten/src/ATen/cuda/CUDADataType.h`](diffhunk://#diff-9188bb13b1a49f459141f5f9b875593d1c5ce2beb5ad711fdbaf5bc7089ec015L81-L98): Enhanced data type conversion to include new float8 types for both CUDA and ROCm environments.\r\n* [`aten/src/ATen/cuda/tunable/GemmHipblaslt.h`](diffhunk://#diff-bfa1a3b5d4bef1892bf50338775f3b0fd8cd31fc1868148f3968b98aefb68e3fL29-R80): Updated `HipDataTypeFor` template to handle new float8 types and added hard-coded enum values for ROCm versions prior to 6.3.\r\n\r\n### Removal of Outdated Checks:\r\n* [`cmake/public/LoadHIP.cmake`](diffhunk://#diff-b98e27b9a5f196a6965a99ee5a7bb15b3fc633d6375b767635b1b04ccb2fd3d5L169-L197): Removed the check for `HIP_NEW_TYPE_ENUMS` as it is no longer necessary with the updated ROCm versions. [[1]](diffhunk://#diff-b98e27b9a5f196a6965a99ee5a7bb15b3fc633d6375b767635b1b04ccb2fd3d5L169-L197) [[2]](diffhunk://#diff-b98e27b9a5f196a6965a99ee5a7bb15b3fc633d6375b767635b1b04ccb2fd3d5L211-R182)\r\n\r\nThese changes ensure better compatibility and performance on newer hardware and software environments, particularly for users leveraging ROCm and CUDA for deep learning and scientific computing tasks.\r\n\n\ncc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd",
        "open",
        "2025-02-06T18:57:21Z",
        null,
        null,
        "module: rocm, open source, release notes: linalg_frontend",
        "main",
        "ocp_gfx950",
        11,
        98,
        25,
        8,
        1,
        0,
        "jeffdaily",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146632"
    ],
    [
        146631,
        "Support ignoring parameters in FSDP2",
        "Differential Revision: D69153051\n\n\n\n\ncc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o",
        "open",
        "2025-02-06T18:46:40Z",
        null,
        null,
        "oncall: distributed, fb-exported, release notes: distributed (fsdp), ciflow/inductor",
        "main",
        "export-D69153051",
        3,
        392,
        5,
        1,
        17,
        2,
        "awgu, weifengpy, weifengpy, weifengpy",
        "COMMENTED, COMMENTED, COMMENTED, COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146631"
    ],
    [
        146626,
        " [inductor] Improve type annotations in _inductor/pattern_matcher.py",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146626\n* #146248\n\n\n\ncc @ezyang @SherlockNoMad @EikanWang @jgong5 @wenzhe-nrv @voznesenskym @penguinwu @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @desertfire @chauhang @aakhundov",
        "open",
        "2025-02-06T17:59:04Z",
        null,
        null,
        "open source, release notes: fx, topic: not user facing, fx, module: inductor, ciflow/inductor, suppress-api-compatibility-check, suppress-bc-linter",
        "gh/rec/132/base",
        "gh/rec/132/head",
        2,
        41,
        33,
        6,
        2,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146626"
    ],
    [
        146625,
        "Move capture_provenance to make_node_impl",
        "Previously we were only logging `make_user_impl` implementations, which only gets triggered for operations done on python SymInts, not cpp SymInts. Instead `make_node_impl` will get triggered for both python and cpp SymInt operations.\r\n\r\n\r\ncc @ezyang @SherlockNoMad @EikanWang @jgong5 @wenzhe-nrv",
        "open",
        "2025-02-06T17:53:35Z",
        null,
        null,
        "ciflow/trunk, release notes: fx, fx, ciflow/inductor, merging",
        "main",
        "angelayi/test_expression_created",
        1,
        89,
        90,
        1,
        13,
        2,
        "bobrenjc93, bobrenjc93",
        "APPROVED, APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146625"
    ],
    [
        146623,
        "bug fix: ensure 4d input in _scaled_dot_product_attention_math_mps",
        "This pr addresses the issue in the MPS backend for `_scaled_dot_product_attention_math_mps` where a 3d input like (num_heads, seq_len, query_dim) cannot be automatically treated as (1, num_heads, seq_len, query_dim), which can be inferred on cpu or cuda, which can be circumvented by adding a util function to ensure a 4d shape.\r\n\r\nThe issue was found in https://github.com/hiyouga/LLaMA-Factory/issues/6835, in [transformers qwen2_vl](https://github.com/huggingface/transformers/blob/1590c664306766f32ba68c50e67f14d61b16925d/src/transformers/models/qwen2_vl/modeling_qwen2_vl.py#L373C14-L373C93), 3d q/k/v were passed into sdpa function, which lead to an error.\r\n\r\nConsidering consistency, since this pattern might pop up elsewhere in the transformers codebase, I think it makes more sense to maintain the same intuition across all platforms.\r\n\r\n---\r\nreproduce code:\r\n```\r\nimport torch\r\nimport torch.nn.functional as F\r\n\r\nhead_num, seq_len, embed_dim = 16, 16, 80\r\nbsz = 1\r\n\r\nq = torch.randn(head_num, seq_len, embed_dim)\r\nk = torch.randn(head_num, seq_len, embed_dim)\r\nv = torch.randn(head_num, seq_len, embed_dim)\r\nattention_mask = torch.ones(1, seq_len, seq_len)\r\n\r\noo_cpu = F.scaled_dot_product_attention(\r\n    q.to(\"cpu\"),\r\n    k.to(\"cpu\"),\r\n    v.to(\"cpu\"),\r\n    attention_mask.to(\"cpu\"),\r\n    dropout_p=0.0\r\n)\r\n\r\nif torch.backends.mps.is_available():\r\n    oo_mps = F.scaled_dot_product_attention(\r\n        q.to(\"mps\"),\r\n        k.to(\"mps\"),\r\n        v.to(\"mps\"),\r\n        attention_mask.to(\"mps\"),\r\n        dropout_p=0.0\r\n    )\r\n    assert torch.allclose(oo_cpu, oo_mps.to(\"cpu\"), atol=1e-5)\r\n```\r\n\r\nerror outputs:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/opt/homebrew/Caskroom/miniconda/base/envs/torch-dev/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-2-5169b8d2c5dd>\", line 21, in <module>\r\n    oo_mps = F.scaled_dot_product_attention(\r\nIndexError: Dimension out of range (expected to be in range of [-3, 2], but got 3)\r\n```\r\n\r\nhardware and envs:\r\n```\r\ntorch               2.6.0\r\napple m3 max\r\n```\r\n\r\n---\r\n\r\n",
        "open",
        "2025-02-06T17:15:59Z",
        null,
        null,
        "triaged, open source, topic: bug fixes, release notes: mps, ciflow/mps",
        "main",
        "main",
        2,
        66,
        14,
        6,
        5,
        0,
        "Skylion007, Skylion007, malfet, malfet",
        "COMMENTED, COMMENTED, COMMENTED, COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146623"
    ],
    [
        146622,
        "Fix inductor non-stable argsort/sort test",
        "- Prevent the inductor test for argsort/sort from wrongly failing when the argsort/sort output with stable=False differs from pytorch but is still a valid argsort output.\r\n- Add functionality to allow alternative assert_equal functions in inductor tests for future cases.\r\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @desertfire @chauhang @aakhundov",
        "open",
        "2025-02-06T17:10:12Z",
        null,
        null,
        "triaged, open source, topic: not user facing, module: inductor",
        "main",
        "PYT-466",
        2,
        169,
        19,
        1,
        1,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146622"
    ],
    [
        146620,
        "Enable qint8 and quint8 add for AArch64 using ACL directly",
        "This enables qint8 and quint8 add for AArch64 through Arm Compute Library (ACL) directly.\r\nIt\u2019s based on changes in PR #145942 which enables the use of ACL directly in ATen.\r\nRelative performance improvement using OMP_NUM_THREADS=1 is ~15x, using OMP_NUM_THREADS=32 it\u2019s ~5.4x.\r\n\r\nScript to benchmark quantised add performance:\r\n```\r\nimport torch\r\nimport torch.profiler as profiler\r\n\r\na_f32 = torch.rand((400, 3456),dtype=torch.float)\r\nb_f32 = torch.rand((400, 3456),dtype=torch.float)\r\na_q = torch.quantize_per_tensor(a_f32, 1.2, 0, torch.qint8)\r\nb_q = torch.quantize_per_tensor(b_f32, 1.7, 5, torch.qint8)\r\n\r\nwith profiler.profile(with_stack=True, profile_memory=False, record_shapes=True) as prof:\r\n    for i in range(1000):     \r\n        _ = torch.ops.quantized.add(a_q, b_q, 1.3, 2)\r\nprint(prof.key_averages(group_by_input_shape=True).table(sort_by='self_cpu_time_total', row_limit=50))\r\n```\n\ncc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10",
        "open",
        "2025-02-06T16:54:08Z",
        null,
        null,
        "module: cpu, triaged, open source, release notes: quantization, release notes: releng",
        "main",
        "acl_qadd",
        10,
        577,
        15,
        3,
        2,
        1,
        "malfet, malfet",
        "COMMENTED, COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146620"
    ],
    [
        146617,
        "Generate test reports for pytest when option is given",
        "The argument needs to be appended when test reports should be generated. `IS_CI` is not necessarily set, so rather check `TEST_SAVE_XML` instead as in other places where test reports are conditionally enabled.\r\n\r\nSee also https://github.com/pytorch/pytorch/issues/126523",
        "open",
        "2025-02-06T16:12:06Z",
        null,
        null,
        "triaged, open source, topic: not user facing",
        "main",
        "Flamefire-patch-1",
        1,
        2,
        1,
        1,
        1,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146617"
    ],
    [
        146616,
        "[don't merge] test baseline",
        "Fixes #ISSUE_NUMBER\r\n",
        "open",
        "2025-02-06T16:04:22Z",
        null,
        null,
        "open source, topic: not user facing, ciflow/binaries_wheel, ciflow/xpu",
        "main",
        "test_main",
        1,
        1,
        0,
        1,
        7,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146616"
    ],
    [
        146615,
        "[BE][Ez]: Enable specific ruff rules to prevent antipatterns and bugs",
        "Enables a few ruff rules\r\n* Ban print statements within asserts (likely bugs)\r\n* ~Use string for Decimal literal to prevent loss of precision~ \r\n* ~Do not use default args for __post__init__ in dataclasses, they likely were meant to go into the factory method, the __init__, or somewhere else. The default values are useless here.~\r\n\r\nWait until ruff upgrade for the last 2\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-02-06T16:04:20Z",
        null,
        null,
        "triaged, open source, better-engineering, topic: not user facing, module: dynamo, ciflow/inductor",
        "main",
        "skylion007/enable-RUF-2025-02-06",
        2,
        4,
        3,
        3,
        1,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146615"
    ],
    [
        146614,
        "[CD] Add python 3.13t build for xpu",
        "Fixes #146451\r\n",
        "open",
        "2025-02-06T15:55:52Z",
        null,
        null,
        "open source, ciflow/trunk, topic: not user facing, ciflow/binaries_wheel",
        "main",
        "xpu_py_3_13t",
        3,
        339,
        2,
        1,
        5,
        1,
        "atalman",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146614"
    ],
    [
        146612,
        "[WIP] BaseSubclass",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146612\n\n",
        "open",
        "2025-02-06T15:33:23Z",
        null,
        null,
        "",
        "gh/IvanKobzarev/100/base",
        "gh/IvanKobzarev/100/head",
        3,
        98,
        17,
        1,
        2,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146612"
    ],
    [
        146604,
        "[Profiler] Enable CUPTI teardown to reduce profiler overhead",
        "The problem is that the profiler slowed down\r\ntraining by roughly 10-20% even after completion\r\nbecause cuptiFinalize was not called in Kineto due to TEARDOWN_CUPTI=0. Disabling CUPTI teardown was a workaround for crashes which occured when CUDA graphs were used. This issue was fixed in CUDA 12.6. Also there is no point in disabling CUPTI teardown if CUDA Graphs are not used.\r\n\r\nFixes #144455 \r\n\n\ncc @robieta @chaekit @guotuofeng @guyang3532 @dzhulgakov @davidberard98 @briancoutinho @sraikund16 @sanrise",
        "open",
        "2025-02-06T13:43:37Z",
        null,
        null,
        "triaged, open source, oncall: profiler, topic: not user facing",
        "main",
        "fix/144455_teardown_cupti",
        2,
        22,
        4,
        4,
        8,
        1,
        "sraikund16, sraikund16, davidberard98, mgmtea, mgmtea, mgmtea, mgmtea",
        "APPROVED, APPROVED, CHANGES_REQUESTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146604"
    ],
    [
        146597,
        "Fix bug in _ref_test_helper in test_ops and fix failing test on Aarch64",
        "We have a failing unit test on Aarch64\r\n\r\n```\r\nException: Caused by reference input at index 34: SampleInput(input=Tensor[size=(5, 5, 4), device=\"cpu\", dtype=torch.complex64, contiguous=False], args=(), kwargs={}, broadcasts_input=False, name='')\r\n\r\nTo execute this test, run the following from the base repo dir:\r\n    PYTORCH_OPINFO_SAMPLE_INPUT_INDEX=34 python test/test_ops.py TestCommonCPU.test_python_ref__refs_square_cpu_complex64\r\n\r\nThis message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0\r\n```\r\n\r\nAfter debugging it I found that `ex` variable is not being reset to None on each loop inside _ref_test_helper. Which after fixing, highlighted another expectedFailure to reenable - `nn.functional.hinge_embedding_loss` which was incorrectly being skipped due to the same problem.\r\n\r\nhttps://github.com/pytorch/pytorch/blob/4a545eb85d6ba06079787a83f8ab1a8c8f67c76f/test/test_ops.py#L546\r\nex variable is not reset after this for next loop iteration",
        "open",
        "2025-02-06T11:07:07Z",
        null,
        null,
        "triaged, open source, topic: not user facing",
        "main",
        "test_fix",
        3,
        3,
        11,
        1,
        3,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146597"
    ],
    [
        146596,
        "separate f16 vectorized class from bf16",
        "This refactoring is required as part of https://github.com/pytorch/pytorch/pull/143666\n\ncc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @malfet @snadampal @milpuz01",
        "open",
        "2025-02-06T10:58:15Z",
        null,
        null,
        "module: cpu, open source, module: arm, topic: not user facing",
        "main",
        "refactor",
        4,
        1031,
        404,
        1,
        1,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146596"
    ],
    [
        146595,
        "skip test_torch_dynamo_codegen_pow if CPU backend is not cpp",
        "The test asserts that `aten.pow` is not present in the generated kernel code. When using a CPU backend other than cpp, the kernel contains comments referencing the aten ops that produced the kernel in this case `aten.pow`. \r\n\r\nThis PR skips that test case if the CPU backend is not cpp.\r\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-02-06T10:46:32Z",
        null,
        null,
        "triaged, open source, topic: not user facing, module: dynamo",
        "main",
        "skip-if-cpu-backend-not-cpp",
        1,
        4,
        0,
        1,
        1,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146595"
    ],
    [
        146593,
        "[NOT FOR LANDING] experimental NVSHMEM integration",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146593\n* #146592\n\n\n\ncc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o",
        "open",
        "2025-02-06T10:26:18Z",
        null,
        null,
        "oncall: distributed, release notes: distributed (c10d)",
        "gh/yifuwang/195/base",
        "gh/yifuwang/195/head",
        7,
        471,
        4,
        2,
        1,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146593"
    ],
    [
        146592,
        "clang-format CUDASymmetricMemory.cu",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #146593\n* __->__ #146592\n\n\n\ncc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o",
        "open",
        "2025-02-06T10:26:14Z",
        null,
        null,
        "oncall: distributed, release notes: distributed (c10d)",
        "gh/yifuwang/194/base",
        "gh/yifuwang/194/head",
        1,
        20,
        10,
        1,
        2,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146592"
    ],
    [
        146589,
        "[DDP] Use NCCL allocated memory for gradient bucket",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146589\n\r\nSo that NVLink SHARP comes with zero-copy on H100+ platforms, for DDP applications.\r\nLess SM usage, less memory contention between NCCL kernel and compute kernels.\r\n\r\nAdded env `DDP_DISABLE_COMM_MEM` as a back-out option:\r\n```\r\nAn environment variable to disable comm-optimized memory pool.\r\nDefault is 0, which means comm-optimized memory pool is enabled.\r\nUsers can set it to 1 in case of seeing regression or OOM (because this\r\ncomm MemPool may not share space with regular compute MemPool).\r\n```\r\n\r\ncc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o\n\nDifferential Revision: [D69297766](https://our.internmc.facebook.com/intern/diff/D69297766)",
        "open",
        "2025-02-06T09:01:24Z",
        null,
        null,
        "oncall: distributed, ciflow/trunk, release notes: distributed (c10d), release notes: distributed (ddp), merging",
        "gh/kwen2501/123/base",
        "gh/kwen2501/123/head",
        7,
        127,
        13,
        6,
        14,
        2,
        "Skylion007, Skylion007, Skylion007, Skylion007, kwen2501, Skylion007, kwen2501, syed-ahmed, syed-ahmed, syed-ahmed, syed-ahmed, syed-ahmed, syed-ahmed, yifuwang, kwen2501, yifuwang, fegin, syed-ahmed, kwen2501, c-p-i-o, c-p-i-o, c-p-i-o, c-p-i-o, fduwjj, fduwjj",
        "COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, CHANGES_REQUESTED, APPROVED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, APPROVED, COMMENTED, APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146589"
    ],
    [
        146587,
        "[Dynamo] Allow dynamo to handle `str.xxx()`",
        "\r\nFixes #146350\r\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-02-06T08:47:14Z",
        null,
        null,
        "open source, topic: not user facing, module: dynamo",
        "main",
        "fix/dynamo/str",
        2,
        13,
        0,
        2,
        10,
        1,
        "zou3519, shink, shink, zou3519",
        "COMMENTED, COMMENTED, COMMENTED, APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146587"
    ],
    [
        146583,
        "[symbolic shapes] Log symnode id",
        "We want to log the symnode id which will help us with provenance tracking between expressions created.\r\n\r\n\r\ncc @ezyang @SherlockNoMad @EikanWang @jgong5 @wenzhe-nrv",
        "open",
        "2025-02-06T07:45:13Z",
        null,
        null,
        "ciflow/trunk, release notes: fx, fx, ciflow/inductor, merging",
        "main",
        "angelayi/test",
        1,
        26,
        7,
        1,
        7,
        1,
        "bobrenjc93",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146583"
    ],
    [
        146582,
        "[Partitioner] Reduce time consuming of partitions merger",
        "This patch optimize maybe_merge_partition func through 3-ways:\r\n\r\nRemove unnecessary copy https://github.com/pytorch/pytorch/blob/main/torch/fx/passes/infra/partitioner.py#L99. The number of copied nodes is large if we can merge all of the nodes of graph into one partition.\r\nRecord users of each partition to avoid duplicate iteration over nodes https://github.com/pytorch/pytorch/blob/main/torch/fx/passes/infra/partitioner.py#L133. The trip count of this loop maybe very large.\r\nThe nodes number of each partitions maybe not balance https://github.com/pytorch/pytorch/blob/main/torch/fx/passes/infra/partitioner.py#L145. We always encounter one issue: one partition has n nodes, but the other has one node. Merge the smaller partition into the larger can help to reduce time consuming.\n\ncc @ezyang @SherlockNoMad @EikanWang @jgong5 @wenzhe-nrv",
        "open",
        "2025-02-06T07:35:55Z",
        null,
        null,
        "triaged, open source, release notes: fx, topic: not user facing, fx",
        "main",
        "lingzhiz/optimize_partition_merger",
        1,
        39,
        26,
        3,
        2,
        0,
        "Skylion007",
        "COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146582"
    ],
    [
        146581,
        "Clarify that compile(module) only affects the forward method",
        "Fixes #141616\r\n\r\n## Changes\r\n\r\n- Add `Note` to Clarify how compile works with `nn.Module`\r\n- Optimize plain url address with clickable description\r\n\r\n## Test Result\r\n\r\n### Before\r\n\r\n![image](https://github.com/user-attachments/assets/15ff9985-7e91-4d71-be7d-cdd38eacd3f9)\r\n![image](https://github.com/user-attachments/assets/26e27ba4-52da-4336-b72d-a0f9d0ebe839)\r\n\r\n\r\n### After\r\n\r\n![image](https://github.com/user-attachments/assets/5eaa8421-19e8-4186-af3d-dab4323d2c95)\r\n![image](https://github.com/user-attachments/assets/a96a8a79-6320-4748-931f-33f4dbc640eb)\r\n\r\n",
        "open",
        "2025-02-06T07:34:49Z",
        null,
        null,
        "triaged, open source, topic: not user facing",
        "main",
        "opt/docs/compile",
        1,
        8,
        2,
        1,
        2,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146581"
    ],
    [
        146580,
        "[Partitioner] Remove unnecessary upstream nodes in dependency viewer",
        "We iterate upstream nodes to update partition map. But actually did nothing due to we iterate nodes with reversed topological order https://github.com/pytorch/pytorch/pull/136608/files#diff-f2f9dd3903fd99955732eb694941fea0cb7301a58d59554787f3311d417e5615L193 so that there exists no upstream nodes in assignment. Remove it to reduce for-loop overhead which up to O(N * N) complexity.\r\n\n\ncc @ezyang @SherlockNoMad @EikanWang @jgong5 @wenzhe-nrv",
        "open",
        "2025-02-06T07:32:01Z",
        null,
        null,
        "triaged, open source, release notes: fx, topic: not user facing, fx",
        "main",
        "lingzhiz/remove_upstream_nodes",
        1,
        0,
        19,
        1,
        2,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146580"
    ],
    [
        146578,
        "add `torch.float4_e2m1fn_x2` to PyTorch",
        "Summary:\r\n\r\nAdds the `torch.float4_e2m1fn_x2` dtype to PyTorch, as detailed in\r\nhttps://github.com/pytorch/pytorch/issues/146414 . Please see the issue for a detailed definition of the format.  \r\n\r\nNote that I decided to keep the casts out of this to significantly simplify the code, as defining casting between packed and unpacked formats will be tricky using the existing casting machinery.  \r\n\r\nExample of basic functionality:\r\n\r\n```python\r\nimport torch\r\n\r\n# creation with empty\r\nx0 = torch.empty(4, 4, dtype=torch.float4_e2m1fn_x2)\r\n\r\n# printing, prints the uint8 representation of the stored values\r\nprint(x0)\r\n\r\n# view as other dtype\r\nx0.view(torch.uint8).view(torch.float4_e2m1fn_x2)\r\n```\r\n\r\nDone in this PR:\r\n* tensor creation and tensor printing works (no other ops defined)\r\n\r\nFor future PRs:\r\n* torch._scaled_mm\r\n* PT2\r\n* various cleanups (detailed in comments with issue numbers)\r\n\r\nTest Plan:\r\n\r\n```\r\npytest test/quantization/core/experimental/test_floatx.py -s\r\n```\r\n\r\nReviewers:\r\n\r\nSubscribers:\r\n\r\nTasks:\r\n\r\nTags:\r\n\r\ncc @yanbing-j @albanD @kadeng @penguinwu @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10",
        "open",
        "2025-02-06T07:08:42Z",
        null,
        null,
        "release notes: quantization",
        "gh/vkuzo/1/head",
        "gh/vkuzo/2/head",
        7,
        70,
        5,
        3,
        2,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146578"
    ],
    [
        146574,
        "[ROCm][TunableOp] Offline results are saved to file when offline tuning is disabled.",
        "This PR is to fix UT breakage that has been reported internally and is considered high priority. When `tunable.record_untuned_enable(False)` is invoked, we flush the results of the untuned gemm file.\r\n\r\nOffline tuning I/O currently doesn't have a set untuned results filename member function or untuned results write to file member function. When performing back-to-back unit tests, the same ofstream ends up getting reused between UTs. Due to the way the UT are executed, this can lead to unexpected failures.\r\n\r\ncc: @jfactory07 \r\n\r\n\r\n\r\ncc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang",
        "open",
        "2025-02-06T06:04:28Z",
        null,
        null,
        "module: rocm, open source, topic: not user facing, ciflow/rocm",
        "main",
        "fix_tunableop_untuned_fileio",
        1,
        2,
        0,
        1,
        4,
        0,
        "jeffdaily",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146574"
    ],
    [
        146573,
        "add python root bin to windows load path.",
        "This PR is extend python root bin path to dll load list. \r\nIt makes PyTorch robust and compatible to more dependency libraries, such as `intel-pti`.\r\n\r\ncc @peterjc123 @mszhanyi @skyline75489 @nbcsm @iremyux @Blackhex @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10",
        "open",
        "2025-02-06T05:58:55Z",
        null,
        null,
        "module: windows, open source, ciflow/trunk, topic: not user facing, intel",
        "main",
        "xu_add_init_path",
        1,
        8,
        1,
        1,
        1,
        0,
        "EikanWang",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146573"
    ],
    [
        146571,
        "[Dynamo][autograd.Function] Relax backward speculation strict mode a bit",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #146742\n* #146741\n* __->__ #146571\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-02-06T05:05:51Z",
        null,
        null,
        "topic: not user facing, module: dynamo, ciflow/inductor",
        "gh/yanboliang/63/base",
        "gh/yanboliang/63/head",
        7,
        228,
        44,
        5,
        1,
        0,
        "yanboliang, zou3519, zou3519, zou3519, yanboliang, zou3519, zou3519",
        "COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146571"
    ],
    [
        146562,
        "WIP hacky reordering pass",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146562\n* #146561\n* #146560\n* #146559\n* #146558\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @desertfire @chauhang @aakhundov",
        "open",
        "2025-02-06T01:41:51Z",
        null,
        null,
        "module: inductor, ciflow/inductor",
        "gh/wconstab/396/base",
        "gh/wconstab/396/head",
        3,
        325,
        24,
        1,
        2,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146562"
    ],
    [
        146561,
        "Improve comms debug visualization",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #146562\n* __->__ #146561\n* #146560\n* #146559\n* #146558\n\n",
        "open",
        "2025-02-06T01:41:46Z",
        null,
        null,
        "module: inductor, ciflow/inductor",
        "gh/wconstab/395/base",
        "gh/wconstab/395/head",
        1,
        2,
        3,
        1,
        2,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146561"
    ],
    [
        146560,
        "enable reorder",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #146562\n* #146561\n* __->__ #146560\n* #146559\n* #146558\n\n",
        "open",
        "2025-02-06T01:41:41Z",
        null,
        null,
        "module: inductor, ciflow/inductor",
        "gh/wconstab/394/base",
        "gh/wconstab/394/head",
        1,
        1,
        1,
        1,
        2,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146560"
    ],
    [
        146559,
        "Apply changes from https://github.com/pytorch/pytorch/commit/211847de3c1c3d6cbd299e14a001b794eabf2a2d",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #146562\n* #146561\n* #146560\n* __->__ #146559\n* #146558\n\n",
        "open",
        "2025-02-06T01:41:36Z",
        null,
        null,
        "oncall: distributed, ciflow/inductor",
        "gh/wconstab/393/base",
        "gh/wconstab/393/head",
        1,
        65,
        15,
        1,
        2,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146559"
    ],
    [
        146558,
        "[dtensor] support mixed precision for redistribute (#20)",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #146562\n* #146561\n* #146560\n* #146559\n* __->__ #146558\n\n",
        "open",
        "2025-02-06T01:41:31Z",
        null,
        null,
        "oncall: distributed, ciflow/inductor",
        "gh/wconstab/392/base",
        "gh/wconstab/392/head",
        2,
        52,
        15,
        1,
        3,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146558"
    ],
    [
        146557,
        "Add fqn_modifier at loading_state_dict and unit test",
        "In Fusion model, users might change the state_dict keys by state_dict_hook\r\nThe load_state_dict APIs here won't call model.state_dict() so that the hooks won't be called to change the keys, causing the mismatch between fqn and state_dict keys.\r\n\r\nThe PR here suggests users to add how they would change the state_dict key prefix (they can name it, here we call \"fqn_modifiers\") by default\r\nDuring loading state_dict, we have the prefix change during getting fqn so that they can be processed same as through state_dict hook.\r\n\r\nFor example:\r\nThere's a state_dict_hook:\r\n\r\n```\r\ndef _state_dict_hook(self, destination, prefix, keep_vars):\r\n    \"\"\"Remove \"embedding\" from the original embedding in the state_dict\r\n    name. This keeps the orginal state dict name for the embedding\r\n    from before fusing with the FusionEmbedding.\r\n\r\n    [!Note] This update changes the order of the OrderedDict\r\n    \"\"\"\r\n    key = prefix + \"embedding.weight\"\r\n    new_key = prefix + \"weight\"\r\n    destination[new_key] = destination[key]\r\n    del destination[key]\r\n```\r\n\r\nIn the dsd after this PR, we would skip \"embedding.\" before \"weight\" if find the \"fqn_modifiers\" attribute at that module\r\n```\r\ndef fqn_modifiers(self) -> Dict[str, str]:\r\n    return {\r\n        \"weight\": \"embedding\",\r\n    }\r\n```\n\ncc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o @LucasLLC @MeetVadakkanchery @mhorowitz @pradeepfn @ekr0",
        "open",
        "2025-02-06T01:16:54Z",
        null,
        null,
        "oncall: distributed, topic: not user facing, module: distributed_checkpoint",
        "main",
        "dsd_fqn_modifiers",
        3,
        97,
        7,
        4,
        1,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146557"
    ],
    [
        146545,
        "Update test.sh to run a greater set of unit tests on aarch64",
        "expanded set of unit tests that run  on aarch64 to be the entire set of tests that can be run by run_test.py\r\n\r\n\n\ncc @seemethere @malfet @pytorch/pytorch-dev-infra",
        "open",
        "2025-02-05T23:41:27Z",
        null,
        null,
        "module: ci, triaged, open source, topic: not user facing",
        "main",
        "expanded_unit_tests",
        1,
        1,
        22,
        2,
        3,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146545"
    ],
    [
        146543,
        "Update local_timer.py to improve queue handling",
        "- Switched from `multiprocessing.Queue` to `torch.multiprocessing.Queue`\r\n- Wrapped `qsize()` in `try-except` to prevent `NotImplementedError`\n\ncc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o",
        "open",
        "2025-02-05T23:33:01Z",
        null,
        null,
        "oncall: distributed, triaged, open source, release notes: distributed (torchelastic)",
        "main",
        "update-local-timer",
        1,
        12,
        9,
        1,
        2,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146543"
    ],
    [
        146540,
        "[export] Draft export custom streamer",
        "* Instead of using tlparse's StreamHandler, draft-export will use its own, which will capture the logs, filter them, and only output the relevant ones to the log file. \r\n* To do this, the CaptureStructuredTrace logger will use a `LogRecord` which is basically a dictionary with a custom hash function based on what is being logged. This allows us to deduplicate logs which represent the same thing, such as:\r\n  * \"missing_fake_kernel\" logs with the same operator\r\n  * \"mismatched_fake_kernel\" logs with the same operator and reasoning\r\n  * \"propagate_real_tensor\", \"create_unbacked_symbol\", and \"guard_added\" logs occurring on lines with the same stacktrace",
        "open",
        "2025-02-05T23:24:25Z",
        null,
        null,
        "release notes: export",
        "main",
        "angelayi/draft_logger",
        2,
        89,
        48,
        1,
        1,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146540"
    ],
    [
        146537,
        "[WIP] Log graph breaks",
        "Graph breaks currently aren't logged. We want to log them. Need to test before merging.\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @desertfire @chauhang @aakhundov",
        "open",
        "2025-02-05T23:08:22Z",
        null,
        null,
        "topic: not user facing, module: inductor, module: dynamo, ciflow/inductor, dynamo-logging",
        "main",
        "gh/raymo/log-graph-breaks",
        4,
        64,
        7,
        1,
        1,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146537"
    ],
    [
        146535,
        "[wip] disable decorator for ca",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146535\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @yf225",
        "open",
        "2025-02-05T22:52:10Z",
        null,
        null,
        "module: dynamo, ciflow/inductor, module: compiled autograd",
        "gh/xmfan/174/base",
        "gh/xmfan/174/head",
        9,
        48,
        0,
        2,
        2,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146535"
    ],
    [
        146532,
        "[symbolic shapes] Log SymNode id for provenance",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* (to be filled)\n\n\n\ncc @ezyang @SherlockNoMad @EikanWang @jgong5 @wenzhe-nrv",
        "open",
        "2025-02-05T22:47:15Z",
        null,
        null,
        "release notes: fx, fx, ciflow/inductor",
        "gh/angelayi/66/base",
        "gh/angelayi/66/head",
        3,
        148,
        92,
        1,
        1,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146532"
    ],
    [
        146530,
        "[aoti] Fix FakeTensorMode not detected in aot_compile when there's no input",
        "Summary:\r\nFixes https://github.com/pytorch/pytorch/issues/118304\r\n\r\nWhen we don't have inputs, we should still try to get a FakeTensorMode because there can be unbacked symints in the graph.\r\n\r\nSo we get the FakeTensorMode once when entering compile_fx, and then using the that FakeTensorMode for the rest of the lowering.\r\n\r\nTest Plan:\r\n```\r\nbuck run fbcode//mode/dev-nosan //caffe2/test/inductor:test_aot_inductor -- -r unbacked_arg\r\n```\r\n\r\nDifferential Revision: D69158049\r\n\r\n\r\n\r\n\r\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @desertfire @chauhang @aakhundov",
        "open",
        "2025-02-05T22:25:56Z",
        null,
        null,
        "fb-exported, ciflow/trunk, module: inductor, ciflow/inductor, release notes: AO frontend",
        "main",
        "export-D69158049",
        3,
        60,
        12,
        1,
        8,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146530"
    ],
    [
        146526,
        "[inductor] add units to estimated runtime log",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146526\n* #146513\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @desertfire @chauhang @aakhundov",
        "open",
        "2025-02-05T21:57:04Z",
        null,
        null,
        "module: inductor, ciflow/inductor",
        "gh/xmfan/173/base",
        "gh/xmfan/173/head",
        1,
        2,
        2,
        1,
        2,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146526"
    ],
    [
        146525,
        "[dynamo] improved graph break messages for some common graph break sites",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146525\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-02-05T21:47:17Z",
        null,
        null,
        "module: dynamo, ciflow/inductor, release notes: dynamo, module: compile ux",
        "gh/williamwen42/205/base",
        "gh/williamwen42/205/head",
        14,
        517,
        76,
        2,
        1,
        0,
        "zou3519, zou3519, zou3519, zou3519, zou3519, zou3519, jansel, bobrenjc93",
        "COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146525"
    ],
    [
        146520,
        "CUDA CachingHostAllocator tracks registrations to call correct free",
        "Users may change the allocator config at will. torch unit tests do this. However, allocations using cudaHostRegister should use corresonding cudaHostUnregister and similarly for cudaHostAlloc / cudaFreeHost.",
        "open",
        "2025-02-05T21:19:31Z",
        null,
        null,
        "triaged, open source, topic: not user facing",
        "main",
        "cuda_host_allocator_free",
        1,
        15,
        6,
        1,
        2,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146520"
    ],
    [
        146514,
        "[DTensor][Test] Create a simple unit test for tensordot",
        "Fixes #ISSUE_NUMBER\r\n\r\nThe dims and shape of the tensors are from a specific Shampoo use case. We want to create a unit test for it to make sure there are no regressions for this.\n\ncc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wconstab @d4l3k @c-p-i-o",
        "open",
        "2025-02-05T19:35:29Z",
        null,
        null,
        "oncall: distributed, Merged, Reverted, ciflow/trunk, topic: not user facing, merging, ci-no-td",
        "main",
        "tensordot",
        1,
        23,
        0,
        2,
        13,
        1,
        "tianyu-l",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146514"
    ],
    [
        146513,
        "[dynamo] check for incompatible configs",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146513\n\r\ninternal: https://fb.workplace.com/groups/1075192433118967/permalink/1599802033991335/\r\n\r\nAssuming flags don't change during compilation, we shouldn't allow incompatible configs to be set at torch.compile wrap time.\r\n\r\nNot in this PR: For flags that need to change during compilation, we'd have to be strict about where they can be used in the compile lifecycle\r\n\r\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @desertfire @chauhang @aakhundov",
        "open",
        "2025-02-05T19:09:40Z",
        null,
        null,
        "Merged, Reverted, ciflow/trunk, module: inductor, module: dynamo, ciflow/inductor, release notes: dynamo, merging, ci-no-td",
        "gh/xmfan/172/base",
        "gh/xmfan/172/head",
        3,
        31,
        0,
        4,
        13,
        0,
        "williamwen42",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146513"
    ],
    [
        146512,
        "Fix `DispatchStub.cpp` compilation for gcc 14",
        "Otherwise I get the following error:\r\n\r\n```bash\r\n\r\n.../intel-xpu-backend-for-triton/pytorch/aten/src/ATen/native/DispatchStub.cpp:152:18: error: no matching function for call to \u2018find(std::array<c10::DeviceType, 7>::const_iterator, std::array<c10::DeviceType, 7>::const_iterator, const c10::DeviceType&)\u2019\r\n  152 |     if (std::find(supported_devices.begin(), supported_devices.end(), device_type) == supported_devices.end()) {\r\n      |         ~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nIn file included from /usr/include/c++/14/bits/locale_facets.h:48,\r\n                 from /usr/include/c++/14/bits/basic_ios.h:37,\r\n                 from /usr/include/c++/14/ios:46,\r\n                 from /usr/include/c++/14/ostream:40,\r\n                 from .../intel-xpu-backend-for-triton/pytorch/c10/core/DeviceType.h:13,\r\n                 from .../intel-xpu-backend-for-triton/pytorch/aten/src/ATen/native/DispatchStub.h:3,\r\n                 from .../intel-xpu-backend-for-triton/pytorch/aten/src/ATen/native/DispatchStub.cpp:2:\r\n/usr/include/c++/14/bits/streambuf_iterator.h:435:5: note: candidate: \u2018template<class _CharT2> typename __gnu_cxx::__enable_if<std::__is_char<_CharT2>::__value, std::istreambuf_iterator<_CharT, std::char_traits<_CharT> > >::__type std::find(istreambuf_iterator<_CharT, char_traits<_CharT> >, istreambuf_iterator<_CharT, char_traits<_CharT> >, const _CharT2&)\u2019\r\n  435 |     find(istreambuf_iterator<_CharT> __first,\r\n      |     ^~~~\r\n/usr/include/c++/14/bits/streambuf_iterator.h:435:5: note:   template argument deduction/substitution failed:\r\n.../intel-xpu-backend-for-triton/pytorch/aten/src/ATen/native/DispatchStub.cpp:152:18: note:   mismatched types \u2018std::istreambuf_iterator<_CharT, std::char_traits<_CharT> >\u2019 and \u2018const std::array<c10::DeviceType, 7>::value_type*\u2019 {aka \u2018const c10::DeviceType*\u2019}\r\n  152 |     if (std::find(supported_devices.begin(), supported_devices.end(), device_type) == supported_devices.end()) {\r\n      |         ~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n```\r\n",
        "open",
        "2025-02-05T18:51:17Z",
        null,
        null,
        "triaged, open source, module: dispatch, topic: not user facing",
        "main",
        "patch-2",
        1,
        1,
        0,
        1,
        3,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146512"
    ],
    [
        146510,
        "[ONNX] Bump torchlib opset to 22",
        "Fixes #ISSUE_NUMBER\r\n",
        "open",
        "2025-02-05T18:47:25Z",
        null,
        null,
        "open source, release notes: onnx",
        "main",
        "justinchu/torchlib-opset22",
        30,
        20652,
        436,
        30,
        1,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146510"
    ],
    [
        146509,
        "[BE][CI][Easy] bump `ruff` to 0.9.0: long statements in docstrings",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #145606\n* #144546\n* #144569\n* #145148\n* __->__ #146509\n\n\n\ncc @ezyang @SherlockNoMad @EikanWang @jgong5 @wenzhe-nrv",
        "open",
        "2025-02-05T18:43:13Z",
        null,
        null,
        "open source, ciflow/trunk, release notes: onnx, topic: not user facing, fx",
        "gh/XuehaiPan/240/base",
        "gh/XuehaiPan/240/head",
        4,
        26,
        6,
        4,
        4,
        0,
        "justinchuby",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146509"
    ],
    [
        146506,
        "Support contextlib.ExitStack",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146506\n* #146505\n* #146503\n* #146502\n* #146501\n* #146500\n* #146504\n* #146499\n* #146498\n* #146497\n* #146496\n* #146493\n* #146492\n* #146491\n* #146495\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-02-05T17:22:12Z",
        null,
        null,
        "open source, module: dynamo, ciflow/inductor",
        "gh/guilhermeleobas/107/base",
        "gh/guilhermeleobas/107/head",
        2,
        621,
        9,
        5,
        2,
        0,
        "guilhermeleobas",
        "COMMENTED",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146506"
    ],
    [
        146505,
        "Allow setting attribute to NestedUserFunctionVariable",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #146506\n* __->__ #146505\n* #146503\n* #146502\n* #146501\n* #146500\n* #146504\n* #146499\n* #146498\n* #146497\n* #146496\n* #146493\n* #146492\n* #146491\n* #146495\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-02-05T17:22:05Z",
        null,
        null,
        "open source, module: dynamo, ciflow/inductor",
        "gh/guilhermeleobas/106/base",
        "gh/guilhermeleobas/106/head",
        2,
        21,
        1,
        5,
        2,
        0,
        "anijain2305",
        "COMMENTED",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146505"
    ],
    [
        146504,
        "Introduce `UserDefinedExceptionClassVariable`",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #146506\n* #146505\n* #146503\n* #146502\n* #146501\n* #146500\n* __->__ #146504\n* #146499\n* #146498\n* #146497\n* #146496\n* #146493\n* #146492\n* #146491\n* #146495\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-02-05T17:21:58Z",
        null,
        null,
        "open source, module: dynamo, ciflow/inductor",
        "gh/guilhermeleobas/105/base",
        "gh/guilhermeleobas/105/head",
        5,
        45,
        5,
        5,
        2,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146504"
    ],
    [
        146503,
        "Create new dynamo ObservedExceptions at runtime",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #146506\n* #146505\n* __->__ #146503\n* #146502\n* #146501\n* #146500\n* #146504\n* #146499\n* #146498\n* #146497\n* #146496\n* #146493\n* #146492\n* #146491\n* #146495\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-02-05T17:21:52Z",
        null,
        null,
        "open source, module: dynamo, ciflow/inductor",
        "gh/guilhermeleobas/104/base",
        "gh/guilhermeleobas/104/head",
        3,
        18,
        1,
        5,
        2,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146503"
    ],
    [
        146502,
        "Correctly propagate exception to parent tx",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #146506\n* #146505\n* #146503\n* __->__ #146502\n* #146501\n* #146500\n* #146504\n* #146499\n* #146498\n* #146497\n* #146496\n* #146493\n* #146492\n* #146491\n* #146495\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-02-05T17:21:46Z",
        null,
        null,
        "open source, module: dynamo, ciflow/inductor",
        "gh/guilhermeleobas/103/base",
        "gh/guilhermeleobas/103/head",
        2,
        92,
        3,
        5,
        2,
        0,
        "anijain2305",
        "APPROVED",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146502"
    ],
    [
        146501,
        "Update CPython tests for ctx manager to use unittest",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #146506\n* #146505\n* #146503\n* #146502\n* __->__ #146501\n* #146500\n* #146504\n* #146499\n* #146498\n* #146497\n* #146496\n* #146493\n* #146492\n* #146491\n* #146495\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-02-05T17:21:40Z",
        null,
        null,
        "open source, topic: not user facing, module: dynamo, ciflow/inductor",
        "gh/guilhermeleobas/102/base",
        "gh/guilhermeleobas/102/head",
        1,
        206,
        211,
        5,
        1,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146501"
    ],
    [
        146500,
        "Allow trace through unittest",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #146506\n* #146505\n* #146503\n* #146502\n* #146501\n* __->__ #146500\n* #146504\n* #146499\n* #146498\n* #146497\n* #146496\n* #146493\n* #146492\n* #146491\n* #146495\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-02-05T17:21:34Z",
        null,
        null,
        "open source, module: dynamo, ciflow/inductor, release notes: dynamo",
        "gh/guilhermeleobas/101/base",
        "gh/guilhermeleobas/101/head",
        6,
        623,
        14,
        5,
        1,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146500"
    ],
    [
        146499,
        "Add `__context/cause/suppress_context/traceback__` to Exception",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #146506\n* #146505\n* #146503\n* #146502\n* #146501\n* #146500\n* #146504\n* __->__ #146499\n* #146498\n* #146497\n* #146496\n* #146493\n* #146492\n* #146491\n* #146495\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-02-05T17:21:27Z",
        null,
        null,
        "open source, module: dynamo, ciflow/inductor, release notes: dynamo",
        "gh/guilhermeleobas/100/base",
        "gh/guilhermeleobas/100/head",
        5,
        361,
        13,
        5,
        1,
        0,
        "guilhermeleobas, guilhermeleobas",
        "COMMENTED, COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146499"
    ],
    [
        146498,
        "Add `sys.exc_info` and `sys.exception`",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #146506\n* #146505\n* #146503\n* #146502\n* #146501\n* #146500\n* #146504\n* #146499\n* __->__ #146498\n* #146497\n* #146496\n* #146493\n* #146492\n* #146491\n* #146495\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-02-05T17:21:20Z",
        null,
        null,
        "open source, module: dynamo, ciflow/inductor, release notes: dynamo",
        "gh/guilhermeleobas/99/base",
        "gh/guilhermeleobas/99/head",
        3,
        162,
        0,
        5,
        1,
        1,
        "guilhermeleobas, anijain2305",
        "COMMENTED, APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146498"
    ],
    [
        146497,
        "Propagate `AttributeError` to user code in user_defined.py",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #146506\n* #146505\n* #146503\n* #146502\n* #146501\n* #146500\n* #146504\n* #146499\n* #146498\n* __->__ #146497\n* #146496\n* #146493\n* #146492\n* #146491\n* #146495\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-02-05T17:21:13Z",
        null,
        null,
        "open source, topic: not user facing, module: dynamo, ciflow/inductor",
        "gh/guilhermeleobas/98/base",
        "gh/guilhermeleobas/98/head",
        3,
        38,
        1,
        5,
        1,
        1,
        "anijain2305, anijain2305",
        "COMMENTED, CHANGES_REQUESTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146497"
    ],
    [
        146496,
        "Handle `is`/`is not`",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #146506\n* #146505\n* #146503\n* #146502\n* #146501\n* #146500\n* #146504\n* #146499\n* #146498\n* #146497\n* __->__ #146496\n* #146493\n* #146492\n* #146491\n* #146495\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-02-05T17:21:06Z",
        null,
        null,
        "open source, module: dynamo, ciflow/inductor, release notes: dynamo",
        "gh/guilhermeleobas/97/base",
        "gh/guilhermeleobas/97/head",
        2,
        26,
        0,
        4,
        1,
        1,
        "anijain2305, anijain2305",
        "COMMENTED, CHANGES_REQUESTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146496"
    ],
    [
        146495,
        "Fix round(...) with constants",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #146506\n* #146505\n* #146503\n* #146502\n* #146501\n* #146500\n* #146504\n* #146499\n* #146498\n* #146497\n* #146496\n* #146493\n* #146492\n* #146491\n* __->__ #146495\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-02-05T17:20:59Z",
        null,
        null,
        "open source, ciflow/trunk, module: dynamo, ciflow/inductor, release notes: dynamo, merging",
        "gh/guilhermeleobas/96/base",
        "gh/guilhermeleobas/96/head",
        2,
        10,
        1,
        4,
        6,
        0,
        "anijain2305",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146495"
    ],
    [
        146494,
        "Fix STOPITERATION_ERROR opcode",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #146506\n* #146505\n* #146504\n* #146503\n* #146502\n* #146501\n* #146500\n* #146499\n* #146498\n* #146497\n* #146496\n* #146495\n* __->__ #146494\n* #146493\n* #146492\n* #146491\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-02-05T17:20:54Z",
        null,
        null,
        "open source, module: dynamo, ciflow/inductor, release notes: dynamo",
        "gh/guilhermeleobas/95/base",
        "gh/guilhermeleobas/95/head",
        1,
        5,
        4,
        2,
        1,
        1,
        "anijain2305, anijain2305",
        "COMMENTED, CHANGES_REQUESTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146494"
    ],
    [
        146493,
        "Add `RAISE_VARARGS 0`",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #146506\n* #146505\n* #146503\n* #146502\n* #146501\n* #146500\n* #146504\n* #146499\n* #146498\n* #146497\n* #146496\n* __->__ #146493\n* #146492\n* #146491\n* #146495\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-02-05T17:20:47Z",
        null,
        null,
        "open source, module: dynamo, ciflow/inductor, release notes: dynamo",
        "gh/guilhermeleobas/94/base",
        "gh/guilhermeleobas/94/head",
        2,
        27,
        1,
        4,
        1,
        0,
        "zou3519, guilhermeleobas",
        "COMMENTED, COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146493"
    ],
    [
        146492,
        "Add `WITH_EXCEPT_START` opcode",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #146506\n* #146505\n* #146503\n* #146502\n* #146501\n* #146500\n* #146504\n* #146499\n* #146498\n* #146497\n* #146496\n* #146493\n* __->__ #146492\n* #146491\n* #146495\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-02-05T17:20:40Z",
        null,
        null,
        "open source, module: dynamo, ciflow/inductor, release notes: dynamo",
        "gh/guilhermeleobas/93/base",
        "gh/guilhermeleobas/93/head",
        2,
        46,
        0,
        4,
        1,
        1,
        "zou3519, anijain2305",
        "COMMENTED, APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146492"
    ],
    [
        146491,
        "Add `make_dynamo_test`",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #146506\n* #146505\n* #146503\n* #146502\n* #146501\n* #146500\n* #146504\n* #146499\n* #146498\n* #146497\n* #146496\n* #146493\n* #146492\n* __->__ #146491\n* #146495\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-02-05T17:20:34Z",
        null,
        null,
        "open source, topic: not user facing, module: dynamo, ciflow/inductor",
        "gh/guilhermeleobas/92/base",
        "gh/guilhermeleobas/92/head",
        1,
        45,
        0,
        4,
        1,
        0,
        "zou3519, anijain2305, Skylion007, zou3519",
        "APPROVED, APPROVED, COMMENTED, COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146491"
    ],
    [
        146490,
        "[export] Serialize special values of float into strings for json.",
        "Summary: Currently inf is serialized as Infinity in JSON which is not standard compliant. Instead we will tweak all special floating points into strings and handle them at json layer.\n\nTest Plan:\nsee D69060784\nCI\n\nDifferential Revision: D69186425\n\n\n",
        "open",
        "2025-02-05T16:36:50Z",
        null,
        null,
        "fb-exported, ciflow/trunk, ciflow/inductor, release notes: export",
        "main",
        "export-D69186425",
        6,
        133,
        53,
        3,
        2,
        0,
        "yiming0416",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146490"
    ],
    [
        146489,
        "Update code_template.py re.compile() is directly applied to the regex\u2026",
        "\u2026 string inside the class variable\r\n\r\nre.compile() is directly applied to the regex string inside the class variable\r\n\r\nRegular expressions are very expensive computationally. So, this avoids any redundant compilation.\r\nFixes #ISSUE_NUMBER\r\n",
        "open",
        "2025-02-05T16:23:56Z",
        null,
        null,
        "open source, topic: not user facing",
        "main",
        "patch-1",
        1,
        3,
        2,
        1,
        2,
        0,
        "mikaylagawarecki",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146489"
    ],
    [
        146485,
        "Update quantile doc",
        "Fixes #146156\r\n",
        "open",
        "2025-02-05T15:33:15Z",
        null,
        null,
        "triaged, open source, release notes: python_frontend",
        "main",
        "patch-1",
        1,
        3,
        3,
        1,
        2,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146485"
    ],
    [
        146482,
        "Update addr doc",
        "Fixes https://github.com/pytorch/pytorch/issues/146399\r\n",
        "open",
        "2025-02-05T13:29:04Z",
        null,
        null,
        "topic: not user facing",
        "main",
        "albanD-patch-1",
        1,
        1,
        1,
        1,
        1,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146482"
    ],
    [
        146481,
        "[WIP][Windows][Inductor] Enable Inductor UT on XPU Windows.",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146481\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @desertfire @chauhang @aakhundov",
        "open",
        "2025-02-05T13:22:21Z",
        null,
        null,
        "open source, topic: not user facing, module: inductor, module: dynamo, ciflow/inductor, keep-going, ciflow/xpu",
        "gh/etaf/96/base",
        "gh/etaf/96/head",
        9,
        21,
        17,
        1,
        1,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146481"
    ],
    [
        146480,
        "[Submodule]: Update KleidiAI submodule to v1.3.0",
        "Change-Id: I687255982c72ee7daca438a15b718f07298963cc\r\n\n\ncc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @malfet @snadampal @milpuz01",
        "open",
        "2025-02-05T12:54:37Z",
        null,
        null,
        "module: cpu, open source, module: arm, ciflow/trunk, topic: not user facing, merging",
        "main",
        "kleidiai_submodule_update",
        1,
        1,
        1,
        1,
        9,
        1,
        "digantdesai, malfet",
        "APPROVED, APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146480"
    ],
    [
        146478,
        "[c10d] Add hccl distributed backend to c10d data structures",
        " # MOTIVATION\r\nIntel Gaudi is an out-of-tree PyTorch accelerator having its own device /dispatch key ```hpu``` .\r\nWith this change we add entries for Gaudi's distributed backend ```hccl``` to the c10d Backend data structures.\r\nThis is to ensure that there is no naming conflict in case a new in-tree accelerator is introduced with the same backend name.\r\n\r\n\r\nThe Out-of-tree backends are registered calling https://github.com/pytorch/pytorch/blob/fd0cd6a08f706b7bb1dedb296217b6441e4fb9ff/torch/distributed/distributed_c10d.py#L302\r\n\r\nSuccessful registration adds the backend name to the list : \r\nhttps://github.com/pytorch/pytorch/blob/fd0cd6a08f706b7bb1dedb296217b6441e4fb9ff/torch/distributed/distributed_c10d.py#L265\r\n\r\nWe are binding the process group creator constructs at run-time so if there are other distributed backend with the same device name they can safely add the device type to the dictionary \r\n\r\nhttps://github.com/pytorch/pytorch/blob/fd0cd6a08f706b7bb1dedb296217b6441e4fb9ff/torch/distributed/distributed_c10d.py#L274\r\n\r\nAnd add another entry to the dictionary with the same backend name ( but different device name )\r\nhttps://github.com/pytorch/pytorch/blob/fd0cd6a08f706b7bb1dedb296217b6441e4fb9ff/torch/distributed/distributed_c10d.py#L268\r\n\r\nIn addition the out-of-tree devices can utilize the ```backend_list``` to check for successful backend registration  eg: APIs like ```is_hccl_available```\r\n \r\n\n\ncc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o",
        "open",
        "2025-02-05T12:10:24Z",
        null,
        null,
        "oncall: distributed, triaged, open source, release notes: distributed (c10d)",
        "main",
        "c10d_add_hccl_to_backends",
        1,
        19,
        8,
        1,
        3,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146478"
    ],
    [
        146477,
        "Improve error handling when checking CUDA version in case nvcc is not found",
        "Fixes:\r\n- https://github.com/pytorch/pytorch/issues/101138\r\n\r\n**Description**\r\nThe PR enhances error handling in `_check_cuda_version` by verifying the existence of the `nvcc` executable before invoking `subprocess.check_output`. If `nvcc` is missing, a `FileNotFoundError` is raised with a clear message, guiding users to check their CUDA installation and path configuration.\r\n\r\n**Testing**\r\nManually tested with and without `nvcc` present in the expected path.\r\n\n\ncc @peterjc123 @mszhanyi @skyline75489 @nbcsm @iremyux @Blackhex",
        "open",
        "2025-02-05T12:09:22Z",
        null,
        null,
        "module: windows, triaged, open source, release notes: fx",
        "main",
        "pytorch-101138",
        1,
        4,
        1,
        1,
        1,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146477"
    ],
    [
        146476,
        "[Feat]: Improve KleidiAI 4 bit kernel performance",
        "Description:\r\n1. New thread blocking accelerates GEMVs\r\n\r\nPerf improvements:\r\n12% speedup in LLM prefill phase and upto 16% speedup in autoregressive phase\r\n\r\n\r\nChange-Id: Ie574ff8459fdb75701ae366158b4e118c70694e4\r\n\r\n\r\ncc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @malfet @snadampal @milpuz01",
        "open",
        "2025-02-05T11:50:00Z",
        null,
        null,
        "module: cpu, triaged, open source, module: arm, ciflow/trunk, topic: performance, release notes: intel, merging",
        "main",
        "kleidiai_threading_improvement",
        1,
        171,
        312,
        1,
        10,
        0,
        "malfet",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146476"
    ],
    [
        146475,
        "fix: replace stderr with stdout for download messages in hub.py",
        "This PR addresses an issue where download logs in `hub.py` are sent to `stderr` instead of `stdout`. Hence, when running models with workers, these messages are incorrectly categorized as errors, leading to confusion. ",
        "open",
        "2025-02-05T10:29:29Z",
        null,
        null,
        "triaged, open source, ciflow/trunk, topic: not user facing, merging",
        "main",
        "main",
        1,
        2,
        2,
        1,
        12,
        0,
        "mikaylagawarecki",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146475"
    ],
    [
        146474,
        "Fix torch.take_along_dim param type and default description",
        "## Changes\r\n\r\n- Change type description to `LongTensor`, consistent with [`torch.take`](https://pytorch.org/docs/stable/generated/torch.take.html)\r\n- Add `dim` param default value description\r\n\r\n## Test Result\r\n\r\n**Before**\r\n![image](https://github.com/user-attachments/assets/720ce158-2bc1-48b5-a188-56fcc7188d96)\r\n\r\n**After**\r\n![image](https://github.com/user-attachments/assets/05fe20bd-9476-4b97-ac2b-9b161d6532a1)\r\n\r\n",
        "open",
        "2025-02-05T10:01:46Z",
        null,
        null,
        "triaged, open source, ciflow/trunk, release notes: python_frontend, merging",
        "main",
        "opt/docs/take_along_dim",
        1,
        2,
        2,
        1,
        7,
        0,
        "mikaylagawarecki",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146474"
    ],
    [
        146473,
        "[export] Fix logger handler",
        "Differential Revision: D69169179\n\n\n",
        "open",
        "2025-02-05T08:28:27Z",
        null,
        null,
        "fb-exported, release notes: export",
        "main",
        "export-D69169179",
        1,
        2,
        0,
        1,
        2,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146473"
    ],
    [
        146472,
        "Refactoring pipeline parallelism test cases to be device agnostic [1/n]",
        "In this series of PR we intend to refactor pipeline parallelism test cases to enable to be completely device agnostic.\r\n\r\nThese changes will include the following approaches to do the same :\r\n\r\n\r\n- Allowing for multiple device types using instantiate_device_type_test\r\n- Replacing calls to cuda stream with torch.get_device_module(device) wherever it applies\r\n\r\nThis should result in improvement in usability for all devices\r\n\r\n\r\nFor this PR we have shown support for the following devices:\r\n\r\n- CPU (wherever applicable)\r\n- CUDA\r\n- HPU\r\n- XPU\r\n\r\nTo add other device new users can simply append their device to the device list \r\n\r\ncc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o",
        "open",
        "2025-02-05T07:53:31Z",
        null,
        null,
        "oncall: distributed, triaged, open source, ciflow/trunk, topic: not user facing, merging, module: pipelining",
        "main",
        "AnantGulati_pipeline_refactoring",
        4,
        57,
        41,
        5,
        5,
        1,
        "H-Huang",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146472"
    ],
    [
        146467,
        "Fix an issue where functional collectives don't force fx stride on inputs when compiled",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146467\n\r\nFixes https://github.com/pytorch/pytorch/issues/146416\r\n\r\nAlso added contiguity checks in the C++ functional collective ops to prevent striding issues introduced during compilation manifest as silent correctness issues.\r\n\r\ncc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @desertfire @chauhang @aakhundov",
        "open",
        "2025-02-05T03:54:36Z",
        null,
        null,
        "oncall: distributed, ciflow/trunk, release notes: distributed (c10d), module: inductor, ciflow/inductor, merging",
        "gh/yifuwang/193/base",
        "gh/yifuwang/193/head",
        4,
        105,
        27,
        5,
        8,
        1,
        "Chillee, lw, shunting314",
        "APPROVED, APPROVED, APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146467"
    ],
    [
        146466,
        "Fix one_hot inconsistent errors after compile",
        "Fixes #146274\r\n\r\n**Test Result**\r\n\r\n```python\r\n>>> import torch\r\n>>> f = torch.nn.functional.one_hot\r\n>>> a = torch.arange(0, 5) % 3  # [0,1,2,0,1]\r\n>>> num_classes = 0\r\n>>> torch.nn.functional.one_hot(a,num_classes)\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nRuntimeError: Class values must be smaller than num_classes.\r\n\r\n>>> torch.compile(torch.nn.functional.one_hot)(a,num_classes)\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/zong/code/pytorch/torch/_dynamo/eval_frame.py\", line 570, in _fn\r\n    return fn(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/zong/code/pytorch/torch/_dynamo/external_utils.py\", line 48, in inner\r\n    return fn(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^\r\nRuntimeError: Class values must be smaller than num_classes.\r\n\r\n```\r\n\r\ncc @bdhirsh",
        "open",
        "2025-02-05T02:47:27Z",
        null,
        null,
        "triaged, open source, topic: not user facing",
        "main",
        "fix/aten/one_hot",
        1,
        18,
        9,
        1,
        2,
        1,
        "zou3519",
        "COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146466"
    ],
    [
        146464,
        "[symbolic shapes] Log id for each SymNode",
        "Fixes #ISSUE_NUMBER\r\n\n\ncc @ezyang @SherlockNoMad @EikanWang @jgong5 @wenzhe-nrv",
        "open",
        "2025-02-05T01:32:37Z",
        null,
        null,
        "release notes: fx, fx, ciflow/inductor",
        "main",
        "angelayi/provenance_id",
        3,
        148,
        92,
        1,
        1,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146464"
    ],
    [
        146456,
        "Fix workarea compute in lapackSyevd",
        "work-query APIs return floating point values, that could loose precision when converted back to int. Solve this by using `nextafter` and `ceil`\r\nAdd regression test \r\n\r\nFixes #145801\r\n",
        "open",
        "2025-02-05T00:24:37Z",
        null,
        null,
        "ciflow/trunk, release notes: linalg_frontend, merging",
        "main",
        "wdvr/iss_145801",
        2,
        14,
        2,
        5,
        4,
        0,
        "malfet",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146456"
    ],
    [
        146455,
        "[logging] Save compile state in CompiledFxGraph and make it available at runtime",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146455\n\r\nSummary: To support logging the correct compile_id for runtime timings (like Triton autotuning), save the compile_id in CompiledFxGraph make it available to logging utilities, i.e., dynamo_timed.\r\n\r\nThe previous attempt put the compile_id in the inductor_metadata with the Triton output code, but that broke Triton caching and we reverted. This version does the following:\r\n* When creating or deserializing a CompiledFxGraph, save the compile-time compile_id.\r\n* Implement a class `RuntimeCompileContext` that's analogous to `CompileContext` where we can look up the compile_id at runtime.\r\n* Set this runtime compile context during `CompiledFxGraph.__call__`.\r\n* Removes the compile_id as a param to dynamo_timed; dynamo_timed can figure it out instead.\r\n* Removes separate dynamo_timed params for compile-time and runtime dynamo_compile column names. We can use one param have dynamo_timed figure out whether to treat as a runtime or compile-time event.\r\n\r\nTest Plan:\r\n* tlparse (`python benchmarks/dynamo/torchbench.py --performance --training --amp --backend inductor --device cuda --print-compilation-time --repeat 5 --cold-start-latency --only nanogpt`): https://fburl.com/bu5i8efk\r\n* dynamo_compile: https://fburl.com/scuba/dynamo_compile/sandbox/3d74ps92\r\n* pt2_compile_events: https://fburl.com/scuba/pt2_compile_events/ooqoe5tu\r\n\r\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @desertfire @chauhang @aakhundov",
        "open",
        "2025-02-05T00:23:56Z",
        null,
        null,
        "topic: not user facing, module: inductor, module: dynamo, ciflow/inductor",
        "gh/masnesral/176/base",
        "gh/masnesral/176/head",
        7,
        113,
        74,
        5,
        2,
        0,
        "masnesral, masnesral, masnesral, masnesral, masnesral, masnesral, masnesral, masnesral, masnesral, masnesral, xmfan, masnesral, masnesral, xmfan, xmfan, masnesral, xmfan, xmfan, masnesral",
        "COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146455"
    ],
    [
        146454,
        "[dynamo][fullgraph] Raise NoGraphError if no graph with fullgraph=True",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146454\n* #146507\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-02-04T23:50:47Z",
        null,
        null,
        "ciflow/trunk, topic: not user facing, module: dynamo, ciflow/inductor",
        "gh/anijain2305/669/base",
        "gh/anijain2305/669/head",
        3,
        77,
        0,
        4,
        1,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146454"
    ],
    [
        146452,
        "cpp_wrapper: enable all CI inductor tests",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146452\n* #146706\n* #146424\n* #146109\n* #146449\n* #144349\n* #144293\n* #144002\n\nWith the speedups from precompiled headers, we can now enable all currently enabled CI tests for inductor in cpp_wrapper mode.",
        "open",
        "2025-02-04T22:58:10Z",
        null,
        null,
        "open source, topic: not user facing, ciflow/inductor, keep-going, ci-no-test-timeout",
        "gh/benjaminglass1/66/base",
        "gh/benjaminglass1/66/head",
        1,
        25,
        45,
        6,
        1,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146452"
    ],
    [
        146449,
        "cpp_wrapper: handle mixed-device C-shim fallbacks",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #146452\n* #146706\n* #146424\n* #146109\n* __->__ #146449\n* #144349\n* #144293\n* #144002\n\nFixes an error from test_torch, where a CUDA cpp_wrapper run called a CUDA native C-shim kernel with two CPU tensors.\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @desertfire @chauhang @aakhundov",
        "open",
        "2025-02-04T22:07:25Z",
        null,
        null,
        "open source, topic: not user facing, module: inductor, ciflow/inductor",
        "gh/benjaminglass1/65/base",
        "gh/benjaminglass1/65/head",
        5,
        90,
        42,
        4,
        1,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146449"
    ],
    [
        146448,
        "[ROCm] Indexing perf optimization via Unroll/WideFetch/IdxReuse/OneDupOpt",
        "Fixes #ISSUE_NUMBER\r\n\n\ncc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd",
        "open",
        "2025-02-04T21:56:59Z",
        null,
        null,
        "module: rocm, triaged, open source, release notes: cuda, ciflow/rocm",
        "main",
        "main",
        1,
        49,
        0,
        1,
        2,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146448"
    ],
    [
        146443,
        "Stop poisoning fork on Dataloader creation when pin_memory is enabled",
        "Fixes https://github.com/pytorch/pytorch/issues/144687\r\nNeeds https://github.com/pytorch/pytorch/pull/146098 that already landed to fix the issue above\r\n\r\nA longer-term fix would be to move cuda's non-poisoning is_available() check to c++. But that would be quite a bit of work.\r\n\r\nThis PR also updates the behavior of current_accelerator() in python to match getAccelerator() in C++ and update all docs to reflect that.",
        "open",
        "2025-02-04T20:54:13Z",
        null,
        null,
        "release notes: dataloader, topic: bug fixes",
        "main",
        "fix_dataloader",
        6,
        66,
        26,
        5,
        1,
        1,
        "ngimel, guangyey, guangyey, guangyey, guangyey, andrewkho",
        "APPROVED, COMMENTED, APPROVED, COMMENTED, COMMENTED, APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146443"
    ],
    [
        146440,
        "[sigmoid] Implement a OSS only model runner.",
        "Summary: Implement an oss version of modelrunner with clean dependencies. The new oss model runner only removes thrift and only use json header to load the model.\n\nTest Plan: Test will be added in the next diff separately. (D69060784)\n\nDifferential Revision: D68846877\n\n\n",
        "open",
        "2025-02-04T19:38:20Z",
        null,
        null,
        "fb-exported, ciflow/trunk, ciflow/inductor, release notes: export",
        "main",
        "export-D68846877",
        1,
        5,
        4,
        1,
        5,
        0,
        "SherlockNoMad",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146440"
    ],
    [
        146436,
        "[Testing] Reduce `test_exp` flakiness",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146436\n\r\nBy setting `reference_in_float` to false,  as `exp(a + b)` could yield significantly different results than `exp(a.half()+b.half())` as one can see in the following example (which is accidentally the random values generated by MacOS RNG for this test)\r\n\r\n```\r\n>>> import torch\r\n>>> x=torch.tensor(2.5599, dtype=torch.half)\r\n>>> y=torch.tensor(0.6970, dtype=torch.half)\r\n>>> (x + y).exp()\r\ntensor(26., dtype=torch.float16)\r\n>>> (x.float() + y.float()).exp()\r\ntensor(25.9799)\r\n```\r\n\r\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @desertfire @chauhang @aakhundov",
        "open",
        "2025-02-04T18:54:11Z",
        null,
        null,
        "Merged, Reverted, topic: not user facing, ciflow/mps, module: inductor, ciflow/inductor, ci-no-td",
        "gh/malfet/169/base",
        "gh/malfet/169/head",
        2,
        9,
        2,
        2,
        6,
        0,
        "dcci, malfet, dcci, jansel",
        "COMMENTED, COMMENTED, APPROVED, APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146436"
    ],
    [
        146427,
        "add the `torch.float8_e8m0fnu` dtype to PyTorch",
        "Summary:\r\n\r\nAdds the `torch.float8_e8m0fnu` dtype to PyTorch, as detailed in\r\nhttps://github.com/pytorch/pytorch/issues/146414 . Please see the issue for a detailed definition of the format.  Example of basic functionality:\r\n\r\n```python\r\nimport torch\r\n\r\n# round trip\r\nx0 = torch.randn(4, 4, dtype=torch.float32)\r\nx1 = x0.to(torch.float8_e8m0fnu)  # RNE rounding\r\nx2 = x1.to(torch.float32)  # 2 ** exponent\r\n\r\n# creation with empty\r\nx0 = torch.empty(4, 4, dtype=torch.float8_e8m0fnu)\r\n\r\n# printing\r\nprint(x0)\r\n```\r\n\r\nDone in this PR:\r\n* numerical correctness\r\n* op coverage (except for `torch._scaled_mm`): create tensor, cast to/from float32\r\n* printing a tensor works\r\n\r\nFor future PRs:\r\n* performance optimizations for casting\r\n* torch._scaled_mm\r\n* PT2\r\n* various cleanups (detailed in comments with issue numbers)\r\n\r\nTest Plan:\r\n\r\n```\r\npytest test/quantization/core/experimental/test_float8.py -s\r\n```\r\n\r\nReviewers:\r\n\r\nSubscribers:\r\n\r\nTasks:\r\n\r\nTags:\r\n\r\ncc @yanbing-j @albanD @kadeng @penguinwu @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10",
        "open",
        "2025-02-04T18:07:11Z",
        null,
        null,
        "module: cpu, release notes: quantization, module: float8",
        "main",
        "gh/vkuzo/1/head",
        23,
        508,
        43,
        8,
        2,
        0,
        "vkuzo, vkuzo, vkuzo, vkuzo, vkuzo, vkuzo, vkuzo, vkuzo, vkuzo, vkuzo, vkuzo, drisspg, drisspg, drisspg, drisspg, vkuzo, vkuzo, drisspg, drisspg, vkuzo, vkuzo, eqy, vkuzo, drisspg",
        "COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146427"
    ],
    [
        146426,
        "Test typing of arithmetic operators on Tensor (see #145838)",
        "See #145838\r\n\r\nStack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146426\n\r\n",
        "open",
        "2025-02-04T18:06:50Z",
        null,
        null,
        "open source, ciflow/trunk, topic: not user facing, merging",
        "gh/rec/131/base",
        "gh/rec/131/head",
        2,
        462,
        0,
        5,
        10,
        0,
        "Skylion007, Skylion007, rec, rec",
        "COMMENTED, APPROVED, COMMENTED, COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146426"
    ],
    [
        146424,
        "cpp_wrapper: fix test_torchinductor* tests",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #146452\n* #146706\n* __->__ #146424\n* #146109\n* #146449\n* #144349\n* #144293\n* #144002\n\n\n\ncc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @voznesenskym @penguinwu @EikanWang @Guobing-Chen @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @desertfire @chauhang @aakhundov",
        "open",
        "2025-02-04T18:04:13Z",
        null,
        null,
        "module: cpu, open source, topic: not user facing, module: inductor, ciflow/inductor",
        "gh/benjaminglass1/64/base",
        "gh/benjaminglass1/64/head",
        3,
        22,
        7,
        5,
        1,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146424"
    ],
    [
        146421,
        "experimental specialization logging",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146421\n\n\n\ncc @ezyang @SherlockNoMad @EikanWang @jgong5 @wenzhe-nrv\n\nDifferential Revision: [D69163120](https://our.internmc.facebook.com/intern/diff/D69163120)",
        "open",
        "2025-02-04T17:34:46Z",
        null,
        null,
        "ciflow/trunk, release notes: fx, fx, ciflow/inductor",
        "gh/bobrenjc93/270/base",
        "gh/bobrenjc93/270/head",
        1,
        37,
        1,
        6,
        2,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146421"
    ],
    [
        146420,
        "[ROCm] Enable tunable warp size for stride one indexing backwards kernel",
        "Enable tunable warp size for stride one indexing backwards kernel. This will allow for the indexing backward kernel with stride one to work on smaller warp sizes.\n\ncc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd",
        "open",
        "2025-02-04T17:24:38Z",
        null,
        null,
        "module: rocm, triaged, open source, topic: not user facing, ciflow/periodic, rocm, ciflow/rocm",
        "main",
        "improve-backwards-indexing-with-stride-1",
        1,
        74,
        0,
        1,
        1,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146420"
    ],
    [
        146418,
        "[BE]: Add TypeVarTuple to RNN Args for better type inference",
        "Fixes #ISSUE_NUMBER\r\n",
        "open",
        "2025-02-04T16:46:07Z",
        null,
        null,
        "open source",
        "main",
        "skylion007/typevartuple-nn-rnn-2025-02-04",
        1,
        5,
        4,
        1,
        2,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146418"
    ],
    [
        146417,
        "Only call triton in worker process, kick off worker processes earlier, during inductor codegen",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\r\n* __->__ #146417\r\n\r\n### Big idea\r\nThis PR extends https://github.com/pytorch/pytorch/pull/144288 by combining calling triton in worker processes with the future cache: we kick off triton compilation in the worker processes earlier, during inductor codegen. Basically instead of calling async_compile.triton for the first time only after the entire code has been generated, we start compiling as soon as we know we'll need to compile the kernel. Then, when loading the generated inductor code, we can simply read from our in memory future cache, considerably increasing the parallelism.\r\n### Implementation Overview\r\nIn total, the diff does the following:\r\n- Converts TritonFuture to LambdaFuture, only calling triton.compile on worker processes\r\n- Now that triton.compile() isn't called on the main process, we call TritonBundler on all compiled kernels when we get them back from workers\r\n- Extend @eellison's future cache to a class, mostly as a refactor\r\n- Finally, call async_compile.triton ahead of time in Scheduler.codegen if workers are warmed up. This causes the subsequent\r\nasync_compile.triton call that occurs after codegen to cache hit on cold start.\r\nIn the diffs after this, I will add more to CompiledTritonKernels so that TritonBundler, on a warm start, automatically populates the in memory cache on warm start with the existing triton kernels, avoiding calling triton altogether on warm starts.\r\nBecause LambdaFutures are much faster to kick off than TritonFutures, due to not needing to load from TritonCodeCache at all, the time spent kicking off these worker jobs is pretty minimal for inductor codegen.\r\n\r\nDifferential Revision: [D69123174](https://our.internmc.facebook.com/intern/diff/D69123174/)\r\n\r\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @desertfire @chauhang @aakhundov",
        "open",
        "2025-02-04T16:20:19Z",
        null,
        null,
        "fb-exported, ciflow/trunk, topic: not user facing, module: inductor, ciflow/inductor",
        "gh/jamesjwu/106/base",
        "gh/jamesjwu/106/head",
        9,
        250,
        77,
        16,
        17,
        1,
        "jamesjwu, jamesjwu, masnesral, jamesjwu, jansel",
        "COMMENTED, COMMENTED, COMMENTED, COMMENTED, CHANGES_REQUESTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146417"
    ],
    [
        146415,
        "Only call triton in worker process, ahead of time compile",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146415\n\n# Big idea\nThis PR extends https://github.com/pytorch/pytorch/pull/144288 by combining calling triton in worker processes with the future cache: we kick off triton compilation in the worker processes earlier, during inductor codegen. Basically instead of calling async_compile.triton for the first time only after the entire code has been generated, we start compiling as soon as we know we'll need to compile the kernel. Then, when loading the generated inductor code, we can simply read from our in memory future cache, considerably increasing the parallelism.\n# Implementation Overview\nIn total, the diff does the following:\n- Converts TritonFuture to LambdaFuture, only calling triton.compile on worker processes\n- Now that triton.compile() isn't called on the main process, we call TritonBundler on all compiled kernels when we get them back from workers\n- Extend @eellison's future cache to a class, mostly as a refactor\n- Finally, call async_compile.triton ahead of time in Scheduler.codegen if workers are warmed up. This causes the subsequent\nasync_compile.triton call that occurs after codegen to cache hit on cold start.\nIn the diffs after this, I will add more to CompiledTritonKernels so that TritonBundler, on a warm start, automatically populates the in memory cache on warm start with the existing triton kernels, avoiding calling triton altogether on warm starts.\nBecause LambdaFutures are much faster to kick off than TritonFutures, due to not needing to load from TritonCodeCache at all, the time spent kicking off these worker jobs is pretty minimal for inductor codegen.\n\n### Can we split the diff for easier review?\nIt's best if this diff lands atomically with all of these changes, as doing the ahead of time codegen compile is only performant if we replace TritonFuture with LambdaFuture(as we don't need to load the triton kernel on the main process). However, I've made a diff stack for easier reviewing here:\n- D69070048 - Run async_compile.triton ahead of time in Scheduler.codegen\n- D68633454 - Only call triton in worker process\n\nDifferential Revision: [D69070616](https://our.internmc.facebook.com/intern/diff/D69070616/)\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @desertfire @chauhang @aakhundov",
        "open",
        "2025-02-04T16:14:20Z",
        null,
        null,
        "fb-exported, module: inductor, ciflow/inductor",
        "gh/jamesjwu/105/base",
        "gh/jamesjwu/105/head",
        6,
        111,
        63,
        1,
        3,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146415"
    ],
    [
        146407,
        "[ROCm] Unskip std:bad_alloc failures",
        "Flakey MI300 issue related to memory usage should now be resolved after https://github.com/pytorch/pytorch/actions/runs/13007160888?pr=145829.\n\ncc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @hongxiayang @naromero77amd @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @desertfire @chauhang @aakhundov",
        "open",
        "2025-02-04T14:14:17Z",
        null,
        null,
        "module: rocm, triaged, open source, ciflow/trunk, topic: not user facing, module: inductor, ciflow/inductor, merging, ciflow/rocm",
        "main",
        "unskip-bad-alloc",
        1,
        0,
        7,
        1,
        4,
        0,
        "jeffdaily",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146407"
    ],
    [
        146406,
        "Only enable aotriton on x86_64 and aarch64",
        "Make `USE_FLASH_ATTENTION` and `USE_MEM_EFF_ATTENTION` depend on `CPU_INTEL OR CPU_AARCH64`.\r\n\r\n[aotriton pre-built](https://github.com/ROCm/aotriton/releases) is only available on x86_64.\r\n\r\nAlthough `AOTRITON_INSTALL_FROM_SOURCE` can be specified to build from source, building aotriton requires CUDA, so on architectures without CUDA support (like riscv64), it still needs to be disabled.",
        "open",
        "2025-02-04T13:33:47Z",
        null,
        null,
        "triaged, open source, topic: not user facing",
        "main",
        "patch-1",
        1,
        3,
        2,
        1,
        4,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146406"
    ],
    [
        146403,
        "Use std::string_view",
        "Fixes #ISSUE_NUMBER\r\n",
        "open",
        "2025-02-04T12:59:16Z",
        null,
        null,
        "open source, topic: not user facing",
        "main",
        "string_view_gen2",
        2,
        2,
        1,
        1,
        1,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146403"
    ],
    [
        146395,
        "[dynamo][builtin-skipfile-cleanup] Remove random",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146395\n* #146339\n* #146116\n* #146322\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-02-04T05:35:35Z",
        null,
        null,
        "module: dynamo, ciflow/inductor, keep-going",
        "gh/anijain2305/668/base",
        "gh/anijain2305/668/head",
        1,
        0,
        2,
        1,
        2,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146395"
    ],
    [
        146393,
        "PEP585: More fixes 2",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146393\n* #146392\n* #146391\n\n\n\ncc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o @EikanWang @jgong5 @wenzhe-nrv @sanchitintel @ezyang @SherlockNoMad",
        "open",
        "2025-02-04T04:20:02Z",
        null,
        null,
        "oncall: distributed, oncall: jit, release notes: quantization, fx, ciflow/inductor, release notes: AO frontend",
        "gh/aorenste/217/base",
        "gh/aorenste/217/head",
        30,
        62,
        76,
        1,
        1,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146393"
    ],
    [
        146392,
        "PEP585: More fixes",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #146393\n* __->__ #146392\n* #146391\n\n",
        "open",
        "2025-02-04T04:19:56Z",
        null,
        null,
        "release notes: onnx, module: inductor, module: dynamo, ciflow/inductor",
        "gh/aorenste/216/base",
        "gh/aorenste/216/head",
        30,
        108,
        147,
        1,
        1,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146392"
    ],
    [
        146391,
        "PEP585: Add noqa to necessary tests",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146391\n\n",
        "open",
        "2025-02-04T04:19:51Z",
        null,
        null,
        "topic: not user facing",
        "gh/aorenste/215/base",
        "gh/aorenste/215/head",
        7,
        63,
        31,
        7,
        1,
        1,
        "justinchuby, justinchuby, albanD, aorenste",
        "COMMENTED, APPROVED, COMMENTED, COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146391"
    ],
    [
        146388,
        "[WIP][CUDA][cuDNN] Experimental `cudnn_rms_norm`",
        "opt-in for now behind two new native functions---the plan would be to eventually add it as the `CUDA:` backend to `rms_norm`\r\n\r\nInitial experiments show forward ~4-5x speed, up fwd+bwd ~3x speedup\n\ncc @csarofeen @ptrblck @xwang233 @msaroufim",
        "open",
        "2025-02-04T02:55:56Z",
        null,
        null,
        "module: cudnn, module: cuda, open source, module: norms and normalization, topic: not user facing",
        "main",
        "cudnnrmsforward",
        5,
        422,
        0,
        4,
        5,
        0,
        "albanD, eqy",
        "COMMENTED, COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146388"
    ],
    [
        146387,
        "AMD: reverting to default config for better performance.",
        "TopK performance on ROCm performs better on the test suite with the default config.",
        "open",
        "2025-02-04T02:49:19Z",
        null,
        null,
        "open source, release notes: cuda",
        "main",
        "topk_rocm_tune",
        1,
        0,
        9,
        1,
        2,
        1,
        "malfet",
        "APPROVED",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146387"
    ],
    [
        146385,
        "[WIP] Confirm XPU Regression",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146385\n\n",
        "open",
        "2025-02-04T02:32:40Z",
        null,
        null,
        "triaged, open source, topic: not user facing, ciflow/xpu",
        "gh/EikanWang/74/base",
        "gh/EikanWang/74/head",
        1,
        1,
        1,
        1,
        1,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146385"
    ],
    [
        146372,
        "[Submodule] Turning flash-attention integration into 3rd party submod (#144120)",
        "Summary:\n\n# Summary\n\n### Sticky points\n\nCuda-graph rng handling has changed / deviated from original implementation. We will be left with a dangling 'offset' val and confusing naming due to BC\n\n## Dependencies\n- Flash PR: https://github.com/Dao-AILab/flash-attention/pull/1419\n\n### Other Points\n- The BC linter is complaining about losing generate.py and its functions which is not real BC surface\ncc albanD\n\nimported-using-ghimport\n\nTest Plan:\nImported from OSS\n\nBuilding in dev\n`buck build @//mode/dev-nosan -c fbcode.nvcc_arch=h100a  //caffe2:ATen-cu --show-full-output    `\n\nI and Nming the .so I do see that the flash symbols are correctly named:\n```\n0000000001c3dfb0 t pytorch_flash::run_mha_bwd(pytorch_flash::Flash_bwd_params&, CUstream_st*)::$_0::operator()() const::{lambda()#1}::operator()() const::{lambda()#1}::operator()() const::{lambda()#7}::operator()() const\n0000000001c36080 t pytorch_flash::run_mha_fwd(pytorch_flash::Flash_fwd_params&, CUstream_st*, bool)::$_0::operator()() const::{lambda()#2}::operator()() const::{lambda()#1}::operator()() const::{lambda()#6}::operator()() const\n0000000001c360e0 t pytorch_flash::run_mha_fwd(pytorch_flash::Flash_fwd_params&, CUstream_st*, bool)::$_0::operator()() const::{lambda()#2}::operator()() const::{lambda()#1}::operator()() const::{lambda()#7}::operator()() const\n0000000001c35fc0 t pytorch_flash::run_mha_fwd(pytorch_flash::Flash_fwd_params&, CUstream_st*, bool)::$_0::operator()() const::{lambda()#1}::operator()() const::{lambda()#1}::operator()() const::{lambda()#6}::operator()() const\n0000000001c36020 t pytorch_flash::run_mha_fwd(pytorch_flash::Flash_fwd_params&, CUstream_st*, bool)::$_0::operator()() const::{lambda()#1}::operator()() const::{lambda()#1}::operator()() const::{lambda()#7}::operator()() const\n```\n\nReviewed By: vkuzo\n\nDifferential Revision: D68502879\n\nPulled By: drisspg\n",
        "open",
        "2025-02-04T00:34:54Z",
        null,
        null,
        "fb-exported, ciflow/trunk, topic: not user facing, ciflow/inductor, module: sdpa",
        "main",
        "export-D68502879",
        30,
        121,
        4214,
        1,
        9,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146372"
    ],
    [
        146367,
        "[dynamo][EXPERIMENT] Prototype for `mark_traceable`",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146367\n* #146714\n* #146713\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-02-04T00:05:53Z",
        null,
        null,
        "module: dynamo, ciflow/inductor",
        "gh/StrongerXi/81/base",
        "gh/StrongerXi/81/head",
        8,
        366,
        30,
        2,
        2,
        1,
        "StrongerXi, zou3519, StrongerXi, StrongerXi, StrongerXi, StrongerXi, zou3519",
        "COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146367"
    ],
    [
        146364,
        "[DeviceMesh] Add some documentation for `from_group` API and add a 2D test",
        "Fixes #ISSUE_NUMBER\r\n\n\ncc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wconstab @d4l3k @c-p-i-o @tianyu-l @XilunWu",
        "open",
        "2025-02-03T22:54:40Z",
        null,
        null,
        "oncall: distributed, module: dtensor, release notes: distributed (dtensor)",
        "main",
        "add_from_group_doc_and_test",
        2,
        102,
        11,
        1,
        1,
        2,
        "fduwjj, fduwjj, wz337, fegin, wz337",
        "COMMENTED, APPROVED, COMMENTED, COMMENTED, COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146364"
    ],
    [
        146356,
        "[cutlass backend] fix bug for accuminator dtype",
        "Will add unit tests for accuracy. \r\n\r\nStack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #146743\n* __->__ #146356\n\r\n\r\n\r\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @desertfire @chauhang @aakhundov",
        "open",
        "2025-02-03T22:03:13Z",
        null,
        null,
        "Merged, Reverted, ciflow/trunk, topic: not user facing, module: inductor, ciflow/inductor, ci-no-td",
        "gh/henrylhtsang/2/base",
        "gh/henrylhtsang/2/head",
        2,
        7,
        74,
        5,
        10,
        0,
        "Chillee",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146356"
    ],
    [
        146355,
        "[dynamo] replace hardcoded eval frame control flags skip_code_recursive_flag/cache_limit_hit_flag",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146355\n* #145603\n\r\nThis PR and the previous:\r\n- Moves parts of `eval_frame.c` to C++.\r\n- Reduces code duplication in `dynamo__custom_eval_frame` and makes the control flow more clear.\r\n- Enables `convert_frame` to signal to `eval_frame.cpp` in a general manner how to evaluate this frame, recursive frames, and future frames with the same code object (default/compile, skip, run-only). e.g. this will allow us to change skipping/cache limit hit eval_frame behavior directly from convert_frame without requiring changes to C/C++.\r\n\r\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-02-03T22:01:46Z",
        null,
        null,
        "topic: not user facing, module: dynamo, ciflow/inductor",
        "gh/williamwen42/204/base",
        "gh/williamwen42/204/head",
        9,
        215,
        172,
        5,
        1,
        0,
        "williamwen42, jansel, anijain2305, williamwen42, jansel",
        "COMMENTED, CHANGES_REQUESTED, COMMENTED, COMMENTED, COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146355"
    ],
    [
        146352,
        "Build a storage reader/writer to write checkpoints in HF format",
        "Summary: Title - we want to write checkpoints in HF format with DCP, this diff allows this for the non-distributed use case.\r\n\r\nTest Plan:\r\nbuck2 test 'fbcode//mode/dev-nosan' fbcode//caffe2/test/distributed/checkpoint:test_hf_torchtune_storage\r\n\r\nN6476188 --> able to save and load tensor in hf format\r\n\r\nDifferential Revision: D68444967\r\n\r\n\r\n\r\n\r\ncc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o @LucasLLC @MeetVadakkanchery @mhorowitz @pradeepfn @ekr0",
        "open",
        "2025-02-03T21:20:40Z",
        null,
        null,
        "oncall: distributed, fb-exported, ciflow/trunk, module: distributed_checkpoint",
        "main",
        "export-D68444967",
        4,
        316,
        5,
        1,
        6,
        0,
        "saumishr, ankitageorge, ankitageorge, ankitageorge, ankitageorge, ankitageorge, fegin, ankitageorge, ankitageorge",
        "COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146352"
    ],
    [
        146341,
        "Only call triton in worker process; run async_compile.triton ahead of time in Scheduler.codegen",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* (to be filled)\n\n### Big idea\nThis PR extends https://github.com/pytorch/pytorch/pull/144288 by combining calling triton in worker processes with the future cache: we kick off triton compilation in the worker processes earlier, during inductor codegen. Basically instead of calling async_compile.triton for the first time only after the entire code has been generated, we start compiling as soon as we know we'll need to compile the kernel. Then, when loading the generated inductor code, we can simply read from our in memory future cache, considerably increasing the parallelism.\n\n### Implementation Overview\nIn total, the diff does the following:\n- Converts TritonFuture to LambdaFuture, only calling triton.compile on worker processes\n- Now that triton.compile() isn't called on the main process, we call TritonBundler on all compiled kernels when we get them back from workers\n- Extend @eellison's future cache to a class, mostly as a refactor\n- Finally, call async_compile.triton ahead of time in Scheduler.codegen if workers are warmed up. This causes the subsequent async_compile.triton call that occurs after codegen to cache hit on cold start.\n\nIn the diffs after this, I will add more to CompiledTritonKernels so that TritonBundler, on a warm start, automatically populates the in memory cache on warm start with the existing triton kernels, avoiding calling triton altogether on warm starts.\n\nBecause LambdaFutures are much faster to kick off than TritonFutures, due to not needing to load from TritonCodeCache at all, the time spent kicking off these worker jobs is pretty minimal for inductor codegen.\n\n### Can we split the diff for easier review?\nIt's best if this diff lands atomically with all of these changes, as doing the ahead of time codegen compile is only performant if we replace TritonFuture with LambdaFuture(as we don't need to load the triton kernel on the main process). However, I've made a diff stack for easier reviewing here:\nD69070048 - Run async_compile.triton ahead of time in Scheduler.codegen\nD68633454 - Only call triton in worker process\n\nDifferential Revision: [D69013710](https://our.internmc.facebook.com/intern/diff/D69013710/)\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @desertfire @chauhang @aakhundov",
        "open",
        "2025-02-03T20:46:41Z",
        null,
        null,
        "module: inductor, ciflow/inductor",
        "gh/jamesjwu/102/base",
        "gh/jamesjwu/102/head",
        6,
        108,
        71,
        1,
        2,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146341"
    ],
    [
        146335,
        "[dynamic shapes][WIP] mark backed size symbols as size-like",
        "experimental, to apply upper-bound / maxsize size-oblivious semantics to backed symbols\r\n\r\n\r\ncc @ezyang @SherlockNoMad @EikanWang @jgong5 @wenzhe-nrv @voznesenskym @penguinwu @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-02-03T19:56:25Z",
        null,
        null,
        "ciflow/trunk, release notes: fx, fx, module: dynamo, ciflow/inductor",
        "main",
        "pianpwk/treat_sizes_as_size_like",
        5,
        31,
        8,
        7,
        2,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146335"
    ],
    [
        146333,
        "Add optional generator to distribution sampler/rsample methods.",
        "Fixes part of #45115 and #11340\r\nAdds a generator parameter to all the sample/rsample methods of torch distribution classes\n\ncc @fritzo @neerajprad @alicanb @nikitaved",
        "open",
        "2025-02-03T19:47:59Z",
        null,
        null,
        "module: distributions, triaged, open source, topic: not user facing",
        "main",
        "features/distribution_generator",
        30,
        160,
        138,
        1,
        4,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146333"
    ],
    [
        146324,
        "[torch][amdsmi] Avoid ODR violation when loading amdsmi",
        "Summary:\namdsmi bundles its own copy of `libamd_smi.so`. When you're interacting with `amdsmi` from *only* python that's fine, but when you try to interact with `libamd_smi.so` from native code too this poses a problem, because from native code you'll be linking against the copy of `libamd_smi.so` from the SDK.\n\nThis means you'll end up with 2 copies of `libamd_smi.so` in your process, and potentially (Murphey's law says you will, as does our CI) violate ODR.\n\nIn order to avoid this issue from the PT side of the world we can hook the `dlopen(\"path/to/bundled/libamd_smi.so\")` and try to use the already loaded/SDK version of `libamd_smi.so` first, before proceeding to use the `path/to/bundled/libamd_smi.so`.\n\nTest Plan: CI, inspect process using libamd_smi.so from native + python and observe only a single copy loaded\n\nDifferential Revision: D69064038\n\n\n",
        "open",
        "2025-02-03T18:59:18Z",
        null,
        null,
        "fb-exported, ciflow/trunk, topic: not user facing",
        "main",
        "export-D69064038",
        1,
        41,
        1,
        1,
        14,
        1,
        "malfet, danzimm, malfet, malfet",
        "COMMENTED, COMMENTED, APPROVED, COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146324"
    ],
    [
        146321,
        "[ONNX] Support custom axis name through dynamic_shapes",
        "Fixes #143443\r\n\r\nThis PR aims to support custom dynamic axis naming through dynamic_shapes. Currently, _Dim and _DimHint do not support dynamic axis naming (#144273).\r\n\r\n1. **the original dynamic shapes guarantee**\r\nThe axis renaming is only applied when dynamic shapes include string instead of all _Dim and _DimHint. Thus, there will not be any inconsistent behavior to dynamic_shapes with torch.export.export if the given dynamic shapes follow torch.export.export format.\r\n2. _DimHint.AUTO is applied to the axes that are specified with custom names to avoid exporter crash. (_DimHint.DYNAMIC crashes when the export fails.)\r\n3.  There's no need to handle cases where kwargs are out of order with the model signature,\r\n    as torch.export.export supports dynamism only when kwargs and dynamic_shapes are provided in order.\r\n    https://github.com/pytorch/pytorch/blob/49082f9dba3b79a344cb03652972ddbe7c3729cc/torch/export/_trace.py#L2034\r\n4. If `torch.onnx.ExportedProgram` finds the axes share the same constraints, they will have the same name (e.g. s0, s1, ...). Therefore, even if the ONNX users specify them with different custom names, they won't be respected.\r\n\r\nExample model:\r\n```python\r\n        class NestedModel(torch.nn.Module):\r\n            def forward(\r\n                self,\r\n                x: torch.Tensor,\r\n                ys: list[torch.Tensor],\r\n                zs: dict[str, torch.Tensor],\r\n                c: torch.Tensor,\r\n            ):\r\n                y = ys[0] + ys[1] + zs[\"a\"] + zs[\"b\"]\r\n                w = 5\r\n                if x.shape[0] < 3 and c.shape[0] != 4:\r\n                    return x + w, x + y, c\r\n                else:\r\n                    return x - w, x - y, c\r\n\r\n        input = (\r\n            torch.ones(5),\r\n            [torch.zeros(5), torch.ones(5)],\r\n            {\"a\": torch.zeros(5), \"b\": torch.ones(5)},\r\n            torch.ones(6),\r\n        )\r\n\r\n        dynamic_shapes = (\r\n            {0: torch.export.Dim(\"dim_x\", min=3)},  # _Dim\r\n            [(\"custom_name_axis_ys_0\",), (torch.export.Dim.AUTO,)],  # custom name\r\n            {\r\n                \"a\": {0: torch.export.Dim.AUTO},\r\n                \"b\": (\"custom_name_axis_zs_b_0\",),\r\n            },  # _DimHint\r\n            {0: \"custom_name_axis_c_0\"},  # custom name\r\n        )\r\n\r\n```",
        "open",
        "2025-02-03T18:00:09Z",
        null,
        null,
        "open source, release notes: onnx, topic: new features",
        "main",
        "titaiwang/support_axis_name",
        6,
        722,
        247,
        9,
        1,
        0,
        "justinchuby, justinchuby, justinchuby, justinchuby, justinchuby, justinchuby, justinchuby, xadupre, xadupre, xadupre, xadupre, xadupre, justinchuby, titaiwangms, titaiwangms, titaiwangms, titaiwangms, titaiwangms, titaiwangms, titaiwangms, titaiwangms, titaiwangms, justinchuby, justinchuby",
        "COMMENTED, APPROVED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146321"
    ],
    [
        146319,
        "[export][dynamic shapes] use size-oblivious upper bound reasoning for backed symbols",
        "cc @ezyang @SherlockNoMad @EikanWang @jgong5 @wenzhe-nrv",
        "open",
        "2025-02-03T17:31:50Z",
        null,
        null,
        "release notes: fx, fx, ciflow/inductor",
        "main",
        "pianpwk/backed_symint_endofbounds",
        2,
        28,
        11,
        1,
        1,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146319"
    ],
    [
        146318,
        "Hack AC to not clear recomputed activations",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #146633\n* __->__ #146318\n* #145399\n* #145533\n* #145531\n* #145520\n\n",
        "open",
        "2025-02-03T17:29:36Z",
        null,
        null,
        "",
        "gh/soulitzer/351/base",
        "gh/soulitzer/351/head",
        1,
        2,
        2,
        1,
        2,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146318"
    ],
    [
        146313,
        "[Dynamo] fix torch._dynamo.assume_constant_result when used on class method",
        "This PR fixes `torch._dynamo.assume_constant_result` when it is used on a class method, and the class was instantiated inside of the dynamo traced code.\r\n\r\nThe issue: Currently when an object is modified by storing attributes on it in dynamo, the side effects of those attribute stores are tracked using the `SideEffect` system, and the modifications to the class are not performed in the first place.  For example, in\r\n```python\r\nclass A:\r\n    def __init__(self):\r\n        self.value = 123\r\n```\r\nThe `self.value` field will not be set on the class `A` but rather it will only be tracked within the `SideEffect` system.\r\n\r\nThis causes a problem with `torch._dynamo.assume_constant_result` as it converts the value in dynamo back into a normal python value and invokes the function as normal python.  However, it currently does not find the `self.value` field (as it was never set on the underlying object).  This PR checks if the object passed to the `torch._dynamo.assume_constant_result` has any pending mutations from the `SideEffect` system and applies them before calling the `torch._dynamo.assume_constant_result` function.\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-02-03T16:00:09Z",
        null,
        null,
        "triaged, open source, module: dynamo",
        "main",
        "dynamo-fixes-6",
        2,
        37,
        8,
        2,
        8,
        1,
        "anijain2305, StrongerXi, jansel, anijain2305",
        "APPROVED, COMMENTED, CHANGES_REQUESTED, COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146313"
    ],
    [
        146310,
        "Fix type stubs for SymmetricMemory",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146310\n* #146308\n\n",
        "open",
        "2025-02-03T14:11:18Z",
        null,
        null,
        "release notes: distributed (c10d)",
        "gh/lw/7/base",
        "gh/lw/7/head",
        1,
        35,
        4,
        1,
        1,
        1,
        "yifuwang",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146310"
    ],
    [
        146308,
        "Support SymmetricMemory's signaling kernels on sm60 and sm70",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #146310\n* __->__ #146308\n\nBy leveraging libcudacxx's utilities: https://nvidia.github.io/cccl/libcudacxx/extended_api/synchronization_primitives/atomic_ref.html\n\ncc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o",
        "open",
        "2025-02-03T13:35:20Z",
        null,
        null,
        "oncall: distributed, release notes: distributed (c10d)",
        "gh/lw/6/base",
        "gh/lw/6/head",
        3,
        36,
        58,
        1,
        1,
        0,
        "yifuwang",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146308"
    ],
    [
        146290,
        "[ c10d ] modify API to get device string from device with torch.device",
        "Modify the ```get_default_backend_for_device()``` API to extract the device string using ```torch.device()```\r\n\n\ncc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o",
        "open",
        "2025-02-03T03:24:49Z",
        null,
        null,
        "oncall: distributed, triaged, open source, release notes: distributed (c10d)",
        "main",
        "c10d_api_modification",
        1,
        1,
        1,
        1,
        2,
        0,
        "guangyey",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146290"
    ],
    [
        146289,
        "Use device agnostic APIs for device_count and backend in common_fsdp",
        "Replace device specific APIs with device abstracted API\r\n",
        "open",
        "2025-02-03T03:10:30Z",
        null,
        null,
        "triaged, open source, topic: not user facing",
        "main",
        "fsdp_common_cleanup",
        1,
        8,
        8,
        1,
        6,
        0,
        "guangyey",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146289"
    ],
    [
        146288,
        "[Trace PyDispatcher] Capture Vmapped autograd function as graph",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146288\n* #146272\n* #146271\n* #146270\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-02-03T02:14:22Z",
        null,
        null,
        "topic: not user facing, module: dynamo, ciflow/inductor",
        "gh/yanboliang/62/base",
        "gh/yanboliang/62/head",
        5,
        275,
        3,
        3,
        1,
        2,
        "yanboliang, zou3519, zou3519, zou3519, zou3519, zou3519, yanboliang, yanboliang, yanboliang, yanboliang, yanboliang, yanboliang, yanboliang, zou3519, zou3519, zou3519",
        "COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146288"
    ]
][
    [
        146764,
        "Fix standalone runner for CUTLASS auto-tuning backend",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146764\n* #146755\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov",
        "open",
        "2025-02-08T17:23:45Z",
        null,
        null,
        "open source, topic: not user facing, module: inductor, ciflow/inductor",
        "gh/alexsamardzic/26/base",
        "gh/alexsamardzic/26/head",
        1,
        64,
        8,
        2,
        2,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146764"
    ],
    [
        146763,
        "[Break XPU] Align meta calculation for fft_r2c with _fft_r2c_mkl",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146763\n* #145248\n* #146762\n\nFix #146761",
        "open",
        "2025-02-08T16:20:39Z",
        null,
        null,
        "open source, ciflow/trunk, topic: not user facing, ciflow/inductor, ciflow/xpu",
        "gh/etaf/98/base",
        "gh/etaf/98/head",
        1,
        3,
        1,
        1,
        1,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146763"
    ],
    [
        146762,
        "[Break XPU][Inductor UT] Fix XPU Inductor UT introduced from community.",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #146763\n* #145248\n* __->__ #146762\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov",
        "open",
        "2025-02-08T16:20:35Z",
        null,
        null,
        "open source, topic: not user facing, module: inductor, ciflow/inductor",
        "gh/etaf/97/base",
        "gh/etaf/97/head",
        4,
        10,
        2,
        1,
        1,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146762"
    ],
    [
        146756,
        "[Inductor][CPU] Add GEMM tamplates for _weight_int4pack_mm_for_cpu with AVX512",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146756\n* #145250\n* #145245\n\r\n**Summary**\r\nIt's part of the task to enable max-autotune with GEMM template for WoQ INT4 GEMM on CPU. Early work: #133846\r\n\r\nThis PR adds GEMM templates for `torch.ops.aten_weight_int4pack_mm_for_cpu`. The micro kernel used for the templates is based on AVX512 and it's a copy of the ATen implementation of `torch.ops.aten_weight_int4pack_mm_for_cpu` with minor changes.\r\n\r\nDue to better blocking and loop schedule, the GEMM template based implementation outperforms the ATen implementation in all cases we tested.\r\n\r\n**Test plan**\r\n```\r\npython test/inductor/test_cpu_select_algorithm.py -k test_int4_woq_mm_avx512\r\n```\r\n\r\ncc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @voznesenskym @penguinwu @EikanWang @Guobing-Chen @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov",
        "open",
        "2025-02-08T13:51:09Z",
        null,
        null,
        "open source, ciflow/trunk, topic: not user facing, intel, module: inductor, ciflow/inductor",
        "gh/Xia-Weiwen/30/base",
        "gh/Xia-Weiwen/30/head",
        8,
        468,
        21,
        2,
        1,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146756"
    ],
    [
        146755,
        "Fix CUTLASS 2.x kernels for auto-tuning",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #146764\n* __->__ #146755\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov",
        "open",
        "2025-02-08T12:01:14Z",
        null,
        null,
        "open source, topic: not user facing, module: inductor, ciflow/inductor",
        "gh/alexsamardzic/25/base",
        "gh/alexsamardzic/25/head",
        3,
        12,
        30,
        1,
        1,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146755"
    ],
    [
        146754,
        "[MPS] fix inverse bug for N>1024",
        "Fixes #138200 \r\n",
        "open",
        "2025-02-08T10:24:29Z",
        null,
        null,
        "open source, release notes: mps",
        "main",
        "mps-inverse-bugfix",
        4,
        26,
        27,
        1,
        1,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146754"
    ],
    [
        146753,
        "[MPS] fix lu factor for large tensors with bs>1",
        "Try this:\r\n```\r\nimport torch\r\n\r\nbatch_size = 2\r\nA = torch.eye(256, device=\"mps\")[None, :, :].expand(batch_size, -1, -1) + 0.1 * torch.randn((batch_size, 256, 256), device=\"mps\")\r\nA_cpu = A.cpu()\r\nLU_cpu, pivots_cpu = torch.linalg.lu_factor(A_cpu)\r\nLU, pivots = torch.linalg.lu_factor(A)\r\ntorch.testing.assert_close(LU.cpu(), LU_cpu)\r\n```\r\nYou'll get huge difference in LU tensors\r\n<img width=\"706\" alt=\"Screenshot 2025-02-08 at 12 14 39\" src=\"https://github.com/user-attachments/assets/b45f2b3c-e0a5-49c8-aa07-42792150b781\" />\r\n",
        "open",
        "2025-02-08T08:15:29Z",
        null,
        null,
        "open source, release notes: mps",
        "main",
        "lu-factor-fix-batches",
        2,
        7,
        5,
        1,
        1,
        0,
        "Isalia20",
        "COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146753"
    ],
    [
        146752,
        "realize stride symbols in estimate_runtime",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\r\n* __->__ #146752\r\n\r\nUnfortuanlty could not create a local repo, or unit test.\r\nfix https://github.com/pytorch/pytorch/issues/146686\r\n\r\n",
        "open",
        "2025-02-08T07:30:03Z",
        null,
        null,
        "topic: not user facing, ciflow/inductor",
        "gh/laithsakka/107/base",
        "gh/laithsakka/107/head",
        1,
        2,
        1,
        1,
        1,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146752"
    ],
    [
        146751,
        "[MTIA] (4/n) Implement PyTorch APIs to query/reset device peak memory usage",
        "Summary: Public summary (shared with Github): This diff updates the unit test for the PyTorch API \"reset_peak_memory_stats\".\n\nTest Plan:\n```\nbuck2 test //mtia/host_runtime/torch_mtia/tests:test_torch_mtia_api -- -r test_reset_peak_memory_stats\n```\n\nhttps://www.internalfb.com/intern/testinfra/testrun/9007199321947161\n\nReviewed By: yuhc\n\nDifferential Revision: D68989900\n\n\n",
        "open",
        "2025-02-08T07:16:36Z",
        null,
        null,
        "fb-exported",
        "main",
        "export-D68989900",
        2,
        17,
        0,
        1,
        3,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146751"
    ],
    [
        146750,
        "Update instructions about faster linker",
        "This PR adds instructions to specify linker via cmake env `CMAKE_LINKER_TYPE` and also adds `mold` as a linker alternative.\r\n\r\nSince 3.29, cmake introduced [`CMAKE_LINKER_TYPE`](https://cmake.org/cmake/help/latest/variable/CMAKE_LINKER_TYPE.html) that can specify linker without overwriting `ld` file or changing build script.\r\n\r\n`mold` is already stable and **the fastest** (afaict) linker out there, and also easier to install compared with `lld`. So I added it here. After switching to `mold`, the time of linking `libtorch_cuda.so` has been reduced from ~7s to ~0.6s locally.\r\n\r\nAlso note `gold` has been marked deprecated recently[1].\r\n\r\n[1] https://lwn.net/Articles/1007541/",
        "open",
        "2025-02-08T06:30:17Z",
        null,
        null,
        "open source, topic: not user facing",
        "main",
        "update-linker",
        1,
        6,
        8,
        2,
        1,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146750"
    ],
    [
        146748,
        "Update strided test to float32",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\r\n* __->__ #146748\r\n\r\nFixes #146377\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov",
        "open",
        "2025-02-08T04:48:51Z",
        null,
        null,
        "ciflow/trunk, topic: not user facing, module: inductor, ciflow/inductor",
        "gh/drisspg/122/base",
        "gh/drisspg/122/head",
        1,
        3,
        3,
        1,
        6,
        1,
        "BoyuanFeng, leijurv",
        "APPROVED, APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146748"
    ],
    [
        146747,
        "Add hint message for `pack_padded_sequence`",
        "Fixes #144207\r\n\r\nAdd truncate hint message in docs [torch.nn.utils.rnn.pack_padded_sequence](https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pack_padded_sequence.html)\r\n\r\n## Test Result\r\n\r\n![image](https://github.com/user-attachments/assets/46258f36-f6c7-4f11-9213-8513e52a9001)\r\n\r\n",
        "open",
        "2025-02-08T04:28:15Z",
        null,
        null,
        "open source, topic: not user facing",
        "main",
        "opt/nn/rnn",
        1,
        5,
        1,
        1,
        2,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146747"
    ],
    [
        146746,
        "[Inductor] Fix the lowering of squeeze when input is not contiguous",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146746\n\r\n**Summary**\r\nFix issue https://github.com/pytorch/pytorch/issues/143498. The issue happens when we lowering `select = torch.ops.aten.select.int(cat, 1, 0)`. \r\n\r\nFor example, when `cat` is contiguous with size[2, 2] stride[2,1]\r\n\r\n- for eager, it returns a view of size[2,] stride[2,]\r\n- for Inductor lowering, it returns wrong stride 1 instead of 2\r\n```\r\nTensorBox(\r\n  ReinterpretView(\r\n    StorageBox(\r\n      ConcatKernel(name='buf10', layout=FixedLayout('cpu', torch.int64, size=[u0, 2], stride=[2, 1]), inputs=[ComputedBuffer(name='buf8', layout=NonOwningLayout('cpu', torch.int64, size=[u0, 1], stride=[2, 1]), data=Pointwise(device=device(type='cpu'), dtype=torch.int64, inner_fn=<function ReinterpretView.make_loader.<locals>.loader at 0x7f6b856449d0>, ranges=[u0, 1])), ComputedBuffer(name='buf9', layout=NonOwningLayout('cpu', torch.int64, size=[u0, 1], stride=[2, 1]), data=Pointwise(device=device(type='cpu'), dtype=torch.int64, inner_fn=<function ReinterpretView.make_loader.<locals>.loader at 0x7f6b85644790>, ranges=[u0, 1]))])\r\n    ),\r\n    FixedLayout('cpu', torch.int64, size=[u0], stride=[**1**]),\r\n    origins=OrderedSet([select])\r\n  )\r\n)\r\n```\r\n\r\nTo fix this issue, we give the right stride when lowering of `squeeze`.\r\n\r\n**Test Plan**\r\n```\r\npython -u -m pytest -s -v test/inductor/test_unbacked_symints.py -k test_issue_143498\r\n```\r\n\r\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov",
        "open",
        "2025-02-08T03:50:25Z",
        null,
        null,
        "open source, ciflow/trunk, topic: not user facing, module: inductor, ciflow/inductor",
        "gh/leslie-fang-intel/180/base",
        "gh/leslie-fang-intel/180/head",
        2,
        16,
        4,
        2,
        1,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146746"
    ],
    [
        146743,
        "[cutlass backend] refactor tests to remove duplicate logic",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146743\n* #146356\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov",
        "open",
        "2025-02-08T01:36:44Z",
        null,
        null,
        "topic: not user facing, module: inductor, ciflow/inductor",
        "gh/henrylhtsang/3/base",
        "gh/henrylhtsang/3/head",
        1,
        57,
        81,
        1,
        1,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146743"
    ],
    [
        146742,
        "[Dynamo][autograd.Function] Relax backward speculation strict mode: support .requires_grad",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146742\n* #146741\n* #146571\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-02-08T01:30:53Z",
        null,
        null,
        "topic: not user facing, module: dynamo, ciflow/inductor",
        "gh/yanboliang/65/base",
        "gh/yanboliang/65/head",
        2,
        27,
        1,
        1,
        1,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146742"
    ],
    [
        146741,
        "[Dynamo][autograd.Function] Relax backward speculation strict mode: support .data",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #146742\n* __->__ #146741\n* #146571\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-02-08T01:30:49Z",
        null,
        null,
        "topic: not user facing, module: dynamo, ciflow/inductor",
        "gh/yanboliang/64/base",
        "gh/yanboliang/64/head",
        2,
        29,
        1,
        1,
        1,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146741"
    ],
    [
        146739,
        "Testing",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146739\n* #145748\n\nThis reverts commit 5cd5b4d2d54c0220b92ee488dd36d789c9b60af3.",
        "open",
        "2025-02-08T00:30:57Z",
        null,
        null,
        "release notes: releng, ciflow/binaries_wheel",
        "gh/mikaylagawarecki/312/base",
        "gh/mikaylagawarecki/312/head",
        8,
        25,
        80,
        2,
        1,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146739"
    ],
    [
        146738,
        "[audio hash update] update the pinned audio hash",
        "This PR is auto-generated nightly by [this action](https://github.com/pytorch/pytorch/blob/main/.github/workflows/nightly.yml).\nUpdate the pinned audio hash.",
        "open",
        "2025-02-08T00:23:09Z",
        null,
        null,
        "open source, ciflow/trunk, topic: not user facing, ciflow/inductor, merging",
        "main",
        "update-audio-commit-hash/13210264744-1454-1",
        1,
        1,
        1,
        1,
        4,
        0,
        "pytorchbot",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146738"
    ],
    [
        146737,
        "[dynamo][user-defined] Unify standard and non-standard __new__ codebase",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #146678\n* __->__ #146737\n* #146677\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-02-08T00:21:25Z",
        null,
        null,
        "ciflow/trunk, topic: not user facing, module: dynamo, ciflow/inductor, keep-going",
        "gh/anijain2305/676/base",
        "gh/anijain2305/676/head",
        1,
        17,
        28,
        2,
        1,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146737"
    ],
    [
        146736,
        "Document dynamo",
        "Many files in dynamo are currently lacking file/module-level documentation, which makes it hard to know what they do at a glance and without digging into the code. This fixes that.\r\n\r\nNote: documentation was AI-generated and could be incorrect, please review carefully.\r\n\r\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @xmfan @svekars @brycebortree @sekyondaMeta @AlannaBurke",
        "open",
        "2025-02-08T00:15:41Z",
        null,
        null,
        "better-engineering, ciflow/trunk, topic: not user facing, module: inductor, module: dynamo, ciflow/inductor, module: compiled autograd",
        "main",
        "gh/raymo/document-dynamo",
        30,
        601,
        45,
        1,
        4,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146736"
    ],
    [
        146735,
        "[ca] log graph before reodering passes",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146735\n* #146720\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @yf225",
        "open",
        "2025-02-07T23:39:50Z",
        null,
        null,
        "module: dynamo, ciflow/inductor, module: compiled autograd",
        "gh/xmfan/177/base",
        "gh/xmfan/177/head",
        1,
        12,
        0,
        1,
        2,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146735"
    ],
    [
        146734,
        "[CUDA][CUDNN][SDPA] Pass dropout seed and offset to cuDNN in `int64`",
        "Workaround for limitation in cuDNN that does not accept dropout seed/offset in `int32` for SM 10.0 kernels.\n\ncc @csarofeen @ptrblck @xwang233 @msaroufim",
        "open",
        "2025-02-07T23:30:33Z",
        null,
        null,
        "module: cudnn, module: cuda, triaged, open source, topic: not user facing, module: sdpa",
        "main",
        "cudnnsm100dropoutworkaround",
        1,
        31,
        12,
        1,
        1,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146734"
    ],
    [
        146733,
        "[CUDA][SDPA] Don't dispatch to mem eff attn for batch_size >= 65536",
        "#146704\n\ncc @ptrblck @msaroufim",
        "open",
        "2025-02-07T23:29:27Z",
        null,
        null,
        "module: cuda, open source, topic: not user facing, module: sdpa",
        "main",
        "memeff65536",
        2,
        23,
        0,
        1,
        2,
        0,
        "drisspg, drisspg, nikitaved",
        "COMMENTED, APPROVED, COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146733"
    ],
    [
        146731,
        "dont specialize symints when testing truthiness",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #133044\n* #146729\n* #146642\n* __->__ #146731\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-02-07T22:48:49Z",
        null,
        null,
        "module: dynamo, ciflow/inductor",
        "gh/bdhirsh/641/base",
        "gh/bdhirsh/641/head",
        2,
        20,
        1,
        1,
        3,
        1,
        "bobrenjc93",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146731"
    ],
    [
        146730,
        "[BaseHOP] change hop(subgraph, operands) to hop(subgraph, *operands)",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146730\n* #146727\n\nOur three main users are OK with this, with two of them (foreach_map,\ninvoke_quant) prefering it like this.\n\nI was originally worried about BC issues (this now means you cannot add\nany positional args) but I think that's not a concern -- one can always\nadd kwonly args.\n\nTest Plan\n- tests\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov",
        "open",
        "2025-02-07T22:42:43Z",
        null,
        null,
        "release notes: foreach_frontend, module: inductor, module: dynamo, ciflow/inductor",
        "gh/zou3519/1130/base",
        "gh/zou3519/1130/head",
        6,
        35,
        48,
        1,
        1,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146730"
    ],
    [
        146729,
        "support meta_tensor.to(device='cpu') under fake_mode",
        "Fixing this is actually a bit annoying:\r\n\r\n(1) FakeTensorMode sees a function where all of its inputs are real tensors, so it tries to run the real compute before converting the output to a FakeTensor\r\n\r\n(2) we don't actually want this, because the \"real compute\" is support to error normally, when you do `meta_tensor.to(device='cpu')`. Instead, we want FakeTensor to actually skip constant prop and run the normal FakeTensor implementation, which will not error\r\n\r\nStack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #133044\n* __->__ #146729\n* #146642\n* #146731\n\r\n",
        "open",
        "2025-02-07T22:19:32Z",
        null,
        null,
        "ciflow/inductor",
        "gh/bdhirsh/640/base",
        "gh/bdhirsh/640/head",
        2,
        12,
        0,
        2,
        2,
        0,
        "zou3519, zou3519, bdhirsh",
        "COMMENTED, APPROVED, COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146729"
    ],
    [
        146728,
        "[StaticRuntime] Fix a bug that memory planner ignores subblocks",
        "Summary: When Static Runtime graph node has sub-blocks, the memory planner does not consider sub-blocks' inputs as a node's input in memory planner. As the result, such nodes' inputs' lifetime is incorrect and corresponding tensor memory is released earlier than required and causes errors.\n\nDifferential Revision: D69195886\n\n\n\n\ncc @EikanWang @jgong5 @wenzhe-nrv @sanchitintel",
        "open",
        "2025-02-07T22:13:28Z",
        null,
        null,
        "oncall: jit, fb-exported, release notes: jit",
        "main",
        "export-D69195886",
        3,
        75,
        8,
        1,
        7,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146728"
    ],
    [
        146727,
        "Rename PrimHOPBase to BaseHOP + minor changes",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #146730\n* __->__ #146727\n\nThis PR:\n- renames PrimHOPBase to BaseHOP\n- changes the backward pass to always return a tuple (to match the\n  forward pass).\n\nTest Plan:\n- tests\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-02-07T22:00:22Z",
        null,
        null,
        "release notes: foreach_frontend, module: dynamo, ciflow/inductor",
        "gh/zou3519/1129/base",
        "gh/zou3519/1129/head",
        6,
        18,
        18,
        1,
        1,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146727"
    ],
    [
        146726,
        "[ez][BE] get rid of the extra printf('\\n')",
        "Summary: as title\n\nTest Plan:\n```\nAOT_INDUCTOR_DEBUG_INTERMEDIATE_VALUE_PRINTER=3  TORCHINDUCTOR_FORCE_DISABLE_CACHES=1 TORCHINDUCTOR_ABI_COMPATIBLE=1 TORCH_COMPILE_DEBUG=1 TORCH_LOGS=\"+graph, inductor, +schedule, output_code\" buck2 run -c fbcode.enable_gpu_sections=true -c fbcode.nvcc_arch=h100a @//mode/opt fbcode//caffe2/test/inductor:test_aot_inductor -- -r test_addmm_cuda\n```\n\nDifferential Revision: D69328701\n\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov",
        "open",
        "2025-02-07T21:57:49Z",
        null,
        null,
        "fb-exported, ciflow/trunk, topic: not user facing, module: inductor, ciflow/inductor, merging",
        "main",
        "export-D69328701",
        1,
        1,
        2,
        1,
        5,
        0,
        "ColinPeppler",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146726"
    ],
    [
        146723,
        "torch: Log a unified waitcounter for torch.compile and triton.autotune",
        "Summary: Add a second more generic waitcounter to torch.compile. We'll keep expanding this as new generic pytorch compilation sites show up.\n\nTest Plan: Waitcounter only change, relying on existing tests.\n\nDifferential Revision: D69215401\n\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov",
        "open",
        "2025-02-07T20:59:32Z",
        null,
        null,
        "fb-exported, ciflow/trunk, topic: not user facing, module: inductor, ciflow/inductor",
        "main",
        "export-D69215401",
        1,
        3,
        1,
        1,
        3,
        0,
        "davidberard98",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146723"
    ],
    [
        146722,
        "Test on in-graph constructed NJTs",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146722\n* #146721\n\r\nA recent set of bugs has been cropping up related to NJTs that constructed in-graph within a compiled function. This exercises different paths related to symbolic nested ints, etc. Some examples:\r\n* #145874\r\n* #146644\r\n\r\nTo get ahead of these, we should do NJT testing for this case as well.\r\n\r\nThis PR parametrizes the OpInfo tests for compile + forward to cover both in-graph constructed NJT and normal input cases. TBD what fails..\r\n\r\nTODO:\r\n* Do this for compile + backward tests also (?)",
        "open",
        "2025-02-07T20:09:15Z",
        null,
        null,
        "topic: not user facing",
        "gh/jbschlosser/229/base",
        "gh/jbschlosser/229/head",
        2,
        44,
        6,
        2,
        1,
        0,
        "jbschlosser, cpuhrsch, soulitzer",
        "COMMENTED, APPROVED, APPROVED",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146722"
    ],
    [
        146721,
        "Use inductor backend for NJT compile tests",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #146722\n* __->__ #146721\n\r\nWe've been using `backend=\"aot_eager_decomp_partition\"` for NJT compile testing, but this can let inductor bugs slip through. This PR switches the compile tests to use `backend=\"inductor\"`; let's see if test runtime is an issue after this.",
        "open",
        "2025-02-07T20:09:11Z",
        null,
        null,
        "topic: not user facing",
        "gh/jbschlosser/228/base",
        "gh/jbschlosser/228/head",
        1,
        3,
        7,
        2,
        1,
        0,
        "soulitzer",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146721"
    ],
    [
        146720,
        "[ca] remove private API: _compiled_autograd_should_lift",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #146735\n* __->__ #146720\n\r\nSince the functional autograd + compiled autograd migration, we don't trace into nodes anymore, and everything is lifted. We can't support this flag which tries to inline make_fx style in CA initial pass. There's no more usage internally.",
        "open",
        "2025-02-07T20:06:08Z",
        null,
        null,
        "ciflow/trunk, ciflow/inductor, release notes: dynamo",
        "gh/xmfan/176/base",
        "gh/xmfan/176/head",
        4,
        0,
        15,
        1,
        7,
        0,
        "zou3519",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146720"
    ],
    [
        146718,
        "[CUDAGraph] add skip message for unbacked symint",
        "Add explicit skip message for unbacked symint in cudagraph, as suggested by @bdhirsh.\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov",
        "open",
        "2025-02-07T19:29:37Z",
        null,
        null,
        "ciflow/trunk, topic: not user facing, module: inductor, module: dynamo, ciflow/inductor",
        "main",
        "bf/cg-skip-unbacked-symint-msg",
        4,
        31,
        14,
        1,
        1,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146718"
    ],
    [
        146717,
        "[BE][cuDNN] cuDNN to 9.7.1.26 for CUDA 12.8",
        "cuDNN 9.7.1 is out now and is expected to be the longer-lived branch with more potential backports vs. 9.7.0\r\n\r\nCC @nWEIdia @tinglvv \n\ncc @malfet @seemethere @csarofeen @ptrblck @xwang233",
        "open",
        "2025-02-07T19:25:00Z",
        null,
        null,
        "module: build, module: cudnn, triaged, open source, topic: not user facing, topic: build",
        "main",
        "cudnn971",
        6,
        14,
        12,
        1,
        1,
        0,
        "drisspg",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146717"
    ],
    [
        146716,
        "[BE] Remove outdated RPC benchmark",
        "We have lots of outdated unused + uncalled code in our codebase, namely in our benchmarks and examples folders among others. The last change to this directory was 4 years ago and this code looks dead. cc @albanD @H-Huang for feedback\r\n\r\nStack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\r\n* __->__ #146716\r\n\r\n",
        "open",
        "2025-02-07T18:52:53Z",
        null,
        null,
        "release notes: distributed (rpc), skip-pr-sanity-checks",
        "gh/janeyx99/223/base",
        "gh/janeyx99/223/head",
        29,
        0,
        2535,
        1,
        1,
        0,
        "Skylion007, H-Huang",
        "APPROVED, APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146716"
    ],
    [
        146715,
        "[export][ez] Allow math.trunc for serialization.",
        "Summary: as title.\n\nTest Plan: CI\n\nDifferential Revision: D69317084\n\n\n",
        "open",
        "2025-02-07T18:24:55Z",
        null,
        null,
        "fb-exported, ciflow/trunk, ciflow/inductor, release notes: export",
        "main",
        "export-D69317084",
        1,
        1,
        0,
        1,
        2,
        0,
        "angelayi",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146715"
    ],
    [
        146714,
        "[hop] Support more output types for `flat_apply`",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #146367\n* __->__ #146714\n* #146713\n\nThis patch enables `flat_apply` to support certain non-Tensor output\ntypes like containers and graphable types. This will in turn enable the\nupcoming `mark_traceable` to support more output types.\n\nThe patch also exposes a `func_to_graphable` rather than having the\nusers calling the lower level `pytree.flatten(ConstantFunction(...))`.\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-02-07T17:43:34Z",
        null,
        null,
        "module: dynamo, ciflow/inductor",
        "gh/StrongerXi/83/base",
        "gh/StrongerXi/83/head",
        2,
        54,
        21,
        1,
        2,
        0,
        "zou3519",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146714"
    ],
    [
        146713,
        "[dynamo][fx] Support dataclass whose fields have `init=False`",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* (to be filled)\n\nPreviously Dynamo and FX have code paths that reconstruct a dataclass\ninstance based on its type and fields; however they weren't taking\n`init=False` into account (which is supposed to exclude the field from\nconstructor).\n\nThis patch fixes that, and also updates `pytree.LeafSpec` so that its\n`__init__` conforms with the `init` attribute of its fields. Without\nthis change, the aforementioned reconstruction logic would fail.\n\ncc @ezyang @SherlockNoMad @EikanWang @jgong5 @wenzhe-nrv @voznesenskym @penguinwu @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-02-07T17:43:29Z",
        null,
        null,
        "release notes: fx, fx, module: dynamo, ciflow/inductor",
        "gh/StrongerXi/82/base",
        "gh/StrongerXi/82/head",
        4,
        61,
        9,
        1,
        1,
        0,
        "Skylion007",
        "COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146713"
    ],
    [
        146710,
        "[MTIA] (3/n) Implement PyTorch APIs to query/reset device peak memory usage",
        "Summary: Public summary (shared with Github): This diff implements a C++-Python binding to enable `reset_peak_memory_stats`.\n\nTest Plan: The test is implemented in the following diff.\n\nReviewed By: yuhc\n\nDifferential Revision: D68988673\n\n\n",
        "open",
        "2025-02-07T16:52:00Z",
        null,
        null,
        "fb-exported, ciflow/trunk, topic: not user facing, merging",
        "main",
        "export-D68988673",
        3,
        9,
        1,
        1,
        6,
        0,
        "nautsimon",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146710"
    ],
    [
        146709,
        "FSDP: avoid resetting version counter of all_gather_output in inference_mode",
        "Summary:\nFSDP needs to hide VC bumps on its allgather buffer, but it does not need to do this is the allgather buffer was generated under inference mode.\n\nmore details here: https://www.internalfb.com/diff/D69115649?dst_version_fbid=1316814572779281&transaction_fbid=849120230625711\n\nTest Plan: CI\n\nDifferential Revision: D69311496\n\n\n\n\ncc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o",
        "open",
        "2025-02-07T16:39:54Z",
        null,
        null,
        "oncall: distributed, fb-exported, ciflow/trunk, release notes: distributed (fsdp), ciflow/inductor, merging",
        "main",
        "export-D69311496",
        1,
        9,
        1,
        1,
        6,
        0,
        "awgu",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146709"
    ],
    [
        146706,
        "cpp_wrapper: persist autotune example tensors until last use",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\r\n* #146452\r\n* __->__ #146706\r\n* #146424\r\n* #146109\r\n* #146449\r\n* #144349\r\n* #144293\r\n* #144002\r\n\r\nPatches over an issue where randomly generated example tensors can cause kernel autotuning to fail, when those tensors would not be possible outputs from previous kernels in the sequence. This fixes a failure in `test_torchinductor_opinfo.py` when run with compile-time autotuning, `test_comprehensive_nanquantile_cuda_float64`.\r\n\r\nFor clarity, the situation triggering this PR looks like kernels `A -> BCDE -> F` (`BCDE` is fused), where one of the outputs from `A` is a boolean tensor describing some of the input data. Previously, we randomly regenerated that boolean tensor and the input data before passing them to `BCDE`, so that they no longer matched. This caused a `tl.device_assert` call in `BCDE` to fail. With this PR, we reuse the random data input to `A` and the output Boolean tensor, such that they match and pass the device assertion in `BCDE`.\r\n\r\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @desertfire @chauhang @aakhundov",
        "open",
        "2025-02-07T16:08:21Z",
        null,
        null,
        "open source, topic: not user facing, module: inductor, ciflow/inductor",
        "gh/benjaminglass1/67/base",
        "gh/benjaminglass1/67/head",
        1,
        28,
        8,
        2,
        2,
        0,
        "benjaminglass1",
        "COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146706"
    ],
    [
        146705,
        "Remove NO_MULTIPROCESSING_SPAWN checks",
        "py 3.9 has spawn.\r\n\r\ncc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o",
        "open",
        "2025-02-07T15:19:32Z",
        null,
        null,
        "oncall: distributed, triaged, open source, release notes: distributed (torchelastic)",
        "main",
        "NO_MULTIPROCESSING_SPAWN",
        11,
        23,
        156,
        1,
        1,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146705"
    ],
    [
        146696,
        "Add full_like.default to list of ops with kwargs",
        "The _maybe_insert_input_observers_for_node function expects ops, except a few exceptions, to have zero kwargs. full_like.default seems to be one of these cases and should therefore be added to the list.\r\n\r\nAddresses https://github.com/pytorch/pytorch/issues/146621\r\n\r\nFixes #146621 \r\n",
        "open",
        "2025-02-07T11:34:31Z",
        null,
        null,
        "triaged, open source, release notes: quantization, release notes: AO frontend",
        "main",
        "main",
        2,
        3,
        2,
        1,
        2,
        1,
        "Xia-Weiwen",
        "COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146696"
    ],
    [
        146695,
        "Enable Windows tests",
        "Fixes #ISSUE_NUMBER\r\n\n\ncc @peterjc123 @mszhanyi @skyline75489 @nbcsm @iremyux @Blackhex",
        "open",
        "2025-02-07T11:34:26Z",
        null,
        null,
        "module: windows, triaged, open source, topic: not user facing",
        "main",
        "win_test294",
        3,
        1,
        23,
        2,
        1,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146695"
    ],
    [
        146692,
        "[clang-tidy] Add suppression clang-diagnostic-shadow",
        "Summary:\r\nReviewed By: varun2784\r\n\r\nDifferential Revision: D69182465\r\n\r\n\r\n",
        "open",
        "2025-02-07T10:06:04Z",
        null,
        null,
        "fb-exported, topic: not user facing",
        "main",
        "export-D69182465",
        1,
        2,
        1,
        1,
        8,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146692"
    ],
    [
        146690,
        "Enable pt2e quantization path for arm",
        "**Title**: Enable PyTorch 2 Export Quantization path for ARM CPUs.\r\n\r\n**Description:**\r\n - This PR extends the PyTorch 2 Export Quantization (PT2E Quantization) workflow\u2014originally available only on x86 CPUs\u2014to support ARM platforms. PT2E Quantization is an automated, full-graph quantization solution in PyTorch that improves on Eager Mode Quantization by adding support for functionals and automating the overall process. It is part of the torch.ao module and fully supports quantization when using the compile mode.\r\n\r\n**Key Changes:**\r\n\r\n - Introduces ARM-specific support by leveraging oneDNN kernels for matmuls and convolution.\r\n\r\n - Integrates pre-defined configuration selection to automatically choose the best quantization settings based on the selected quantization method.\r\n\r\n**Provides customization options via two flags:**\r\n\r\n - **qat_state:** Indicates whether to use Quantization Aware Training (if set to True) or Post Training Quantization (if set to False). The default remains False.\r\n - **dynamic_state:** Selects between dynamic quantization (if True) and static quantization (if False). The default is also set to False.\r\n![Screenshot 2025-01-22 105543](https://github.com/user-attachments/assets/c611a1ce-9274-4b70-9c58-cae96000d06d)\r\n\r\nThese options allow users to tailor the quantization process for their specific workload requirements (e.g., using QAT for fine-tuning or PTQ for calibration-based quantization).\r\n\r\nTesting and Validation:\r\n\r\nThe new ARM flow has been thoroughly tested across a range of models with all combinations:\r\n**NLP**: Models such as BERT and T5.\r\n**Vision**: Models like ResNet and ViT.\r\n**Custom Models**: user defined models with various operators.\r\n\r\nexample script:\r\n```\r\nimport torch\r\nimport torchvision.models as models\r\nfrom torch.ao.quantization.quantize_pt2e import prepare_pt2e, convert_pt2e\r\nimport torch.ao.quantization.quantizer.arm_inductor_quantizer as armiq\r\nfrom torch.ao.quantization.quantizer.arm_inductor_quantizer import ArmInductorQuantizer\r\nfrom torch.profiler import profile, record_function, ProfilerActivity\r\n\r\nmodel_name = \"resnet50\"\r\nmodel = models.__dict__[model_name](pretrained=True)\r\n\r\n# Set the model to eval mode\r\nmodel = model.eval()\r\n\r\n# Create the data, using the dummy data here as an example\r\ntraced_bs = 500\r\nx = torch.randn(traced_bs, 3, 224, 224).contiguous(memory_format=torch.channels_last)\r\nexample_inputs = (x,)\r\n\r\nwith torch.no_grad():\r\n    exported_model = torch.export.export_for_training(model, example_inputs).module()\r\n    quantizer = armiq.ArmInductorQuantizer()\r\n    quantizer.set_global(armiq.get_default_arm_inductor_quantization_config(is_dynamic=False))\r\n    prepared_model = prepare_pt2e(exported_model, quantizer)\r\n    converted_model = convert_pt2e(prepared_model)\r\n\r\n    with torch.set_grad_enabled(False):\r\n        for _ in range(50):\r\n            converted_model(*example_inputs) #Warmup\r\n        print(\"Warmup over\")\r\n        with profile(activities=[ProfilerActivity.CPU], record_shapes=True) as prof:\r\n            with record_function(\"model_inference\"):\r\n                for _ in range(100):\r\n                    converted_model(*example_inputs)\r\n\r\n    print(prof.key_averages(group_by_input_shape=True).table(sort_by=\"self_cpu_time_total\"))\r\n\r\n```\n\ncc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @malfet @snadampal @milpuz01",
        "open",
        "2025-02-07T09:23:48Z",
        null,
        null,
        "module: cpu, triaged, open source, module: arm, release notes: quantization, release notes: AO frontend",
        "main",
        "devang/pt2e_quantization_arm",
        2,
        1592,
        0,
        1,
        7,
        1,
        "jerryzh168",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146690"
    ],
    [
        146689,
        "Update addbmm, addmm, addmv and baddbmm description",
        "Fixes #146611, following #146482\r\n\r\n## Test Result\r\n\r\n![image](https://github.com/user-attachments/assets/5c1749be-1f10-4e80-a284-b1929ca340eb)\r\n",
        "open",
        "2025-02-07T08:57:51Z",
        null,
        null,
        "triaged, open source, release notes: python_frontend",
        "main",
        "opt/docs/add",
        1,
        4,
        4,
        1,
        7,
        0,
        "mikaylagawarecki",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146689"
    ],
    [
        146687,
        "Make GetCPUAllocatorMaybePinned to be Device-Agnostic",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146687\n\n----\n\n- Keep cuda first to perserve BC\n- Remove cuda first if it is possible to have only one accelerator at a time in the future",
        "open",
        "2025-02-07T08:54:31Z",
        null,
        null,
        "open source, topic: not user facing",
        "gh/fffrog/38/base",
        "gh/fffrog/38/head",
        2,
        18,
        15,
        1,
        2,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146687"
    ],
    [
        146684,
        "Optimize LRScheduler docs",
        "Fixes #120735\r\n\r\nAdd more description about [`LRScheduler`](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.LRScheduler.html#torch.optim.lr_scheduler.LRScheduler)\r\n\r\n## Test Result\r\n\r\n### Before\r\n\r\n![image](https://github.com/user-attachments/assets/619c3ea8-5652-4e61-936f-0cb5aa5a326b)\r\n\r\n### After\r\n\r\n![image](https://github.com/user-attachments/assets/174a6ffc-5da2-4837-bf49-2f09f6c7b6ee)\r\n\r\n![image](https://github.com/user-attachments/assets/ae1bc984-49cc-4d5b-8d81-08f460b71361)\r\n\r\ncc @janeyx99\r\n",
        "open",
        "2025-02-07T08:42:57Z",
        null,
        null,
        "triaged, open source, release notes: optim",
        "main",
        "opt/docs/LRScheduler",
        1,
        21,
        2,
        1,
        1,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146684"
    ],
    [
        146678,
        "[dynamo][not ready] polyfill infra for classes",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146678\n* #146737\n* #146677\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-02-07T06:41:35Z",
        null,
        null,
        "module: dynamo, ciflow/inductor",
        "gh/anijain2305/675/base",
        "gh/anijain2305/675/head",
        7,
        247,
        41,
        3,
        2,
        0,
        "XuehaiPan",
        "COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146678"
    ],
    [
        146677,
        "[dynamo][user-defined] User class.__new__ instead of special casing",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #146678\n* #146737\n* __->__ #146677\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-02-07T06:41:32Z",
        null,
        null,
        "ciflow/trunk, topic: not user facing, module: dynamo, ciflow/inductor, keep-going",
        "gh/anijain2305/674/base",
        "gh/anijain2305/674/head",
        6,
        168,
        104,
        3,
        1,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146677"
    ],
    [
        146675,
        "[ROCm] Move ROCm unstable MI300 jobs back to stable",
        "Fixes #145790\r\n\r\nThis PR moves rocm unstable MI300 back to stable. The change to unstable was introduced through this [PR](https://github.com/pytorch/pytorch/pull/145790). This was because the MI300s were failing with a [docker daemon](https://github.com/pytorch/pytorch/actions/runs/13015957622/job/36306779536) issue which has been resolved.\r\n\r\ncc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd @ZainRizvi \r\n\r\ncc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd",
        "open",
        "2025-02-07T05:22:51Z",
        null,
        null,
        "module: rocm, open source, topic: not user facing, ciflow/inductor, ciflow/rocm, ciflow/inductor-rocm",
        "main",
        "patch-8",
        3,
        80,
        70,
        8,
        1,
        0,
        "jithunnair-amd, jithunnair-amd",
        "COMMENTED, COMMENTED",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146675"
    ],
    [
        146671,
        "Update torch-xpu-ops commit pin",
        "Update the torch-xpu-ops commit to [662837e722cbbc0701fbcf6ea9ad6383158cc44e](https://github.com/intel/torch-xpu-ops/commit/662837e722cbbc0701fbcf6ea9ad6383158cc44e), includes:\r\n\r\n- Aten operator coverage improvement\r\n- SYCL kernel optimization\r\n- Nested Tensor OPs support\r\n",
        "open",
        "2025-02-07T03:28:45Z",
        null,
        null,
        "triaged, open source, topic: not user facing, keep-going, ciflow/xpu",
        "main",
        "xyt/xpu_pin_662837e722cbbc0701fbcf6ea9ad6383158cc44e",
        1,
        1,
        1,
        1,
        2,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146671"
    ],
    [
        146669,
        "Optimize inductor `Self` typing",
        "Replace method return type with `Self` typing\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @desertfire @chauhang @aakhundov",
        "open",
        "2025-02-07T01:59:49Z",
        null,
        null,
        "triaged, open source, topic: not user facing, module: inductor",
        "main",
        "opt/inductor/typing",
        3,
        8,
        5,
        1,
        3,
        0,
        "jansel",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146669"
    ],
    [
        146668,
        "Make sure cutlass kernel .cu file has configuration name and nvcc compile command",
        "I think its good to have everything in the .cu file. Especially the nvcc compile command.\r\n\r\nTechnically, the configuration name can be found in the template already. So let me know if you think its not needed. \r\n\r\nDifferential Revision: D69281295\r\n\r\n\r\n\r\n\r\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @desertfire @chauhang @aakhundov",
        "open",
        "2025-02-07T01:54:45Z",
        null,
        null,
        "fb-exported, ciflow/trunk, topic: not user facing, module: inductor, ciflow/inductor, merging",
        "main",
        "export-D69281295",
        2,
        6,
        0,
        1,
        8,
        0,
        "chenyang78",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146668"
    ],
    [
        146664,
        "[Docs] Fix description of `input` in `torch.addbmm()`",
        "Fixes #146613\r\n",
        "open",
        "2025-02-07T01:27:52Z",
        null,
        null,
        "open source, ciflow/trunk, release notes: python_frontend, topic: docs, merging",
        "main",
        "docs/addbmm",
        1,
        1,
        1,
        1,
        10,
        1,
        "mikaylagawarecki",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146664"
    ],
    [
        146662,
        "[Optimus] Bug fix in the select cat aten pass",
        "Summary: Thanks to Shuai for reporting the bug in the pattern. We found there's a typo in the pass, where we should make sure all the selects will go to the cat node.\n\nTest Plan:\nbuck2 test 'fbcode//mode/dev-nosan' fbcode//caffe2/test/inductor:split_cat_fx_aten_passes -- test_select_cat_post_grad\n\n\nBuck UI: https://www.internalfb.com/buck2/2cd0888e-d803-43a8-8530-d97e6bc281b3\nTest UI: https://www.internalfb.com/intern/testinfra/testrun/6192449699305108\nNetwork: Up: 110KiB  Down: 35KiB  (reSessionID-687be0fa-031a-47a0-8780-5ab4cf4bbd94)\nExecuting actions. Remaining     0/4                                                                              6.6s exec time total\nCommand: test.     Finished 2 local\nTime elapsed: 2:12.0s\nTests finished: Pass 2. Fail 0. Fatal 0. Skip 0. Build failure 0\n\nDifferential Revision: D69278487\n\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @desertfire @chauhang @aakhundov",
        "open",
        "2025-02-07T00:50:59Z",
        null,
        null,
        "fb-exported, module: inductor, ciflow/inductor, release notes: inductor, inductor_pattern_match",
        "main",
        "export-D69278487",
        2,
        53,
        35,
        1,
        4,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146662"
    ],
    [
        146661,
        "[cond] Refactor cond_op's signature to take *operands.",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146661\n* #146660\n\r\n\r\nThis is a BC-breaking change for hop's IR schema. Previously, \r\n```python\r\ntorch.cond(pred, true_fn, false_fn, (a, b))\r\n# Old representation\r\ntorch.ops.higher_order.cond(pred, true_gm, false_gm, (a, b))\r\n# New representation:\r\ntorch.ops.higher_order.cond(pred, true_gm, false_gm, a, b)\r\n```\r\nThe benefits of this change is that it's much easier to construct the schema since the tuple is flattened. What's particularly troublesome about previous node is that it's hard to represent the mutation and alias information inside the tuple: we have to change the legacy schema parser and verify (maybe re-purpose) the aliasInfo to supports nested aliasInfo inside tuple/list.\r\n\r\nWe'll also refactor other control flow operators.\r\n\r\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @desertfire @chauhang @aakhundov\r\n\r\nDifferential Revision: [D69279033](https://our.internmc.facebook.com/intern/diff/D69279033)",
        "open",
        "2025-02-07T00:44:09Z",
        null,
        null,
        "ciflow/trunk, topic: not user facing, module: inductor, module: dynamo, ciflow/inductor, keep-going, release notes: export",
        "gh/ydwu4/208/base",
        "gh/ydwu4/208/head",
        13,
        119,
        181,
        3,
        3,
        0,
        "zou3519",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146661"
    ],
    [
        146660,
        "[hop][inductor] don't promote arg type for cond and while_loop",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #146661\n* __->__ #146660\n\r\nHop subgraph codegen assumes arguments's type are not promoted. Otherwise, we might generate wrong kernel.\r\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @desertfire @chauhang @aakhundov\n\nDifferential Revision: [D69279031](https://our.internmc.facebook.com/intern/diff/D69279031)",
        "open",
        "2025-02-07T00:44:03Z",
        null,
        null,
        "ciflow/trunk, topic: not user facing, module: inductor, ciflow/inductor, merging",
        "gh/ydwu4/207/base",
        "gh/ydwu4/207/head",
        1,
        2,
        2,
        2,
        6,
        1,
        "zou3519, zou3519, ydwu4",
        "COMMENTED, APPROVED, COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146660"
    ],
    [
        146658,
        "[HOP] Mutation and alias rework",
        "This PR reworks the way the input mutations and various aliases are checked\r\n\r\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @ydwu4 \r\n",
        "open",
        "2025-02-07T00:28:18Z",
        null,
        null,
        "open source, topic: not user facing, module: dynamo",
        "main",
        "mutation_alias_rework",
        12,
        116,
        100,
        5,
        2,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146658"
    ],
    [
        146656,
        "Optimize isclose() for CPU and GPU by adding specific implementations",
        "`isclose()` is currently quite slow, so this PR adds specific implementations for both CPU and cuda.\r\n\r\nCUDA implementation seeing ~4.9x improvement at 100m elements and ~18.7x improvement at 400m elements\r\n\r\nCPU implementation seeing ~5.7x improvements at 100m elements and ~5.9x improvements at 400m elements \r\n\r\nsimple benchmark used(adapt with CPU as needed):\r\n```python\r\nimport time\r\nimport numpy as np\r\nimport torch\r\n\r\ndef benchmark_isclose(shape1, shape2, num_runs=5):\r\n    tensor1 = torch.randn(shape1, device=\"cuda\")\r\n    tensor2 = tensor1.clone()\r\n    tensor2 += torch.randn_like(tensor2) * 0.001\r\n\r\n    # warn up\r\n    _ = torch.isclose(tensor1, tensor2)\r\n    torch.cuda.synchronize()\r\n\r\n    times = []\r\n    for _ in range(num_runs):\r\n        start_time = time.perf_counter()\r\n\r\n        _ = torch.isclose(tensor1, tensor2)\r\n        torch.cuda.synchronize()\r\n        end_time = time.perf_counter()\r\n        times.append(end_time - start_time)\r\n\r\n    mean_time = np.mean(times)\r\n    std_time = np.std(times)\r\n\r\n    return mean_time, std_time\r\n\r\n\r\ntest_shapes = [\r\n    (10000, 10000),  # 100M elements\r\n    (20000, 20000),  # 400M elements\r\n]\r\n\r\nprint(\"\\nBenchmarking torch.isclose():\")\r\nprint(\"-\" * 50)\r\n\r\nfor shape in test_shapes:\r\n    total_elements = np.prod(shape)\r\n    print(f\"\\nTensor shape: {shape} ({total_elements:,} elements)\")\r\n\r\n    mean_time, std_time = benchmark_isclose(shape, shape)\r\n    print(f\"Mean time: {mean_time*1000:.2f} ms +/- {std_time*1000:.2f} ms\")\r\n    print(f\"Elements per second: {total_elements/mean_time:,.0f}\")\r\n```\r\n\r\n```\r\n(optimized)\r\nTensor shape: (10000, 10000) (100,000,000 elements)\r\nMean time: 2.73 ms \u00b1 0.26 ms\r\nElements per second: 36,611,905,024\r\n\r\nTensor shape: (20000, 20000) (400,000,000 elements)\r\nMean time: 8.98 ms \u00b1 0.28 ms\r\nElements per second: 44,546,604,660\r\n\r\n(unoptimized)\r\nTensor shape: (10000, 10000) (100,000,000 elements)\r\nMean time: 13.48 ms \u00b1 0.28 ms\r\nElements per second: 7,420,814,236\r\n\r\nTensor shape: (20000, 20000) (400,000,000 elements)\r\nMean time: 166.90 ms \u00b1 4.71 ms\r\nElements per second: 2,396,711,992\r\n```\r\ncc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @albanD \r\n\r\n",
        "open",
        "2025-02-07T00:09:02Z",
        null,
        null,
        "module: cpu, triaged, open source",
        "main",
        "feature/isclose-kernels",
        4,
        171,
        57,
        3,
        2,
        1,
        "albanD",
        "COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146656"
    ],
    [
        146655,
        "torch._scaled_mm with MX dtypes",
        "making https://github.com/pytorch/pytorch/pull/145562 work with in-core dtypes\r\n\r\nnot ready for review yet",
        "open",
        "2025-02-07T00:02:24Z",
        null,
        null,
        "ciflow/inductor",
        "gh/vkuzo/2/head",
        "gh/vkuzo/3/head",
        8,
        291,
        79,
        3,
        2,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146655"
    ],
    [
        146654,
        "[DCP] Introduce modules metadata in the storage_meta",
        "Summary: Introduce the list of modules in the storage_meta which is shared between the planner and the storage writer. We will use it to let the storage writer know about the modules in the state dict and create module directories in the checkpoint.\n\nTest Plan: UTs\n\nDifferential Revision: D69154628\n\n\n\n\ncc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o @LucasLLC @MeetVadakkanchery @mhorowitz @pradeepfn @ekr0",
        "open",
        "2025-02-06T23:43:44Z",
        null,
        null,
        "oncall: distributed, fb-exported, module: distributed_checkpoint",
        "main",
        "export-D69154628",
        1,
        2,
        1,
        1,
        3,
        0,
        "mhorowitz",
        "COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146654"
    ],
    [
        146653,
        "windows Magma build for cu128",
        "https://github.com/pytorch/pytorch/issues/145570\r\n\r\nremoving `.ci/pytorch/windows/internal/cuda_install.bat` as it is a duplicate with` .github/scripts/windows/cuda_install.bat`. The later one is the one in use - https://github.com/pytorch/pytorch/pull/146653/files#diff-613791f266f2f7b81148ca8f447b0cd6c6544f824f5f46a78a2794006c78957bR8\r\n\r\ncc @atalman @ptrblck @nWEIdia ",
        "open",
        "2025-02-06T23:33:34Z",
        null,
        null,
        "open source, Merged, Reverted, release notes: releng, ciflow/binaries_wheel, ci-no-td",
        "main",
        "cu128-win-magma",
        5,
        95,
        223,
        4,
        6,
        1,
        "atalman",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146653"
    ],
    [
        146642,
        "[poc] force UntypedStorage.from_buffer(buf) to return meta storage under FakeTensorMode",
        "context here: https://fb.workplace.com/groups/326136610199609/permalink/495389539940981/\r\n\r\nThis PR is an attempt to make it such that if you create a tensor from an external buffer (using `UntypedStorage.from_buffer(buf)`, we can generate a proper fake tensor for you out of the box.\r\n\r\nThe annoying bit is that there are not any dispatcher ops to interpose on and change behavior. So instead, I took the manual C binding and tweaked the storage device to be \"meta' if we see an active fake mode.\r\n\r\nPut \"poc\" in the title since I... think this is hopefully reasonable, but I can be convinced that it's not :)\r\n\r\n```\r\nfrom torch._subclasses.fake_tensor import FakeTensorMode\r\nimport pickle\r\nimport io\r\nimport torch\r\nfrom contextlib import nullcontext\r\n\r\n\r\nuse_fake_tensor = True\r\nwith FakeTensorMode() if use_fake_tensor else nullcontext():\r\n    obj = [1, 2]\r\n    f = io.BytesIO()\r\n    pickle.Pickler(f).dump(obj)\r\n    byte_storage = torch.ByteStorage._from_buffer(f.getvalue())  # type: ignore[attr-defined]\r\n    \r\n    t = torch.ByteTensor(byte_storage)\r\n```\r\n\r\nStack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #133044\n* #146729\n* __->__ #146642\n* #146731\n\r\n",
        "open",
        "2025-02-06T21:50:27Z",
        null,
        null,
        "release notes: composability",
        "gh/bdhirsh/639/base",
        "gh/bdhirsh/639/head",
        2,
        27,
        8,
        4,
        2,
        1,
        "zou3519",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146642"
    ],
    [
        146641,
        "[dim order]  solve broken doc",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146641\n\nDifferential Revision: [D69265340](https://our.internmc.facebook.com/intern/diff/D69265340/)",
        "open",
        "2025-02-06T21:49:03Z",
        null,
        null,
        "fb-exported, ciflow/trunk, topic: not user facing, merging",
        "gh/gasoonjia/2/base",
        "gh/gasoonjia/2/head",
        1,
        3,
        0,
        3,
        8,
        0,
        "svekars, svekars, Jack-Khuu",
        "COMMENTED, APPROVED, APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146641"
    ],
    [
        146640,
        "POC for mixed prec optim frontend",
        "This PR is a prototype for what a frontend for asking for mixed precision can look like torch.optim through set_dtype_policy in optimizer.py.\r\n\r\nThis is not meant to be landable but to start some discussions on what people want/would like to see and to ask if there are things I haven't considered yet.\r\n\r\nThis currently only works with Adam(W)!\r\n\r\nA toy script for how to use:\r\n```\r\nimport torch\r\n\r\nmodel = torch.nn.Sequential(\r\n    torch.nn.Linear(2, 3),\r\n    torch.nn.Sigmoid(),\r\n    torch.nn.Linear(3, 1),\r\n    torch.nn.Sigmoid(),\r\n)\r\nmodel.to(\"cuda\")\r\n\r\noptim = torch.optim.AdamW(model.named_parameters(), foreach=False)\r\nmp_policy = {\r\n    \"exp_avg\": lambda _: torch.bfloat16,\r\n    \"exp_avg_sq\": lambda _: torch.bfloat16,\r\n    \"max_exp_avg_sq\": lambda _: torch.bfloat16,\r\n}\r\noptim.set_dtype_policy(mp_policy)\r\n\r\ni = torch.tensor([0.1, 0.2, 0.3, 0.4, 0.5, 0.6], device=\"cuda\").reshape(3, 2)\r\nl = model(i).sum()\r\nl.backward()\r\n\r\noptim.step()\r\n```\r\n\r\nStack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146640\n\r\n",
        "open",
        "2025-02-06T21:31:50Z",
        null,
        null,
        "release notes: optim",
        "gh/janeyx99/222/base",
        "gh/janeyx99/222/head",
        3,
        113,
        11,
        2,
        1,
        0,
        "janeyx99",
        "COMMENTED",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146640"
    ],
    [
        146638,
        "use None to slice when list has one element only",
        "When autotune_num_choices_displayed is None and the list of choices has length 1, slicing with `[:-1]` means getting all elements except the last one, which resulted in an empty list.\r\n\r\nSlicing with `[:None]` works. \r\n\r\nDifferential Revision: D69265168\r\n\r\n\r\n\r\n\r\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @desertfire @chauhang @aakhundov",
        "open",
        "2025-02-06T21:08:40Z",
        null,
        null,
        "fb-exported, ciflow/trunk, topic: not user facing, module: inductor, ciflow/inductor, merging",
        "main",
        "export-D69265168",
        1,
        2,
        5,
        1,
        8,
        0,
        "drisspg",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146638"
    ],
    [
        146637,
        "gloo: fix building system gloo with CUDA/HIP",
        "Fix incorrect linking of Gloo's libraries when building with system Gloo. Previously, either Gloo's native library or Gloo's CUDA library were linked. However, Gloo had changed such that all users of Gloo must link the native library, and can optionally link the CUDA or HIP library for Gloo + CUDA/HIP support.\r\nThis had been updated when building/linking with vendored Gloo, but not when using system Gloo.\r\n\r\nFixes: #146239\r\n\r\nReported-by: Adam J Stewart <ajstewart426@gmail.com>\r\n\r\n\n\ncc @malfet @seemethere @ptrblck @msaroufim @eqy",
        "open",
        "2025-02-06T21:04:17Z",
        null,
        null,
        "module: build, module: cuda, triaged, open source, topic: not user facing",
        "main",
        "gloo_cuda",
        2,
        25,
        25,
        1,
        2,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146637"
    ],
    [
        146636,
        "example repro failure",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146636\n\nSummary:\n\nTest Plan:\n\nReviewers:\n\nSubscribers:\n\nTasks:\n\nTags:\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-02-06T21:00:47Z",
        null,
        null,
        "module: dynamo, ciflow/inductor",
        "gh/c00w/37/base",
        "gh/c00w/37/head",
        2,
        5,
        0,
        1,
        2,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146636"
    ],
    [
        146634,
        "Add Structured Tracing for Traced Graph Edge Details for AC Debugging",
        "Summary:\nUpdating the structured trace infrastructure so that we are able to output to Zoomer and have an E2E solution.\n\nContext Doc: https://docs.google.com/document/d/1T6omIBEWVhbOiwDLSLffgQwjxiT2rQv8QvvQwXkw4fY/edit?usp=sharing\n\nTest Plan:\n### Testing Structured Log + tlparse locally\n\nCommand:\n```\nTORCH_TRACE=/data/users/basilwong/fbsource/fbcode/log_torch_trace buck2 run mode/opt //aps_models/ads/icvr:icvr_launcher -- mode=local_fb_fm_v4 launcher.num_workers=2\n```\n\nTorch Trace Logs (local then sent to paste): P1686419449\n```\ncat log_torch_trace/dedicated_log_torch_trace_rank_0_2lg012xo.log | pastry\nP1686419449\n```\n\ntlparse output: https://manifold.edge.x2p.facebook.net/v0/read/tree/logs/.tmpyiv5wj/rank_1/index.html?bucketName=tlparse_reports&apiKey=tlparse_reports-key&withPayload=1&timeoutMsec=100\n\ntlparse graph edge details output: https://manifold.edge.x2p.facebook.net/v0/read/tree/logs/.tmpyiv5wj/rank_1/9_0_0/joint_graph_information_397.txt?bucketName=tlparse_reports&apiKey=tlparse_reports-key&withPayload=1&timeoutMsec=100\n\nDifferential Revision: D61557220\n\n\n",
        "open",
        "2025-02-06T20:29:48Z",
        null,
        null,
        "fb-exported, topic: not user facing, ciflow/inductor",
        "main",
        "export-D61557220",
        2,
        137,
        38,
        1,
        4,
        1,
        "jansel",
        "CHANGES_REQUESTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146634"
    ],
    [
        146633,
        "[NJT] Fix inference mode for composite implicit ops without nested-specific kernel",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146633\n\n",
        "open",
        "2025-02-06T20:02:09Z",
        null,
        null,
        "ciflow/trunk, topic: bug fixes, release notes: nested tensor, merging",
        "gh/soulitzer/352/base",
        "gh/soulitzer/352/head",
        2,
        27,
        4,
        3,
        10,
        1,
        "jbschlosser",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146633"
    ],
    [
        146632,
        "[ROCm] OCP FP8 Support for new GPUs",
        "TLDR: Follow up/ Build on top of https://github.com/pytorch/pytorch/pull/144476. add OCP FP8 support for gfx950\r\nrefer to https://github.com/pytorch/ao/pull/1677\r\n\r\nThis pull request includes several changes to improve compatibility and support for new GPU architectures and data types, particularly for ROCm. The key updates involve adding support for new ROCm versions and GPU architectures, updating data type handling, and removing outdated checks.\r\n\r\n### Improvements to GPU Architecture and ROCm Version Support:\r\n* [`aten/src/ATen/Context.cpp`](diffhunk://#diff-33de472d304acbe57d693c8567370c638068bedc1aa0ce8e9dc115dad05a7810L323-R326): Added support for new GPU architectures `gfx1200`, `gfx1201`, and `gfx950` based on ROCm version checks.\r\n* [`aten/src/ATen/native/cuda/Blas.cpp`](diffhunk://#diff-e8a569efee1e650172f120a0fdcda024fe3e4703a4ee3336425c8f685af6b3abL196-R199): Updated architecture support in multiple functions to include `gfx1200`, `gfx1201`, and `gfx950` based on ROCm version checks. [[1]](diffhunk://#diff-e8a569efee1e650172f120a0fdcda024fe3e4703a4ee3336425c8f685af6b3abL196-R199) [[2]](diffhunk://#diff-e8a569efee1e650172f120a0fdcda024fe3e4703a4ee3336425c8f685af6b3abL865-R876)\r\n\r\n### Updates to Data Type Handling:\r\n* [`aten/src/ATen/cuda/CUDADataType.h`](diffhunk://#diff-9188bb13b1a49f459141f5f9b875593d1c5ce2beb5ad711fdbaf5bc7089ec015L81-L98): Enhanced data type conversion to include new float8 types for both CUDA and ROCm environments.\r\n* [`aten/src/ATen/cuda/tunable/GemmHipblaslt.h`](diffhunk://#diff-bfa1a3b5d4bef1892bf50338775f3b0fd8cd31fc1868148f3968b98aefb68e3fL29-R80): Updated `HipDataTypeFor` template to handle new float8 types and added hard-coded enum values for ROCm versions prior to 6.3.\r\n\r\n### Removal of Outdated Checks:\r\n* [`cmake/public/LoadHIP.cmake`](diffhunk://#diff-b98e27b9a5f196a6965a99ee5a7bb15b3fc633d6375b767635b1b04ccb2fd3d5L169-L197): Removed the check for `HIP_NEW_TYPE_ENUMS` as it is no longer necessary with the updated ROCm versions. [[1]](diffhunk://#diff-b98e27b9a5f196a6965a99ee5a7bb15b3fc633d6375b767635b1b04ccb2fd3d5L169-L197) [[2]](diffhunk://#diff-b98e27b9a5f196a6965a99ee5a7bb15b3fc633d6375b767635b1b04ccb2fd3d5L211-R182)\r\n\r\nThese changes ensure better compatibility and performance on newer hardware and software environments, particularly for users leveraging ROCm and CUDA for deep learning and scientific computing tasks.\r\n\n\ncc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd",
        "open",
        "2025-02-06T18:57:21Z",
        null,
        null,
        "module: rocm, open source, release notes: linalg_frontend",
        "main",
        "ocp_gfx950",
        11,
        98,
        25,
        8,
        1,
        0,
        "jeffdaily",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146632"
    ],
    [
        146631,
        "Support ignoring parameters in FSDP2",
        "Differential Revision: D69153051\n\n\n\n\ncc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o",
        "open",
        "2025-02-06T18:46:40Z",
        null,
        null,
        "oncall: distributed, fb-exported, release notes: distributed (fsdp), ciflow/inductor",
        "main",
        "export-D69153051",
        3,
        392,
        5,
        1,
        17,
        2,
        "awgu, weifengpy, weifengpy, weifengpy",
        "COMMENTED, COMMENTED, COMMENTED, COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146631"
    ],
    [
        146626,
        " [inductor] Improve type annotations in _inductor/pattern_matcher.py",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146626\n* #146248\n\n\n\ncc @ezyang @SherlockNoMad @EikanWang @jgong5 @wenzhe-nrv @voznesenskym @penguinwu @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @desertfire @chauhang @aakhundov",
        "open",
        "2025-02-06T17:59:04Z",
        null,
        null,
        "open source, release notes: fx, topic: not user facing, fx, module: inductor, ciflow/inductor, suppress-api-compatibility-check, suppress-bc-linter",
        "gh/rec/132/base",
        "gh/rec/132/head",
        2,
        41,
        33,
        6,
        2,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146626"
    ],
    [
        146625,
        "Move capture_provenance to make_node_impl",
        "Previously we were only logging `make_user_impl` implementations, which only gets triggered for operations done on python SymInts, not cpp SymInts. Instead `make_node_impl` will get triggered for both python and cpp SymInt operations.\r\n\r\n\r\ncc @ezyang @SherlockNoMad @EikanWang @jgong5 @wenzhe-nrv",
        "open",
        "2025-02-06T17:53:35Z",
        null,
        null,
        "ciflow/trunk, release notes: fx, fx, ciflow/inductor, merging",
        "main",
        "angelayi/test_expression_created",
        1,
        89,
        90,
        1,
        13,
        2,
        "bobrenjc93, bobrenjc93",
        "APPROVED, APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146625"
    ],
    [
        146623,
        "bug fix: ensure 4d input in _scaled_dot_product_attention_math_mps",
        "This pr addresses the issue in the MPS backend for `_scaled_dot_product_attention_math_mps` where a 3d input like (num_heads, seq_len, query_dim) cannot be automatically treated as (1, num_heads, seq_len, query_dim), which can be inferred on cpu or cuda, which can be circumvented by adding a util function to ensure a 4d shape.\r\n\r\nThe issue was found in https://github.com/hiyouga/LLaMA-Factory/issues/6835, in [transformers qwen2_vl](https://github.com/huggingface/transformers/blob/1590c664306766f32ba68c50e67f14d61b16925d/src/transformers/models/qwen2_vl/modeling_qwen2_vl.py#L373C14-L373C93), 3d q/k/v were passed into sdpa function, which lead to an error.\r\n\r\nConsidering consistency, since this pattern might pop up elsewhere in the transformers codebase, I think it makes more sense to maintain the same intuition across all platforms.\r\n\r\n---\r\nreproduce code:\r\n```\r\nimport torch\r\nimport torch.nn.functional as F\r\n\r\nhead_num, seq_len, embed_dim = 16, 16, 80\r\nbsz = 1\r\n\r\nq = torch.randn(head_num, seq_len, embed_dim)\r\nk = torch.randn(head_num, seq_len, embed_dim)\r\nv = torch.randn(head_num, seq_len, embed_dim)\r\nattention_mask = torch.ones(1, seq_len, seq_len)\r\n\r\noo_cpu = F.scaled_dot_product_attention(\r\n    q.to(\"cpu\"),\r\n    k.to(\"cpu\"),\r\n    v.to(\"cpu\"),\r\n    attention_mask.to(\"cpu\"),\r\n    dropout_p=0.0\r\n)\r\n\r\nif torch.backends.mps.is_available():\r\n    oo_mps = F.scaled_dot_product_attention(\r\n        q.to(\"mps\"),\r\n        k.to(\"mps\"),\r\n        v.to(\"mps\"),\r\n        attention_mask.to(\"mps\"),\r\n        dropout_p=0.0\r\n    )\r\n    assert torch.allclose(oo_cpu, oo_mps.to(\"cpu\"), atol=1e-5)\r\n```\r\n\r\nerror outputs:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/opt/homebrew/Caskroom/miniconda/base/envs/torch-dev/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-2-5169b8d2c5dd>\", line 21, in <module>\r\n    oo_mps = F.scaled_dot_product_attention(\r\nIndexError: Dimension out of range (expected to be in range of [-3, 2], but got 3)\r\n```\r\n\r\nhardware and envs:\r\n```\r\ntorch               2.6.0\r\napple m3 max\r\n```\r\n\r\n---\r\n\r\n",
        "open",
        "2025-02-06T17:15:59Z",
        null,
        null,
        "triaged, open source, topic: bug fixes, release notes: mps, ciflow/mps",
        "main",
        "main",
        2,
        66,
        14,
        6,
        5,
        0,
        "Skylion007, Skylion007, malfet, malfet",
        "COMMENTED, COMMENTED, COMMENTED, COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146623"
    ],
    [
        146622,
        "Fix inductor non-stable argsort/sort test",
        "- Prevent the inductor test for argsort/sort from wrongly failing when the argsort/sort output with stable=False differs from pytorch but is still a valid argsort output.\r\n- Add functionality to allow alternative assert_equal functions in inductor tests for future cases.\r\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @desertfire @chauhang @aakhundov",
        "open",
        "2025-02-06T17:10:12Z",
        null,
        null,
        "triaged, open source, topic: not user facing, module: inductor",
        "main",
        "PYT-466",
        2,
        169,
        19,
        1,
        1,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146622"
    ],
    [
        146620,
        "Enable qint8 and quint8 add for AArch64 using ACL directly",
        "This enables qint8 and quint8 add for AArch64 through Arm Compute Library (ACL) directly.\r\nIt\u2019s based on changes in PR #145942 which enables the use of ACL directly in ATen.\r\nRelative performance improvement using OMP_NUM_THREADS=1 is ~15x, using OMP_NUM_THREADS=32 it\u2019s ~5.4x.\r\n\r\nScript to benchmark quantised add performance:\r\n```\r\nimport torch\r\nimport torch.profiler as profiler\r\n\r\na_f32 = torch.rand((400, 3456),dtype=torch.float)\r\nb_f32 = torch.rand((400, 3456),dtype=torch.float)\r\na_q = torch.quantize_per_tensor(a_f32, 1.2, 0, torch.qint8)\r\nb_q = torch.quantize_per_tensor(b_f32, 1.7, 5, torch.qint8)\r\n\r\nwith profiler.profile(with_stack=True, profile_memory=False, record_shapes=True) as prof:\r\n    for i in range(1000):     \r\n        _ = torch.ops.quantized.add(a_q, b_q, 1.3, 2)\r\nprint(prof.key_averages(group_by_input_shape=True).table(sort_by='self_cpu_time_total', row_limit=50))\r\n```\n\ncc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10",
        "open",
        "2025-02-06T16:54:08Z",
        null,
        null,
        "module: cpu, triaged, open source, release notes: quantization, release notes: releng",
        "main",
        "acl_qadd",
        10,
        577,
        15,
        3,
        2,
        1,
        "malfet, malfet",
        "COMMENTED, COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146620"
    ],
    [
        146617,
        "Generate test reports for pytest when option is given",
        "The argument needs to be appended when test reports should be generated. `IS_CI` is not necessarily set, so rather check `TEST_SAVE_XML` instead as in other places where test reports are conditionally enabled.\r\n\r\nSee also https://github.com/pytorch/pytorch/issues/126523",
        "open",
        "2025-02-06T16:12:06Z",
        null,
        null,
        "triaged, open source, topic: not user facing",
        "main",
        "Flamefire-patch-1",
        1,
        2,
        1,
        1,
        1,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146617"
    ],
    [
        146616,
        "[don't merge] test baseline",
        "Fixes #ISSUE_NUMBER\r\n",
        "open",
        "2025-02-06T16:04:22Z",
        null,
        null,
        "open source, topic: not user facing, ciflow/binaries_wheel, ciflow/xpu",
        "main",
        "test_main",
        1,
        1,
        0,
        1,
        7,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146616"
    ],
    [
        146615,
        "[BE][Ez]: Enable specific ruff rules to prevent antipatterns and bugs",
        "Enables a few ruff rules\r\n* Ban print statements within asserts (likely bugs)\r\n* ~Use string for Decimal literal to prevent loss of precision~ \r\n* ~Do not use default args for __post__init__ in dataclasses, they likely were meant to go into the factory method, the __init__, or somewhere else. The default values are useless here.~\r\n\r\nWait until ruff upgrade for the last 2\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-02-06T16:04:20Z",
        null,
        null,
        "triaged, open source, better-engineering, topic: not user facing, module: dynamo, ciflow/inductor",
        "main",
        "skylion007/enable-RUF-2025-02-06",
        2,
        4,
        3,
        3,
        1,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146615"
    ],
    [
        146614,
        "[CD] Add python 3.13t build for xpu",
        "Fixes #146451\r\n",
        "open",
        "2025-02-06T15:55:52Z",
        null,
        null,
        "open source, ciflow/trunk, topic: not user facing, ciflow/binaries_wheel",
        "main",
        "xpu_py_3_13t",
        3,
        339,
        2,
        1,
        5,
        1,
        "atalman",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146614"
    ],
    [
        146612,
        "[WIP] BaseSubclass",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146612\n\n",
        "open",
        "2025-02-06T15:33:23Z",
        null,
        null,
        "",
        "gh/IvanKobzarev/100/base",
        "gh/IvanKobzarev/100/head",
        3,
        98,
        17,
        1,
        2,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146612"
    ],
    [
        146604,
        "[Profiler] Enable CUPTI teardown to reduce profiler overhead",
        "The problem is that the profiler slowed down\r\ntraining by roughly 10-20% even after completion\r\nbecause cuptiFinalize was not called in Kineto due to TEARDOWN_CUPTI=0. Disabling CUPTI teardown was a workaround for crashes which occured when CUDA graphs were used. This issue was fixed in CUDA 12.6. Also there is no point in disabling CUPTI teardown if CUDA Graphs are not used.\r\n\r\nFixes #144455 \r\n\n\ncc @robieta @chaekit @guotuofeng @guyang3532 @dzhulgakov @davidberard98 @briancoutinho @sraikund16 @sanrise",
        "open",
        "2025-02-06T13:43:37Z",
        null,
        null,
        "triaged, open source, oncall: profiler, topic: not user facing",
        "main",
        "fix/144455_teardown_cupti",
        2,
        22,
        4,
        4,
        8,
        1,
        "sraikund16, sraikund16, davidberard98, mgmtea, mgmtea, mgmtea, mgmtea",
        "APPROVED, APPROVED, CHANGES_REQUESTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146604"
    ],
    [
        146597,
        "Fix bug in _ref_test_helper in test_ops and fix failing test on Aarch64",
        "We have a failing unit test on Aarch64\r\n\r\n```\r\nException: Caused by reference input at index 34: SampleInput(input=Tensor[size=(5, 5, 4), device=\"cpu\", dtype=torch.complex64, contiguous=False], args=(), kwargs={}, broadcasts_input=False, name='')\r\n\r\nTo execute this test, run the following from the base repo dir:\r\n    PYTORCH_OPINFO_SAMPLE_INPUT_INDEX=34 python test/test_ops.py TestCommonCPU.test_python_ref__refs_square_cpu_complex64\r\n\r\nThis message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0\r\n```\r\n\r\nAfter debugging it I found that `ex` variable is not being reset to None on each loop inside _ref_test_helper. Which after fixing, highlighted another expectedFailure to reenable - `nn.functional.hinge_embedding_loss` which was incorrectly being skipped due to the same problem.\r\n\r\nhttps://github.com/pytorch/pytorch/blob/4a545eb85d6ba06079787a83f8ab1a8c8f67c76f/test/test_ops.py#L546\r\nex variable is not reset after this for next loop iteration",
        "open",
        "2025-02-06T11:07:07Z",
        null,
        null,
        "triaged, open source, topic: not user facing",
        "main",
        "test_fix",
        3,
        3,
        11,
        1,
        3,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146597"
    ],
    [
        146596,
        "separate f16 vectorized class from bf16",
        "This refactoring is required as part of https://github.com/pytorch/pytorch/pull/143666\n\ncc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @malfet @snadampal @milpuz01",
        "open",
        "2025-02-06T10:58:15Z",
        null,
        null,
        "module: cpu, open source, module: arm, topic: not user facing",
        "main",
        "refactor",
        4,
        1031,
        404,
        1,
        1,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146596"
    ],
    [
        146595,
        "skip test_torch_dynamo_codegen_pow if CPU backend is not cpp",
        "The test asserts that `aten.pow` is not present in the generated kernel code. When using a CPU backend other than cpp, the kernel contains comments referencing the aten ops that produced the kernel in this case `aten.pow`. \r\n\r\nThis PR skips that test case if the CPU backend is not cpp.\r\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-02-06T10:46:32Z",
        null,
        null,
        "triaged, open source, topic: not user facing, module: dynamo",
        "main",
        "skip-if-cpu-backend-not-cpp",
        1,
        4,
        0,
        1,
        1,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146595"
    ],
    [
        146593,
        "[NOT FOR LANDING] experimental NVSHMEM integration",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146593\n* #146592\n\n\n\ncc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o",
        "open",
        "2025-02-06T10:26:18Z",
        null,
        null,
        "oncall: distributed, release notes: distributed (c10d)",
        "gh/yifuwang/195/base",
        "gh/yifuwang/195/head",
        7,
        471,
        4,
        2,
        1,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146593"
    ],
    [
        146592,
        "clang-format CUDASymmetricMemory.cu",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #146593\n* __->__ #146592\n\n\n\ncc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o",
        "open",
        "2025-02-06T10:26:14Z",
        null,
        null,
        "oncall: distributed, release notes: distributed (c10d)",
        "gh/yifuwang/194/base",
        "gh/yifuwang/194/head",
        1,
        20,
        10,
        1,
        2,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146592"
    ],
    [
        146589,
        "[DDP] Use NCCL allocated memory for gradient bucket",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146589\n\r\nSo that NVLink SHARP comes with zero-copy on H100+ platforms, for DDP applications.\r\nLess SM usage, less memory contention between NCCL kernel and compute kernels.\r\n\r\nAdded env `DDP_DISABLE_COMM_MEM` as a back-out option:\r\n```\r\nAn environment variable to disable comm-optimized memory pool.\r\nDefault is 0, which means comm-optimized memory pool is enabled.\r\nUsers can set it to 1 in case of seeing regression or OOM (because this\r\ncomm MemPool may not share space with regular compute MemPool).\r\n```\r\n\r\ncc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o\n\nDifferential Revision: [D69297766](https://our.internmc.facebook.com/intern/diff/D69297766)",
        "open",
        "2025-02-06T09:01:24Z",
        null,
        null,
        "oncall: distributed, ciflow/trunk, release notes: distributed (c10d), release notes: distributed (ddp), merging",
        "gh/kwen2501/123/base",
        "gh/kwen2501/123/head",
        7,
        127,
        13,
        6,
        14,
        2,
        "Skylion007, Skylion007, Skylion007, Skylion007, kwen2501, Skylion007, kwen2501, syed-ahmed, syed-ahmed, syed-ahmed, syed-ahmed, syed-ahmed, syed-ahmed, yifuwang, kwen2501, yifuwang, fegin, syed-ahmed, kwen2501, c-p-i-o, c-p-i-o, c-p-i-o, c-p-i-o, fduwjj, fduwjj",
        "COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, CHANGES_REQUESTED, APPROVED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, APPROVED, COMMENTED, APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146589"
    ],
    [
        146587,
        "[Dynamo] Allow dynamo to handle `str.xxx()`",
        "\r\nFixes #146350\r\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-02-06T08:47:14Z",
        null,
        null,
        "open source, topic: not user facing, module: dynamo",
        "main",
        "fix/dynamo/str",
        2,
        13,
        0,
        2,
        10,
        1,
        "zou3519, shink, shink, zou3519",
        "COMMENTED, COMMENTED, COMMENTED, APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146587"
    ],
    [
        146583,
        "[symbolic shapes] Log symnode id",
        "We want to log the symnode id which will help us with provenance tracking between expressions created.\r\n\r\n\r\ncc @ezyang @SherlockNoMad @EikanWang @jgong5 @wenzhe-nrv",
        "open",
        "2025-02-06T07:45:13Z",
        null,
        null,
        "ciflow/trunk, release notes: fx, fx, ciflow/inductor, merging",
        "main",
        "angelayi/test",
        1,
        26,
        7,
        1,
        7,
        1,
        "bobrenjc93",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146583"
    ],
    [
        146582,
        "[Partitioner] Reduce time consuming of partitions merger",
        "This patch optimize maybe_merge_partition func through 3-ways:\r\n\r\nRemove unnecessary copy https://github.com/pytorch/pytorch/blob/main/torch/fx/passes/infra/partitioner.py#L99. The number of copied nodes is large if we can merge all of the nodes of graph into one partition.\r\nRecord users of each partition to avoid duplicate iteration over nodes https://github.com/pytorch/pytorch/blob/main/torch/fx/passes/infra/partitioner.py#L133. The trip count of this loop maybe very large.\r\nThe nodes number of each partitions maybe not balance https://github.com/pytorch/pytorch/blob/main/torch/fx/passes/infra/partitioner.py#L145. We always encounter one issue: one partition has n nodes, but the other has one node. Merge the smaller partition into the larger can help to reduce time consuming.\n\ncc @ezyang @SherlockNoMad @EikanWang @jgong5 @wenzhe-nrv",
        "open",
        "2025-02-06T07:35:55Z",
        null,
        null,
        "triaged, open source, release notes: fx, topic: not user facing, fx",
        "main",
        "lingzhiz/optimize_partition_merger",
        1,
        39,
        26,
        3,
        2,
        0,
        "Skylion007",
        "COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146582"
    ],
    [
        146581,
        "Clarify that compile(module) only affects the forward method",
        "Fixes #141616\r\n\r\n## Changes\r\n\r\n- Add `Note` to Clarify how compile works with `nn.Module`\r\n- Optimize plain url address with clickable description\r\n\r\n## Test Result\r\n\r\n### Before\r\n\r\n![image](https://github.com/user-attachments/assets/15ff9985-7e91-4d71-be7d-cdd38eacd3f9)\r\n![image](https://github.com/user-attachments/assets/26e27ba4-52da-4336-b72d-a0f9d0ebe839)\r\n\r\n\r\n### After\r\n\r\n![image](https://github.com/user-attachments/assets/5eaa8421-19e8-4186-af3d-dab4323d2c95)\r\n![image](https://github.com/user-attachments/assets/a96a8a79-6320-4748-931f-33f4dbc640eb)\r\n\r\n",
        "open",
        "2025-02-06T07:34:49Z",
        null,
        null,
        "triaged, open source, topic: not user facing",
        "main",
        "opt/docs/compile",
        1,
        8,
        2,
        1,
        2,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146581"
    ],
    [
        146580,
        "[Partitioner] Remove unnecessary upstream nodes in dependency viewer",
        "We iterate upstream nodes to update partition map. But actually did nothing due to we iterate nodes with reversed topological order https://github.com/pytorch/pytorch/pull/136608/files#diff-f2f9dd3903fd99955732eb694941fea0cb7301a58d59554787f3311d417e5615L193 so that there exists no upstream nodes in assignment. Remove it to reduce for-loop overhead which up to O(N * N) complexity.\r\n\n\ncc @ezyang @SherlockNoMad @EikanWang @jgong5 @wenzhe-nrv",
        "open",
        "2025-02-06T07:32:01Z",
        null,
        null,
        "triaged, open source, release notes: fx, topic: not user facing, fx",
        "main",
        "lingzhiz/remove_upstream_nodes",
        1,
        0,
        19,
        1,
        2,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146580"
    ],
    [
        146578,
        "add `torch.float4_e2m1fn_x2` to PyTorch",
        "Summary:\r\n\r\nAdds the `torch.float4_e2m1fn_x2` dtype to PyTorch, as detailed in\r\nhttps://github.com/pytorch/pytorch/issues/146414 . Please see the issue for a detailed definition of the format.  \r\n\r\nNote that I decided to keep the casts out of this to significantly simplify the code, as defining casting between packed and unpacked formats will be tricky using the existing casting machinery.  \r\n\r\nExample of basic functionality:\r\n\r\n```python\r\nimport torch\r\n\r\n# creation with empty\r\nx0 = torch.empty(4, 4, dtype=torch.float4_e2m1fn_x2)\r\n\r\n# printing, prints the uint8 representation of the stored values\r\nprint(x0)\r\n\r\n# view as other dtype\r\nx0.view(torch.uint8).view(torch.float4_e2m1fn_x2)\r\n```\r\n\r\nDone in this PR:\r\n* tensor creation and tensor printing works (no other ops defined)\r\n\r\nFor future PRs:\r\n* torch._scaled_mm\r\n* PT2\r\n* various cleanups (detailed in comments with issue numbers)\r\n\r\nTest Plan:\r\n\r\n```\r\npytest test/quantization/core/experimental/test_floatx.py -s\r\n```\r\n\r\nReviewers:\r\n\r\nSubscribers:\r\n\r\nTasks:\r\n\r\nTags:\r\n\r\ncc @yanbing-j @albanD @kadeng @penguinwu @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10",
        "open",
        "2025-02-06T07:08:42Z",
        null,
        null,
        "release notes: quantization",
        "gh/vkuzo/1/head",
        "gh/vkuzo/2/head",
        7,
        70,
        5,
        3,
        2,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146578"
    ],
    [
        146574,
        "[ROCm][TunableOp] Offline results are saved to file when offline tuning is disabled.",
        "This PR is to fix UT breakage that has been reported internally and is considered high priority. When `tunable.record_untuned_enable(False)` is invoked, we flush the results of the untuned gemm file.\r\n\r\nOffline tuning I/O currently doesn't have a set untuned results filename member function or untuned results write to file member function. When performing back-to-back unit tests, the same ofstream ends up getting reused between UTs. Due to the way the UT are executed, this can lead to unexpected failures.\r\n\r\ncc: @jfactory07 \r\n\r\n\r\n\r\ncc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang",
        "open",
        "2025-02-06T06:04:28Z",
        null,
        null,
        "module: rocm, open source, topic: not user facing, ciflow/rocm",
        "main",
        "fix_tunableop_untuned_fileio",
        1,
        2,
        0,
        1,
        4,
        0,
        "jeffdaily",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146574"
    ],
    [
        146573,
        "add python root bin to windows load path.",
        "This PR is extend python root bin path to dll load list. \r\nIt makes PyTorch robust and compatible to more dependency libraries, such as `intel-pti`.\r\n\r\ncc @peterjc123 @mszhanyi @skyline75489 @nbcsm @iremyux @Blackhex @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10",
        "open",
        "2025-02-06T05:58:55Z",
        null,
        null,
        "module: windows, open source, ciflow/trunk, topic: not user facing, intel",
        "main",
        "xu_add_init_path",
        1,
        8,
        1,
        1,
        1,
        0,
        "EikanWang",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146573"
    ],
    [
        146571,
        "[Dynamo][autograd.Function] Relax backward speculation strict mode a bit",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #146742\n* #146741\n* __->__ #146571\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-02-06T05:05:51Z",
        null,
        null,
        "topic: not user facing, module: dynamo, ciflow/inductor",
        "gh/yanboliang/63/base",
        "gh/yanboliang/63/head",
        7,
        228,
        44,
        5,
        1,
        0,
        "yanboliang, zou3519, zou3519, zou3519, yanboliang, zou3519, zou3519",
        "COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146571"
    ],
    [
        146562,
        "WIP hacky reordering pass",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146562\n* #146561\n* #146560\n* #146559\n* #146558\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @desertfire @chauhang @aakhundov",
        "open",
        "2025-02-06T01:41:51Z",
        null,
        null,
        "module: inductor, ciflow/inductor",
        "gh/wconstab/396/base",
        "gh/wconstab/396/head",
        3,
        325,
        24,
        1,
        2,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146562"
    ],
    [
        146561,
        "Improve comms debug visualization",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #146562\n* __->__ #146561\n* #146560\n* #146559\n* #146558\n\n",
        "open",
        "2025-02-06T01:41:46Z",
        null,
        null,
        "module: inductor, ciflow/inductor",
        "gh/wconstab/395/base",
        "gh/wconstab/395/head",
        1,
        2,
        3,
        1,
        2,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146561"
    ],
    [
        146560,
        "enable reorder",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #146562\n* #146561\n* __->__ #146560\n* #146559\n* #146558\n\n",
        "open",
        "2025-02-06T01:41:41Z",
        null,
        null,
        "module: inductor, ciflow/inductor",
        "gh/wconstab/394/base",
        "gh/wconstab/394/head",
        1,
        1,
        1,
        1,
        2,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146560"
    ],
    [
        146559,
        "Apply changes from https://github.com/pytorch/pytorch/commit/211847de3c1c3d6cbd299e14a001b794eabf2a2d",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #146562\n* #146561\n* #146560\n* __->__ #146559\n* #146558\n\n",
        "open",
        "2025-02-06T01:41:36Z",
        null,
        null,
        "oncall: distributed, ciflow/inductor",
        "gh/wconstab/393/base",
        "gh/wconstab/393/head",
        1,
        65,
        15,
        1,
        2,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146559"
    ],
    [
        146558,
        "[dtensor] support mixed precision for redistribute (#20)",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #146562\n* #146561\n* #146560\n* #146559\n* __->__ #146558\n\n",
        "open",
        "2025-02-06T01:41:31Z",
        null,
        null,
        "oncall: distributed, ciflow/inductor",
        "gh/wconstab/392/base",
        "gh/wconstab/392/head",
        2,
        52,
        15,
        1,
        3,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146558"
    ],
    [
        146557,
        "Add fqn_modifier at loading_state_dict and unit test",
        "In Fusion model, users might change the state_dict keys by state_dict_hook\r\nThe load_state_dict APIs here won't call model.state_dict() so that the hooks won't be called to change the keys, causing the mismatch between fqn and state_dict keys.\r\n\r\nThe PR here suggests users to add how they would change the state_dict key prefix (they can name it, here we call \"fqn_modifiers\") by default\r\nDuring loading state_dict, we have the prefix change during getting fqn so that they can be processed same as through state_dict hook.\r\n\r\nFor example:\r\nThere's a state_dict_hook:\r\n\r\n```\r\ndef _state_dict_hook(self, destination, prefix, keep_vars):\r\n    \"\"\"Remove \"embedding\" from the original embedding in the state_dict\r\n    name. This keeps the orginal state dict name for the embedding\r\n    from before fusing with the FusionEmbedding.\r\n\r\n    [!Note] This update changes the order of the OrderedDict\r\n    \"\"\"\r\n    key = prefix + \"embedding.weight\"\r\n    new_key = prefix + \"weight\"\r\n    destination[new_key] = destination[key]\r\n    del destination[key]\r\n```\r\n\r\nIn the dsd after this PR, we would skip \"embedding.\" before \"weight\" if find the \"fqn_modifiers\" attribute at that module\r\n```\r\ndef fqn_modifiers(self) -> Dict[str, str]:\r\n    return {\r\n        \"weight\": \"embedding\",\r\n    }\r\n```\n\ncc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o @LucasLLC @MeetVadakkanchery @mhorowitz @pradeepfn @ekr0",
        "open",
        "2025-02-06T01:16:54Z",
        null,
        null,
        "oncall: distributed, topic: not user facing, module: distributed_checkpoint",
        "main",
        "dsd_fqn_modifiers",
        3,
        97,
        7,
        4,
        1,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146557"
    ],
    [
        146545,
        "Update test.sh to run a greater set of unit tests on aarch64",
        "expanded set of unit tests that run  on aarch64 to be the entire set of tests that can be run by run_test.py\r\n\r\n\n\ncc @seemethere @malfet @pytorch/pytorch-dev-infra",
        "open",
        "2025-02-05T23:41:27Z",
        null,
        null,
        "module: ci, triaged, open source, topic: not user facing",
        "main",
        "expanded_unit_tests",
        1,
        1,
        22,
        2,
        3,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146545"
    ],
    [
        146543,
        "Update local_timer.py to improve queue handling",
        "- Switched from `multiprocessing.Queue` to `torch.multiprocessing.Queue`\r\n- Wrapped `qsize()` in `try-except` to prevent `NotImplementedError`\n\ncc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o",
        "open",
        "2025-02-05T23:33:01Z",
        null,
        null,
        "oncall: distributed, triaged, open source, release notes: distributed (torchelastic)",
        "main",
        "update-local-timer",
        1,
        12,
        9,
        1,
        2,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146543"
    ],
    [
        146540,
        "[export] Draft export custom streamer",
        "* Instead of using tlparse's StreamHandler, draft-export will use its own, which will capture the logs, filter them, and only output the relevant ones to the log file. \r\n* To do this, the CaptureStructuredTrace logger will use a `LogRecord` which is basically a dictionary with a custom hash function based on what is being logged. This allows us to deduplicate logs which represent the same thing, such as:\r\n  * \"missing_fake_kernel\" logs with the same operator\r\n  * \"mismatched_fake_kernel\" logs with the same operator and reasoning\r\n  * \"propagate_real_tensor\", \"create_unbacked_symbol\", and \"guard_added\" logs occurring on lines with the same stacktrace",
        "open",
        "2025-02-05T23:24:25Z",
        null,
        null,
        "release notes: export",
        "main",
        "angelayi/draft_logger",
        2,
        89,
        48,
        1,
        1,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146540"
    ],
    [
        146537,
        "[WIP] Log graph breaks",
        "Graph breaks currently aren't logged. We want to log them. Need to test before merging.\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @desertfire @chauhang @aakhundov",
        "open",
        "2025-02-05T23:08:22Z",
        null,
        null,
        "topic: not user facing, module: inductor, module: dynamo, ciflow/inductor, dynamo-logging",
        "main",
        "gh/raymo/log-graph-breaks",
        4,
        64,
        7,
        1,
        1,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146537"
    ],
    [
        146535,
        "[wip] disable decorator for ca",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146535\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @yf225",
        "open",
        "2025-02-05T22:52:10Z",
        null,
        null,
        "module: dynamo, ciflow/inductor, module: compiled autograd",
        "gh/xmfan/174/base",
        "gh/xmfan/174/head",
        9,
        48,
        0,
        2,
        2,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146535"
    ],
    [
        146532,
        "[symbolic shapes] Log SymNode id for provenance",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* (to be filled)\n\n\n\ncc @ezyang @SherlockNoMad @EikanWang @jgong5 @wenzhe-nrv",
        "open",
        "2025-02-05T22:47:15Z",
        null,
        null,
        "release notes: fx, fx, ciflow/inductor",
        "gh/angelayi/66/base",
        "gh/angelayi/66/head",
        3,
        148,
        92,
        1,
        1,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146532"
    ],
    [
        146530,
        "[aoti] Fix FakeTensorMode not detected in aot_compile when there's no input",
        "Summary:\r\nFixes https://github.com/pytorch/pytorch/issues/118304\r\n\r\nWhen we don't have inputs, we should still try to get a FakeTensorMode because there can be unbacked symints in the graph.\r\n\r\nSo we get the FakeTensorMode once when entering compile_fx, and then using the that FakeTensorMode for the rest of the lowering.\r\n\r\nTest Plan:\r\n```\r\nbuck run fbcode//mode/dev-nosan //caffe2/test/inductor:test_aot_inductor -- -r unbacked_arg\r\n```\r\n\r\nDifferential Revision: D69158049\r\n\r\n\r\n\r\n\r\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @desertfire @chauhang @aakhundov",
        "open",
        "2025-02-05T22:25:56Z",
        null,
        null,
        "fb-exported, ciflow/trunk, module: inductor, ciflow/inductor, release notes: AO frontend",
        "main",
        "export-D69158049",
        3,
        60,
        12,
        1,
        8,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146530"
    ],
    [
        146526,
        "[inductor] add units to estimated runtime log",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146526\n* #146513\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @desertfire @chauhang @aakhundov",
        "open",
        "2025-02-05T21:57:04Z",
        null,
        null,
        "module: inductor, ciflow/inductor",
        "gh/xmfan/173/base",
        "gh/xmfan/173/head",
        1,
        2,
        2,
        1,
        2,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146526"
    ],
    [
        146525,
        "[dynamo] improved graph break messages for some common graph break sites",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146525\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-02-05T21:47:17Z",
        null,
        null,
        "module: dynamo, ciflow/inductor, release notes: dynamo, module: compile ux",
        "gh/williamwen42/205/base",
        "gh/williamwen42/205/head",
        14,
        517,
        76,
        2,
        1,
        0,
        "zou3519, zou3519, zou3519, zou3519, zou3519, zou3519, jansel, bobrenjc93",
        "COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146525"
    ],
    [
        146520,
        "CUDA CachingHostAllocator tracks registrations to call correct free",
        "Users may change the allocator config at will. torch unit tests do this. However, allocations using cudaHostRegister should use corresonding cudaHostUnregister and similarly for cudaHostAlloc / cudaFreeHost.",
        "open",
        "2025-02-05T21:19:31Z",
        null,
        null,
        "triaged, open source, topic: not user facing",
        "main",
        "cuda_host_allocator_free",
        1,
        15,
        6,
        1,
        2,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146520"
    ],
    [
        146514,
        "[DTensor][Test] Create a simple unit test for tensordot",
        "Fixes #ISSUE_NUMBER\r\n\r\nThe dims and shape of the tensors are from a specific Shampoo use case. We want to create a unit test for it to make sure there are no regressions for this.\n\ncc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wconstab @d4l3k @c-p-i-o",
        "open",
        "2025-02-05T19:35:29Z",
        null,
        null,
        "oncall: distributed, Merged, Reverted, ciflow/trunk, topic: not user facing, merging, ci-no-td",
        "main",
        "tensordot",
        1,
        23,
        0,
        2,
        13,
        1,
        "tianyu-l",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146514"
    ],
    [
        146513,
        "[dynamo] check for incompatible configs",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146513\n\r\ninternal: https://fb.workplace.com/groups/1075192433118967/permalink/1599802033991335/\r\n\r\nAssuming flags don't change during compilation, we shouldn't allow incompatible configs to be set at torch.compile wrap time.\r\n\r\nNot in this PR: For flags that need to change during compilation, we'd have to be strict about where they can be used in the compile lifecycle\r\n\r\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @desertfire @chauhang @aakhundov",
        "open",
        "2025-02-05T19:09:40Z",
        null,
        null,
        "Merged, Reverted, ciflow/trunk, module: inductor, module: dynamo, ciflow/inductor, release notes: dynamo, merging, ci-no-td",
        "gh/xmfan/172/base",
        "gh/xmfan/172/head",
        3,
        31,
        0,
        4,
        13,
        0,
        "williamwen42",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146513"
    ],
    [
        146512,
        "Fix `DispatchStub.cpp` compilation for gcc 14",
        "Otherwise I get the following error:\r\n\r\n```bash\r\n\r\n.../intel-xpu-backend-for-triton/pytorch/aten/src/ATen/native/DispatchStub.cpp:152:18: error: no matching function for call to \u2018find(std::array<c10::DeviceType, 7>::const_iterator, std::array<c10::DeviceType, 7>::const_iterator, const c10::DeviceType&)\u2019\r\n  152 |     if (std::find(supported_devices.begin(), supported_devices.end(), device_type) == supported_devices.end()) {\r\n      |         ~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nIn file included from /usr/include/c++/14/bits/locale_facets.h:48,\r\n                 from /usr/include/c++/14/bits/basic_ios.h:37,\r\n                 from /usr/include/c++/14/ios:46,\r\n                 from /usr/include/c++/14/ostream:40,\r\n                 from .../intel-xpu-backend-for-triton/pytorch/c10/core/DeviceType.h:13,\r\n                 from .../intel-xpu-backend-for-triton/pytorch/aten/src/ATen/native/DispatchStub.h:3,\r\n                 from .../intel-xpu-backend-for-triton/pytorch/aten/src/ATen/native/DispatchStub.cpp:2:\r\n/usr/include/c++/14/bits/streambuf_iterator.h:435:5: note: candidate: \u2018template<class _CharT2> typename __gnu_cxx::__enable_if<std::__is_char<_CharT2>::__value, std::istreambuf_iterator<_CharT, std::char_traits<_CharT> > >::__type std::find(istreambuf_iterator<_CharT, char_traits<_CharT> >, istreambuf_iterator<_CharT, char_traits<_CharT> >, const _CharT2&)\u2019\r\n  435 |     find(istreambuf_iterator<_CharT> __first,\r\n      |     ^~~~\r\n/usr/include/c++/14/bits/streambuf_iterator.h:435:5: note:   template argument deduction/substitution failed:\r\n.../intel-xpu-backend-for-triton/pytorch/aten/src/ATen/native/DispatchStub.cpp:152:18: note:   mismatched types \u2018std::istreambuf_iterator<_CharT, std::char_traits<_CharT> >\u2019 and \u2018const std::array<c10::DeviceType, 7>::value_type*\u2019 {aka \u2018const c10::DeviceType*\u2019}\r\n  152 |     if (std::find(supported_devices.begin(), supported_devices.end(), device_type) == supported_devices.end()) {\r\n      |         ~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n```\r\n",
        "open",
        "2025-02-05T18:51:17Z",
        null,
        null,
        "triaged, open source, module: dispatch, topic: not user facing",
        "main",
        "patch-2",
        1,
        1,
        0,
        1,
        3,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146512"
    ],
    [
        146510,
        "[ONNX] Bump torchlib opset to 22",
        "Fixes #ISSUE_NUMBER\r\n",
        "open",
        "2025-02-05T18:47:25Z",
        null,
        null,
        "open source, release notes: onnx",
        "main",
        "justinchu/torchlib-opset22",
        30,
        20652,
        436,
        30,
        1,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146510"
    ],
    [
        146509,
        "[BE][CI][Easy] bump `ruff` to 0.9.0: long statements in docstrings",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #145606\n* #144546\n* #144569\n* #145148\n* __->__ #146509\n\n\n\ncc @ezyang @SherlockNoMad @EikanWang @jgong5 @wenzhe-nrv",
        "open",
        "2025-02-05T18:43:13Z",
        null,
        null,
        "open source, ciflow/trunk, release notes: onnx, topic: not user facing, fx",
        "gh/XuehaiPan/240/base",
        "gh/XuehaiPan/240/head",
        4,
        26,
        6,
        4,
        4,
        0,
        "justinchuby",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146509"
    ],
    [
        146506,
        "Support contextlib.ExitStack",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146506\n* #146505\n* #146503\n* #146502\n* #146501\n* #146500\n* #146504\n* #146499\n* #146498\n* #146497\n* #146496\n* #146493\n* #146492\n* #146491\n* #146495\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-02-05T17:22:12Z",
        null,
        null,
        "open source, module: dynamo, ciflow/inductor",
        "gh/guilhermeleobas/107/base",
        "gh/guilhermeleobas/107/head",
        2,
        621,
        9,
        5,
        2,
        0,
        "guilhermeleobas",
        "COMMENTED",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146506"
    ],
    [
        146505,
        "Allow setting attribute to NestedUserFunctionVariable",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #146506\n* __->__ #146505\n* #146503\n* #146502\n* #146501\n* #146500\n* #146504\n* #146499\n* #146498\n* #146497\n* #146496\n* #146493\n* #146492\n* #146491\n* #146495\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-02-05T17:22:05Z",
        null,
        null,
        "open source, module: dynamo, ciflow/inductor",
        "gh/guilhermeleobas/106/base",
        "gh/guilhermeleobas/106/head",
        2,
        21,
        1,
        5,
        2,
        0,
        "anijain2305",
        "COMMENTED",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146505"
    ],
    [
        146504,
        "Introduce `UserDefinedExceptionClassVariable`",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #146506\n* #146505\n* #146503\n* #146502\n* #146501\n* #146500\n* __->__ #146504\n* #146499\n* #146498\n* #146497\n* #146496\n* #146493\n* #146492\n* #146491\n* #146495\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-02-05T17:21:58Z",
        null,
        null,
        "open source, module: dynamo, ciflow/inductor",
        "gh/guilhermeleobas/105/base",
        "gh/guilhermeleobas/105/head",
        5,
        45,
        5,
        5,
        2,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146504"
    ],
    [
        146503,
        "Create new dynamo ObservedExceptions at runtime",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #146506\n* #146505\n* __->__ #146503\n* #146502\n* #146501\n* #146500\n* #146504\n* #146499\n* #146498\n* #146497\n* #146496\n* #146493\n* #146492\n* #146491\n* #146495\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-02-05T17:21:52Z",
        null,
        null,
        "open source, module: dynamo, ciflow/inductor",
        "gh/guilhermeleobas/104/base",
        "gh/guilhermeleobas/104/head",
        3,
        18,
        1,
        5,
        2,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146503"
    ],
    [
        146502,
        "Correctly propagate exception to parent tx",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #146506\n* #146505\n* #146503\n* __->__ #146502\n* #146501\n* #146500\n* #146504\n* #146499\n* #146498\n* #146497\n* #146496\n* #146493\n* #146492\n* #146491\n* #146495\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-02-05T17:21:46Z",
        null,
        null,
        "open source, module: dynamo, ciflow/inductor",
        "gh/guilhermeleobas/103/base",
        "gh/guilhermeleobas/103/head",
        2,
        92,
        3,
        5,
        2,
        0,
        "anijain2305",
        "APPROVED",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146502"
    ],
    [
        146501,
        "Update CPython tests for ctx manager to use unittest",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #146506\n* #146505\n* #146503\n* #146502\n* __->__ #146501\n* #146500\n* #146504\n* #146499\n* #146498\n* #146497\n* #146496\n* #146493\n* #146492\n* #146491\n* #146495\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-02-05T17:21:40Z",
        null,
        null,
        "open source, topic: not user facing, module: dynamo, ciflow/inductor",
        "gh/guilhermeleobas/102/base",
        "gh/guilhermeleobas/102/head",
        1,
        206,
        211,
        5,
        1,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146501"
    ],
    [
        146500,
        "Allow trace through unittest",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #146506\n* #146505\n* #146503\n* #146502\n* #146501\n* __->__ #146500\n* #146504\n* #146499\n* #146498\n* #146497\n* #146496\n* #146493\n* #146492\n* #146491\n* #146495\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-02-05T17:21:34Z",
        null,
        null,
        "open source, module: dynamo, ciflow/inductor, release notes: dynamo",
        "gh/guilhermeleobas/101/base",
        "gh/guilhermeleobas/101/head",
        6,
        623,
        14,
        5,
        1,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146500"
    ],
    [
        146499,
        "Add `__context/cause/suppress_context/traceback__` to Exception",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #146506\n* #146505\n* #146503\n* #146502\n* #146501\n* #146500\n* #146504\n* __->__ #146499\n* #146498\n* #146497\n* #146496\n* #146493\n* #146492\n* #146491\n* #146495\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-02-05T17:21:27Z",
        null,
        null,
        "open source, module: dynamo, ciflow/inductor, release notes: dynamo",
        "gh/guilhermeleobas/100/base",
        "gh/guilhermeleobas/100/head",
        5,
        361,
        13,
        5,
        1,
        0,
        "guilhermeleobas, guilhermeleobas",
        "COMMENTED, COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146499"
    ],
    [
        146498,
        "Add `sys.exc_info` and `sys.exception`",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #146506\n* #146505\n* #146503\n* #146502\n* #146501\n* #146500\n* #146504\n* #146499\n* __->__ #146498\n* #146497\n* #146496\n* #146493\n* #146492\n* #146491\n* #146495\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-02-05T17:21:20Z",
        null,
        null,
        "open source, module: dynamo, ciflow/inductor, release notes: dynamo",
        "gh/guilhermeleobas/99/base",
        "gh/guilhermeleobas/99/head",
        3,
        162,
        0,
        5,
        1,
        1,
        "guilhermeleobas, anijain2305",
        "COMMENTED, APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146498"
    ],
    [
        146497,
        "Propagate `AttributeError` to user code in user_defined.py",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #146506\n* #146505\n* #146503\n* #146502\n* #146501\n* #146500\n* #146504\n* #146499\n* #146498\n* __->__ #146497\n* #146496\n* #146493\n* #146492\n* #146491\n* #146495\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-02-05T17:21:13Z",
        null,
        null,
        "open source, topic: not user facing, module: dynamo, ciflow/inductor",
        "gh/guilhermeleobas/98/base",
        "gh/guilhermeleobas/98/head",
        3,
        38,
        1,
        5,
        1,
        1,
        "anijain2305, anijain2305",
        "COMMENTED, CHANGES_REQUESTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146497"
    ],
    [
        146496,
        "Handle `is`/`is not`",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #146506\n* #146505\n* #146503\n* #146502\n* #146501\n* #146500\n* #146504\n* #146499\n* #146498\n* #146497\n* __->__ #146496\n* #146493\n* #146492\n* #146491\n* #146495\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-02-05T17:21:06Z",
        null,
        null,
        "open source, module: dynamo, ciflow/inductor, release notes: dynamo",
        "gh/guilhermeleobas/97/base",
        "gh/guilhermeleobas/97/head",
        2,
        26,
        0,
        4,
        1,
        1,
        "anijain2305, anijain2305",
        "COMMENTED, CHANGES_REQUESTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146496"
    ],
    [
        146495,
        "Fix round(...) with constants",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #146506\n* #146505\n* #146503\n* #146502\n* #146501\n* #146500\n* #146504\n* #146499\n* #146498\n* #146497\n* #146496\n* #146493\n* #146492\n* #146491\n* __->__ #146495\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-02-05T17:20:59Z",
        null,
        null,
        "open source, ciflow/trunk, module: dynamo, ciflow/inductor, release notes: dynamo, merging",
        "gh/guilhermeleobas/96/base",
        "gh/guilhermeleobas/96/head",
        2,
        10,
        1,
        4,
        6,
        0,
        "anijain2305",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146495"
    ],
    [
        146494,
        "Fix STOPITERATION_ERROR opcode",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #146506\n* #146505\n* #146504\n* #146503\n* #146502\n* #146501\n* #146500\n* #146499\n* #146498\n* #146497\n* #146496\n* #146495\n* __->__ #146494\n* #146493\n* #146492\n* #146491\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-02-05T17:20:54Z",
        null,
        null,
        "open source, module: dynamo, ciflow/inductor, release notes: dynamo",
        "gh/guilhermeleobas/95/base",
        "gh/guilhermeleobas/95/head",
        1,
        5,
        4,
        2,
        1,
        1,
        "anijain2305, anijain2305",
        "COMMENTED, CHANGES_REQUESTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146494"
    ],
    [
        146493,
        "Add `RAISE_VARARGS 0`",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #146506\n* #146505\n* #146503\n* #146502\n* #146501\n* #146500\n* #146504\n* #146499\n* #146498\n* #146497\n* #146496\n* __->__ #146493\n* #146492\n* #146491\n* #146495\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-02-05T17:20:47Z",
        null,
        null,
        "open source, module: dynamo, ciflow/inductor, release notes: dynamo",
        "gh/guilhermeleobas/94/base",
        "gh/guilhermeleobas/94/head",
        2,
        27,
        1,
        4,
        1,
        0,
        "zou3519, guilhermeleobas",
        "COMMENTED, COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146493"
    ],
    [
        146492,
        "Add `WITH_EXCEPT_START` opcode",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #146506\n* #146505\n* #146503\n* #146502\n* #146501\n* #146500\n* #146504\n* #146499\n* #146498\n* #146497\n* #146496\n* #146493\n* __->__ #146492\n* #146491\n* #146495\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-02-05T17:20:40Z",
        null,
        null,
        "open source, module: dynamo, ciflow/inductor, release notes: dynamo",
        "gh/guilhermeleobas/93/base",
        "gh/guilhermeleobas/93/head",
        2,
        46,
        0,
        4,
        1,
        1,
        "zou3519, anijain2305",
        "COMMENTED, APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146492"
    ],
    [
        146491,
        "Add `make_dynamo_test`",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #146506\n* #146505\n* #146503\n* #146502\n* #146501\n* #146500\n* #146504\n* #146499\n* #146498\n* #146497\n* #146496\n* #146493\n* #146492\n* __->__ #146491\n* #146495\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-02-05T17:20:34Z",
        null,
        null,
        "open source, topic: not user facing, module: dynamo, ciflow/inductor",
        "gh/guilhermeleobas/92/base",
        "gh/guilhermeleobas/92/head",
        1,
        45,
        0,
        4,
        1,
        0,
        "zou3519, anijain2305, Skylion007, zou3519",
        "APPROVED, APPROVED, COMMENTED, COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146491"
    ],
    [
        146490,
        "[export] Serialize special values of float into strings for json.",
        "Summary: Currently inf is serialized as Infinity in JSON which is not standard compliant. Instead we will tweak all special floating points into strings and handle them at json layer.\n\nTest Plan:\nsee D69060784\nCI\n\nDifferential Revision: D69186425\n\n\n",
        "open",
        "2025-02-05T16:36:50Z",
        null,
        null,
        "fb-exported, ciflow/trunk, ciflow/inductor, release notes: export",
        "main",
        "export-D69186425",
        6,
        133,
        53,
        3,
        2,
        0,
        "yiming0416",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146490"
    ],
    [
        146489,
        "Update code_template.py re.compile() is directly applied to the regex\u2026",
        "\u2026 string inside the class variable\r\n\r\nre.compile() is directly applied to the regex string inside the class variable\r\n\r\nRegular expressions are very expensive computationally. So, this avoids any redundant compilation.\r\nFixes #ISSUE_NUMBER\r\n",
        "open",
        "2025-02-05T16:23:56Z",
        null,
        null,
        "open source, topic: not user facing",
        "main",
        "patch-1",
        1,
        3,
        2,
        1,
        2,
        0,
        "mikaylagawarecki",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146489"
    ],
    [
        146485,
        "Update quantile doc",
        "Fixes #146156\r\n",
        "open",
        "2025-02-05T15:33:15Z",
        null,
        null,
        "triaged, open source, release notes: python_frontend",
        "main",
        "patch-1",
        1,
        3,
        3,
        1,
        2,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146485"
    ],
    [
        146482,
        "Update addr doc",
        "Fixes https://github.com/pytorch/pytorch/issues/146399\r\n",
        "open",
        "2025-02-05T13:29:04Z",
        null,
        null,
        "topic: not user facing",
        "main",
        "albanD-patch-1",
        1,
        1,
        1,
        1,
        1,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146482"
    ],
    [
        146481,
        "[WIP][Windows][Inductor] Enable Inductor UT on XPU Windows.",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146481\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @desertfire @chauhang @aakhundov",
        "open",
        "2025-02-05T13:22:21Z",
        null,
        null,
        "open source, topic: not user facing, module: inductor, module: dynamo, ciflow/inductor, keep-going, ciflow/xpu",
        "gh/etaf/96/base",
        "gh/etaf/96/head",
        9,
        21,
        17,
        1,
        1,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146481"
    ],
    [
        146480,
        "[Submodule]: Update KleidiAI submodule to v1.3.0",
        "Change-Id: I687255982c72ee7daca438a15b718f07298963cc\r\n\n\ncc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @malfet @snadampal @milpuz01",
        "open",
        "2025-02-05T12:54:37Z",
        null,
        null,
        "module: cpu, open source, module: arm, ciflow/trunk, topic: not user facing, merging",
        "main",
        "kleidiai_submodule_update",
        1,
        1,
        1,
        1,
        9,
        1,
        "digantdesai, malfet",
        "APPROVED, APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146480"
    ],
    [
        146478,
        "[c10d] Add hccl distributed backend to c10d data structures",
        " # MOTIVATION\r\nIntel Gaudi is an out-of-tree PyTorch accelerator having its own device /dispatch key ```hpu``` .\r\nWith this change we add entries for Gaudi's distributed backend ```hccl``` to the c10d Backend data structures.\r\nThis is to ensure that there is no naming conflict in case a new in-tree accelerator is introduced with the same backend name.\r\n\r\n\r\nThe Out-of-tree backends are registered calling https://github.com/pytorch/pytorch/blob/fd0cd6a08f706b7bb1dedb296217b6441e4fb9ff/torch/distributed/distributed_c10d.py#L302\r\n\r\nSuccessful registration adds the backend name to the list : \r\nhttps://github.com/pytorch/pytorch/blob/fd0cd6a08f706b7bb1dedb296217b6441e4fb9ff/torch/distributed/distributed_c10d.py#L265\r\n\r\nWe are binding the process group creator constructs at run-time so if there are other distributed backend with the same device name they can safely add the device type to the dictionary \r\n\r\nhttps://github.com/pytorch/pytorch/blob/fd0cd6a08f706b7bb1dedb296217b6441e4fb9ff/torch/distributed/distributed_c10d.py#L274\r\n\r\nAnd add another entry to the dictionary with the same backend name ( but different device name )\r\nhttps://github.com/pytorch/pytorch/blob/fd0cd6a08f706b7bb1dedb296217b6441e4fb9ff/torch/distributed/distributed_c10d.py#L268\r\n\r\nIn addition the out-of-tree devices can utilize the ```backend_list``` to check for successful backend registration  eg: APIs like ```is_hccl_available```\r\n \r\n\n\ncc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o",
        "open",
        "2025-02-05T12:10:24Z",
        null,
        null,
        "oncall: distributed, triaged, open source, release notes: distributed (c10d)",
        "main",
        "c10d_add_hccl_to_backends",
        1,
        19,
        8,
        1,
        3,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146478"
    ],
    [
        146477,
        "Improve error handling when checking CUDA version in case nvcc is not found",
        "Fixes:\r\n- https://github.com/pytorch/pytorch/issues/101138\r\n\r\n**Description**\r\nThe PR enhances error handling in `_check_cuda_version` by verifying the existence of the `nvcc` executable before invoking `subprocess.check_output`. If `nvcc` is missing, a `FileNotFoundError` is raised with a clear message, guiding users to check their CUDA installation and path configuration.\r\n\r\n**Testing**\r\nManually tested with and without `nvcc` present in the expected path.\r\n\n\ncc @peterjc123 @mszhanyi @skyline75489 @nbcsm @iremyux @Blackhex",
        "open",
        "2025-02-05T12:09:22Z",
        null,
        null,
        "module: windows, triaged, open source, release notes: fx",
        "main",
        "pytorch-101138",
        1,
        4,
        1,
        1,
        1,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146477"
    ],
    [
        146476,
        "[Feat]: Improve KleidiAI 4 bit kernel performance",
        "Description:\r\n1. New thread blocking accelerates GEMVs\r\n\r\nPerf improvements:\r\n12% speedup in LLM prefill phase and upto 16% speedup in autoregressive phase\r\n\r\n\r\nChange-Id: Ie574ff8459fdb75701ae366158b4e118c70694e4\r\n\r\n\r\ncc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @malfet @snadampal @milpuz01",
        "open",
        "2025-02-05T11:50:00Z",
        null,
        null,
        "module: cpu, triaged, open source, module: arm, ciflow/trunk, topic: performance, release notes: intel, merging",
        "main",
        "kleidiai_threading_improvement",
        1,
        171,
        312,
        1,
        10,
        0,
        "malfet",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146476"
    ],
    [
        146475,
        "fix: replace stderr with stdout for download messages in hub.py",
        "This PR addresses an issue where download logs in `hub.py` are sent to `stderr` instead of `stdout`. Hence, when running models with workers, these messages are incorrectly categorized as errors, leading to confusion. ",
        "open",
        "2025-02-05T10:29:29Z",
        null,
        null,
        "triaged, open source, ciflow/trunk, topic: not user facing, merging",
        "main",
        "main",
        1,
        2,
        2,
        1,
        12,
        0,
        "mikaylagawarecki",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146475"
    ],
    [
        146474,
        "Fix torch.take_along_dim param type and default description",
        "## Changes\r\n\r\n- Change type description to `LongTensor`, consistent with [`torch.take`](https://pytorch.org/docs/stable/generated/torch.take.html)\r\n- Add `dim` param default value description\r\n\r\n## Test Result\r\n\r\n**Before**\r\n![image](https://github.com/user-attachments/assets/720ce158-2bc1-48b5-a188-56fcc7188d96)\r\n\r\n**After**\r\n![image](https://github.com/user-attachments/assets/05fe20bd-9476-4b97-ac2b-9b161d6532a1)\r\n\r\n",
        "open",
        "2025-02-05T10:01:46Z",
        null,
        null,
        "triaged, open source, ciflow/trunk, release notes: python_frontend, merging",
        "main",
        "opt/docs/take_along_dim",
        1,
        2,
        2,
        1,
        7,
        0,
        "mikaylagawarecki",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146474"
    ],
    [
        146473,
        "[export] Fix logger handler",
        "Differential Revision: D69169179\n\n\n",
        "open",
        "2025-02-05T08:28:27Z",
        null,
        null,
        "fb-exported, release notes: export",
        "main",
        "export-D69169179",
        1,
        2,
        0,
        1,
        2,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146473"
    ],
    [
        146472,
        "Refactoring pipeline parallelism test cases to be device agnostic [1/n]",
        "In this series of PR we intend to refactor pipeline parallelism test cases to enable to be completely device agnostic.\r\n\r\nThese changes will include the following approaches to do the same :\r\n\r\n\r\n- Allowing for multiple device types using instantiate_device_type_test\r\n- Replacing calls to cuda stream with torch.get_device_module(device) wherever it applies\r\n\r\nThis should result in improvement in usability for all devices\r\n\r\n\r\nFor this PR we have shown support for the following devices:\r\n\r\n- CPU (wherever applicable)\r\n- CUDA\r\n- HPU\r\n- XPU\r\n\r\nTo add other device new users can simply append their device to the device list \r\n\r\ncc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o",
        "open",
        "2025-02-05T07:53:31Z",
        null,
        null,
        "oncall: distributed, triaged, open source, ciflow/trunk, topic: not user facing, merging, module: pipelining",
        "main",
        "AnantGulati_pipeline_refactoring",
        4,
        57,
        41,
        5,
        5,
        1,
        "H-Huang",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146472"
    ],
    [
        146467,
        "Fix an issue where functional collectives don't force fx stride on inputs when compiled",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146467\n\r\nFixes https://github.com/pytorch/pytorch/issues/146416\r\n\r\nAlso added contiguity checks in the C++ functional collective ops to prevent striding issues introduced during compilation manifest as silent correctness issues.\r\n\r\ncc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @desertfire @chauhang @aakhundov",
        "open",
        "2025-02-05T03:54:36Z",
        null,
        null,
        "oncall: distributed, ciflow/trunk, release notes: distributed (c10d), module: inductor, ciflow/inductor, merging",
        "gh/yifuwang/193/base",
        "gh/yifuwang/193/head",
        4,
        105,
        27,
        5,
        8,
        1,
        "Chillee, lw, shunting314",
        "APPROVED, APPROVED, APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146467"
    ],
    [
        146466,
        "Fix one_hot inconsistent errors after compile",
        "Fixes #146274\r\n\r\n**Test Result**\r\n\r\n```python\r\n>>> import torch\r\n>>> f = torch.nn.functional.one_hot\r\n>>> a = torch.arange(0, 5) % 3  # [0,1,2,0,1]\r\n>>> num_classes = 0\r\n>>> torch.nn.functional.one_hot(a,num_classes)\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nRuntimeError: Class values must be smaller than num_classes.\r\n\r\n>>> torch.compile(torch.nn.functional.one_hot)(a,num_classes)\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/zong/code/pytorch/torch/_dynamo/eval_frame.py\", line 570, in _fn\r\n    return fn(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/zong/code/pytorch/torch/_dynamo/external_utils.py\", line 48, in inner\r\n    return fn(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^\r\nRuntimeError: Class values must be smaller than num_classes.\r\n\r\n```\r\n\r\ncc @bdhirsh",
        "open",
        "2025-02-05T02:47:27Z",
        null,
        null,
        "triaged, open source, topic: not user facing",
        "main",
        "fix/aten/one_hot",
        1,
        18,
        9,
        1,
        2,
        1,
        "zou3519",
        "COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146466"
    ],
    [
        146464,
        "[symbolic shapes] Log id for each SymNode",
        "Fixes #ISSUE_NUMBER\r\n\n\ncc @ezyang @SherlockNoMad @EikanWang @jgong5 @wenzhe-nrv",
        "open",
        "2025-02-05T01:32:37Z",
        null,
        null,
        "release notes: fx, fx, ciflow/inductor",
        "main",
        "angelayi/provenance_id",
        3,
        148,
        92,
        1,
        1,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146464"
    ],
    [
        146456,
        "Fix workarea compute in lapackSyevd",
        "work-query APIs return floating point values, that could loose precision when converted back to int. Solve this by using `nextafter` and `ceil`\r\nAdd regression test \r\n\r\nFixes #145801\r\n",
        "open",
        "2025-02-05T00:24:37Z",
        null,
        null,
        "ciflow/trunk, release notes: linalg_frontend, merging",
        "main",
        "wdvr/iss_145801",
        2,
        14,
        2,
        5,
        4,
        0,
        "malfet",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146456"
    ],
    [
        146455,
        "[logging] Save compile state in CompiledFxGraph and make it available at runtime",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146455\n\r\nSummary: To support logging the correct compile_id for runtime timings (like Triton autotuning), save the compile_id in CompiledFxGraph make it available to logging utilities, i.e., dynamo_timed.\r\n\r\nThe previous attempt put the compile_id in the inductor_metadata with the Triton output code, but that broke Triton caching and we reverted. This version does the following:\r\n* When creating or deserializing a CompiledFxGraph, save the compile-time compile_id.\r\n* Implement a class `RuntimeCompileContext` that's analogous to `CompileContext` where we can look up the compile_id at runtime.\r\n* Set this runtime compile context during `CompiledFxGraph.__call__`.\r\n* Removes the compile_id as a param to dynamo_timed; dynamo_timed can figure it out instead.\r\n* Removes separate dynamo_timed params for compile-time and runtime dynamo_compile column names. We can use one param have dynamo_timed figure out whether to treat as a runtime or compile-time event.\r\n\r\nTest Plan:\r\n* tlparse (`python benchmarks/dynamo/torchbench.py --performance --training --amp --backend inductor --device cuda --print-compilation-time --repeat 5 --cold-start-latency --only nanogpt`): https://fburl.com/bu5i8efk\r\n* dynamo_compile: https://fburl.com/scuba/dynamo_compile/sandbox/3d74ps92\r\n* pt2_compile_events: https://fburl.com/scuba/pt2_compile_events/ooqoe5tu\r\n\r\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @desertfire @chauhang @aakhundov",
        "open",
        "2025-02-05T00:23:56Z",
        null,
        null,
        "topic: not user facing, module: inductor, module: dynamo, ciflow/inductor",
        "gh/masnesral/176/base",
        "gh/masnesral/176/head",
        7,
        113,
        74,
        5,
        2,
        0,
        "masnesral, masnesral, masnesral, masnesral, masnesral, masnesral, masnesral, masnesral, masnesral, masnesral, xmfan, masnesral, masnesral, xmfan, xmfan, masnesral, xmfan, xmfan, masnesral",
        "COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146455"
    ],
    [
        146454,
        "[dynamo][fullgraph] Raise NoGraphError if no graph with fullgraph=True",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146454\n* #146507\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-02-04T23:50:47Z",
        null,
        null,
        "ciflow/trunk, topic: not user facing, module: dynamo, ciflow/inductor",
        "gh/anijain2305/669/base",
        "gh/anijain2305/669/head",
        3,
        77,
        0,
        4,
        1,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146454"
    ],
    [
        146452,
        "cpp_wrapper: enable all CI inductor tests",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146452\n* #146706\n* #146424\n* #146109\n* #146449\n* #144349\n* #144293\n* #144002\n\nWith the speedups from precompiled headers, we can now enable all currently enabled CI tests for inductor in cpp_wrapper mode.",
        "open",
        "2025-02-04T22:58:10Z",
        null,
        null,
        "open source, topic: not user facing, ciflow/inductor, keep-going, ci-no-test-timeout",
        "gh/benjaminglass1/66/base",
        "gh/benjaminglass1/66/head",
        1,
        25,
        45,
        6,
        1,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146452"
    ],
    [
        146449,
        "cpp_wrapper: handle mixed-device C-shim fallbacks",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #146452\n* #146706\n* #146424\n* #146109\n* __->__ #146449\n* #144349\n* #144293\n* #144002\n\nFixes an error from test_torch, where a CUDA cpp_wrapper run called a CUDA native C-shim kernel with two CPU tensors.\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @desertfire @chauhang @aakhundov",
        "open",
        "2025-02-04T22:07:25Z",
        null,
        null,
        "open source, topic: not user facing, module: inductor, ciflow/inductor",
        "gh/benjaminglass1/65/base",
        "gh/benjaminglass1/65/head",
        5,
        90,
        42,
        4,
        1,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146449"
    ],
    [
        146448,
        "[ROCm] Indexing perf optimization via Unroll/WideFetch/IdxReuse/OneDupOpt",
        "Fixes #ISSUE_NUMBER\r\n\n\ncc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd",
        "open",
        "2025-02-04T21:56:59Z",
        null,
        null,
        "module: rocm, triaged, open source, release notes: cuda, ciflow/rocm",
        "main",
        "main",
        1,
        49,
        0,
        1,
        2,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146448"
    ],
    [
        146443,
        "Stop poisoning fork on Dataloader creation when pin_memory is enabled",
        "Fixes https://github.com/pytorch/pytorch/issues/144687\r\nNeeds https://github.com/pytorch/pytorch/pull/146098 that already landed to fix the issue above\r\n\r\nA longer-term fix would be to move cuda's non-poisoning is_available() check to c++. But that would be quite a bit of work.\r\n\r\nThis PR also updates the behavior of current_accelerator() in python to match getAccelerator() in C++ and update all docs to reflect that.",
        "open",
        "2025-02-04T20:54:13Z",
        null,
        null,
        "release notes: dataloader, topic: bug fixes",
        "main",
        "fix_dataloader",
        6,
        66,
        26,
        5,
        1,
        1,
        "ngimel, guangyey, guangyey, guangyey, guangyey, andrewkho",
        "APPROVED, COMMENTED, APPROVED, COMMENTED, COMMENTED, APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146443"
    ],
    [
        146440,
        "[sigmoid] Implement a OSS only model runner.",
        "Summary: Implement an oss version of modelrunner with clean dependencies. The new oss model runner only removes thrift and only use json header to load the model.\n\nTest Plan: Test will be added in the next diff separately. (D69060784)\n\nDifferential Revision: D68846877\n\n\n",
        "open",
        "2025-02-04T19:38:20Z",
        null,
        null,
        "fb-exported, ciflow/trunk, ciflow/inductor, release notes: export",
        "main",
        "export-D68846877",
        1,
        5,
        4,
        1,
        5,
        0,
        "SherlockNoMad",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146440"
    ],
    [
        146436,
        "[Testing] Reduce `test_exp` flakiness",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146436\n\r\nBy setting `reference_in_float` to false,  as `exp(a + b)` could yield significantly different results than `exp(a.half()+b.half())` as one can see in the following example (which is accidentally the random values generated by MacOS RNG for this test)\r\n\r\n```\r\n>>> import torch\r\n>>> x=torch.tensor(2.5599, dtype=torch.half)\r\n>>> y=torch.tensor(0.6970, dtype=torch.half)\r\n>>> (x + y).exp()\r\ntensor(26., dtype=torch.float16)\r\n>>> (x.float() + y.float()).exp()\r\ntensor(25.9799)\r\n```\r\n\r\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @desertfire @chauhang @aakhundov",
        "open",
        "2025-02-04T18:54:11Z",
        null,
        null,
        "Merged, Reverted, topic: not user facing, ciflow/mps, module: inductor, ciflow/inductor, ci-no-td",
        "gh/malfet/169/base",
        "gh/malfet/169/head",
        2,
        9,
        2,
        2,
        6,
        0,
        "dcci, malfet, dcci, jansel",
        "COMMENTED, COMMENTED, APPROVED, APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146436"
    ],
    [
        146427,
        "add the `torch.float8_e8m0fnu` dtype to PyTorch",
        "Summary:\r\n\r\nAdds the `torch.float8_e8m0fnu` dtype to PyTorch, as detailed in\r\nhttps://github.com/pytorch/pytorch/issues/146414 . Please see the issue for a detailed definition of the format.  Example of basic functionality:\r\n\r\n```python\r\nimport torch\r\n\r\n# round trip\r\nx0 = torch.randn(4, 4, dtype=torch.float32)\r\nx1 = x0.to(torch.float8_e8m0fnu)  # RNE rounding\r\nx2 = x1.to(torch.float32)  # 2 ** exponent\r\n\r\n# creation with empty\r\nx0 = torch.empty(4, 4, dtype=torch.float8_e8m0fnu)\r\n\r\n# printing\r\nprint(x0)\r\n```\r\n\r\nDone in this PR:\r\n* numerical correctness\r\n* op coverage (except for `torch._scaled_mm`): create tensor, cast to/from float32\r\n* printing a tensor works\r\n\r\nFor future PRs:\r\n* performance optimizations for casting\r\n* torch._scaled_mm\r\n* PT2\r\n* various cleanups (detailed in comments with issue numbers)\r\n\r\nTest Plan:\r\n\r\n```\r\npytest test/quantization/core/experimental/test_float8.py -s\r\n```\r\n\r\nReviewers:\r\n\r\nSubscribers:\r\n\r\nTasks:\r\n\r\nTags:\r\n\r\ncc @yanbing-j @albanD @kadeng @penguinwu @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10",
        "open",
        "2025-02-04T18:07:11Z",
        null,
        null,
        "module: cpu, release notes: quantization, module: float8",
        "main",
        "gh/vkuzo/1/head",
        23,
        508,
        43,
        8,
        2,
        0,
        "vkuzo, vkuzo, vkuzo, vkuzo, vkuzo, vkuzo, vkuzo, vkuzo, vkuzo, vkuzo, vkuzo, drisspg, drisspg, drisspg, drisspg, vkuzo, vkuzo, drisspg, drisspg, vkuzo, vkuzo, eqy, vkuzo, drisspg",
        "COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146427"
    ],
    [
        146426,
        "Test typing of arithmetic operators on Tensor (see #145838)",
        "See #145838\r\n\r\nStack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146426\n\r\n",
        "open",
        "2025-02-04T18:06:50Z",
        null,
        null,
        "open source, ciflow/trunk, topic: not user facing, merging",
        "gh/rec/131/base",
        "gh/rec/131/head",
        2,
        462,
        0,
        5,
        10,
        0,
        "Skylion007, Skylion007, rec, rec",
        "COMMENTED, APPROVED, COMMENTED, COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146426"
    ],
    [
        146424,
        "cpp_wrapper: fix test_torchinductor* tests",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #146452\n* #146706\n* __->__ #146424\n* #146109\n* #146449\n* #144349\n* #144293\n* #144002\n\n\n\ncc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @voznesenskym @penguinwu @EikanWang @Guobing-Chen @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @desertfire @chauhang @aakhundov",
        "open",
        "2025-02-04T18:04:13Z",
        null,
        null,
        "module: cpu, open source, topic: not user facing, module: inductor, ciflow/inductor",
        "gh/benjaminglass1/64/base",
        "gh/benjaminglass1/64/head",
        3,
        22,
        7,
        5,
        1,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146424"
    ],
    [
        146421,
        "experimental specialization logging",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146421\n\n\n\ncc @ezyang @SherlockNoMad @EikanWang @jgong5 @wenzhe-nrv\n\nDifferential Revision: [D69163120](https://our.internmc.facebook.com/intern/diff/D69163120)",
        "open",
        "2025-02-04T17:34:46Z",
        null,
        null,
        "ciflow/trunk, release notes: fx, fx, ciflow/inductor",
        "gh/bobrenjc93/270/base",
        "gh/bobrenjc93/270/head",
        1,
        37,
        1,
        6,
        2,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146421"
    ],
    [
        146420,
        "[ROCm] Enable tunable warp size for stride one indexing backwards kernel",
        "Enable tunable warp size for stride one indexing backwards kernel. This will allow for the indexing backward kernel with stride one to work on smaller warp sizes.\n\ncc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd",
        "open",
        "2025-02-04T17:24:38Z",
        null,
        null,
        "module: rocm, triaged, open source, topic: not user facing, ciflow/periodic, rocm, ciflow/rocm",
        "main",
        "improve-backwards-indexing-with-stride-1",
        1,
        74,
        0,
        1,
        1,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146420"
    ],
    [
        146418,
        "[BE]: Add TypeVarTuple to RNN Args for better type inference",
        "Fixes #ISSUE_NUMBER\r\n",
        "open",
        "2025-02-04T16:46:07Z",
        null,
        null,
        "open source",
        "main",
        "skylion007/typevartuple-nn-rnn-2025-02-04",
        1,
        5,
        4,
        1,
        2,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146418"
    ],
    [
        146417,
        "Only call triton in worker process, kick off worker processes earlier, during inductor codegen",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\r\n* __->__ #146417\r\n\r\n### Big idea\r\nThis PR extends https://github.com/pytorch/pytorch/pull/144288 by combining calling triton in worker processes with the future cache: we kick off triton compilation in the worker processes earlier, during inductor codegen. Basically instead of calling async_compile.triton for the first time only after the entire code has been generated, we start compiling as soon as we know we'll need to compile the kernel. Then, when loading the generated inductor code, we can simply read from our in memory future cache, considerably increasing the parallelism.\r\n### Implementation Overview\r\nIn total, the diff does the following:\r\n- Converts TritonFuture to LambdaFuture, only calling triton.compile on worker processes\r\n- Now that triton.compile() isn't called on the main process, we call TritonBundler on all compiled kernels when we get them back from workers\r\n- Extend @eellison's future cache to a class, mostly as a refactor\r\n- Finally, call async_compile.triton ahead of time in Scheduler.codegen if workers are warmed up. This causes the subsequent\r\nasync_compile.triton call that occurs after codegen to cache hit on cold start.\r\nIn the diffs after this, I will add more to CompiledTritonKernels so that TritonBundler, on a warm start, automatically populates the in memory cache on warm start with the existing triton kernels, avoiding calling triton altogether on warm starts.\r\nBecause LambdaFutures are much faster to kick off than TritonFutures, due to not needing to load from TritonCodeCache at all, the time spent kicking off these worker jobs is pretty minimal for inductor codegen.\r\n\r\nDifferential Revision: [D69123174](https://our.internmc.facebook.com/intern/diff/D69123174/)\r\n\r\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @desertfire @chauhang @aakhundov",
        "open",
        "2025-02-04T16:20:19Z",
        null,
        null,
        "fb-exported, ciflow/trunk, topic: not user facing, module: inductor, ciflow/inductor",
        "gh/jamesjwu/106/base",
        "gh/jamesjwu/106/head",
        9,
        250,
        77,
        16,
        17,
        1,
        "jamesjwu, jamesjwu, masnesral, jamesjwu, jansel",
        "COMMENTED, COMMENTED, COMMENTED, COMMENTED, CHANGES_REQUESTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146417"
    ],
    [
        146415,
        "Only call triton in worker process, ahead of time compile",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146415\n\n# Big idea\nThis PR extends https://github.com/pytorch/pytorch/pull/144288 by combining calling triton in worker processes with the future cache: we kick off triton compilation in the worker processes earlier, during inductor codegen. Basically instead of calling async_compile.triton for the first time only after the entire code has been generated, we start compiling as soon as we know we'll need to compile the kernel. Then, when loading the generated inductor code, we can simply read from our in memory future cache, considerably increasing the parallelism.\n# Implementation Overview\nIn total, the diff does the following:\n- Converts TritonFuture to LambdaFuture, only calling triton.compile on worker processes\n- Now that triton.compile() isn't called on the main process, we call TritonBundler on all compiled kernels when we get them back from workers\n- Extend @eellison's future cache to a class, mostly as a refactor\n- Finally, call async_compile.triton ahead of time in Scheduler.codegen if workers are warmed up. This causes the subsequent\nasync_compile.triton call that occurs after codegen to cache hit on cold start.\nIn the diffs after this, I will add more to CompiledTritonKernels so that TritonBundler, on a warm start, automatically populates the in memory cache on warm start with the existing triton kernels, avoiding calling triton altogether on warm starts.\nBecause LambdaFutures are much faster to kick off than TritonFutures, due to not needing to load from TritonCodeCache at all, the time spent kicking off these worker jobs is pretty minimal for inductor codegen.\n\n### Can we split the diff for easier review?\nIt's best if this diff lands atomically with all of these changes, as doing the ahead of time codegen compile is only performant if we replace TritonFuture with LambdaFuture(as we don't need to load the triton kernel on the main process). However, I've made a diff stack for easier reviewing here:\n- D69070048 - Run async_compile.triton ahead of time in Scheduler.codegen\n- D68633454 - Only call triton in worker process\n\nDifferential Revision: [D69070616](https://our.internmc.facebook.com/intern/diff/D69070616/)\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @desertfire @chauhang @aakhundov",
        "open",
        "2025-02-04T16:14:20Z",
        null,
        null,
        "fb-exported, module: inductor, ciflow/inductor",
        "gh/jamesjwu/105/base",
        "gh/jamesjwu/105/head",
        6,
        111,
        63,
        1,
        3,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146415"
    ],
    [
        146407,
        "[ROCm] Unskip std:bad_alloc failures",
        "Flakey MI300 issue related to memory usage should now be resolved after https://github.com/pytorch/pytorch/actions/runs/13007160888?pr=145829.\n\ncc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @hongxiayang @naromero77amd @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @desertfire @chauhang @aakhundov",
        "open",
        "2025-02-04T14:14:17Z",
        null,
        null,
        "module: rocm, triaged, open source, ciflow/trunk, topic: not user facing, module: inductor, ciflow/inductor, merging, ciflow/rocm",
        "main",
        "unskip-bad-alloc",
        1,
        0,
        7,
        1,
        4,
        0,
        "jeffdaily",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146407"
    ],
    [
        146406,
        "Only enable aotriton on x86_64 and aarch64",
        "Make `USE_FLASH_ATTENTION` and `USE_MEM_EFF_ATTENTION` depend on `CPU_INTEL OR CPU_AARCH64`.\r\n\r\n[aotriton pre-built](https://github.com/ROCm/aotriton/releases) is only available on x86_64.\r\n\r\nAlthough `AOTRITON_INSTALL_FROM_SOURCE` can be specified to build from source, building aotriton requires CUDA, so on architectures without CUDA support (like riscv64), it still needs to be disabled.",
        "open",
        "2025-02-04T13:33:47Z",
        null,
        null,
        "triaged, open source, topic: not user facing",
        "main",
        "patch-1",
        1,
        3,
        2,
        1,
        4,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146406"
    ],
    [
        146403,
        "Use std::string_view",
        "Fixes #ISSUE_NUMBER\r\n",
        "open",
        "2025-02-04T12:59:16Z",
        null,
        null,
        "open source, topic: not user facing",
        "main",
        "string_view_gen2",
        2,
        2,
        1,
        1,
        1,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146403"
    ],
    [
        146395,
        "[dynamo][builtin-skipfile-cleanup] Remove random",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146395\n* #146339\n* #146116\n* #146322\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-02-04T05:35:35Z",
        null,
        null,
        "module: dynamo, ciflow/inductor, keep-going",
        "gh/anijain2305/668/base",
        "gh/anijain2305/668/head",
        1,
        0,
        2,
        1,
        2,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146395"
    ],
    [
        146393,
        "PEP585: More fixes 2",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146393\n* #146392\n* #146391\n\n\n\ncc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o @EikanWang @jgong5 @wenzhe-nrv @sanchitintel @ezyang @SherlockNoMad",
        "open",
        "2025-02-04T04:20:02Z",
        null,
        null,
        "oncall: distributed, oncall: jit, release notes: quantization, fx, ciflow/inductor, release notes: AO frontend",
        "gh/aorenste/217/base",
        "gh/aorenste/217/head",
        30,
        62,
        76,
        1,
        1,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146393"
    ],
    [
        146392,
        "PEP585: More fixes",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #146393\n* __->__ #146392\n* #146391\n\n",
        "open",
        "2025-02-04T04:19:56Z",
        null,
        null,
        "release notes: onnx, module: inductor, module: dynamo, ciflow/inductor",
        "gh/aorenste/216/base",
        "gh/aorenste/216/head",
        30,
        108,
        147,
        1,
        1,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146392"
    ],
    [
        146391,
        "PEP585: Add noqa to necessary tests",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146391\n\n",
        "open",
        "2025-02-04T04:19:51Z",
        null,
        null,
        "topic: not user facing",
        "gh/aorenste/215/base",
        "gh/aorenste/215/head",
        7,
        63,
        31,
        7,
        1,
        1,
        "justinchuby, justinchuby, albanD, aorenste",
        "COMMENTED, APPROVED, COMMENTED, COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146391"
    ],
    [
        146388,
        "[WIP][CUDA][cuDNN] Experimental `cudnn_rms_norm`",
        "opt-in for now behind two new native functions---the plan would be to eventually add it as the `CUDA:` backend to `rms_norm`\r\n\r\nInitial experiments show forward ~4-5x speed, up fwd+bwd ~3x speedup\n\ncc @csarofeen @ptrblck @xwang233 @msaroufim",
        "open",
        "2025-02-04T02:55:56Z",
        null,
        null,
        "module: cudnn, module: cuda, open source, module: norms and normalization, topic: not user facing",
        "main",
        "cudnnrmsforward",
        5,
        422,
        0,
        4,
        5,
        0,
        "albanD, eqy",
        "COMMENTED, COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146388"
    ],
    [
        146387,
        "AMD: reverting to default config for better performance.",
        "TopK performance on ROCm performs better on the test suite with the default config.",
        "open",
        "2025-02-04T02:49:19Z",
        null,
        null,
        "open source, release notes: cuda",
        "main",
        "topk_rocm_tune",
        1,
        0,
        9,
        1,
        2,
        1,
        "malfet",
        "APPROVED",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146387"
    ],
    [
        146385,
        "[WIP] Confirm XPU Regression",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146385\n\n",
        "open",
        "2025-02-04T02:32:40Z",
        null,
        null,
        "triaged, open source, topic: not user facing, ciflow/xpu",
        "gh/EikanWang/74/base",
        "gh/EikanWang/74/head",
        1,
        1,
        1,
        1,
        1,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146385"
    ],
    [
        146372,
        "[Submodule] Turning flash-attention integration into 3rd party submod (#144120)",
        "Summary:\n\n# Summary\n\n### Sticky points\n\nCuda-graph rng handling has changed / deviated from original implementation. We will be left with a dangling 'offset' val and confusing naming due to BC\n\n## Dependencies\n- Flash PR: https://github.com/Dao-AILab/flash-attention/pull/1419\n\n### Other Points\n- The BC linter is complaining about losing generate.py and its functions which is not real BC surface\ncc albanD\n\nimported-using-ghimport\n\nTest Plan:\nImported from OSS\n\nBuilding in dev\n`buck build @//mode/dev-nosan -c fbcode.nvcc_arch=h100a  //caffe2:ATen-cu --show-full-output    `\n\nI and Nming the .so I do see that the flash symbols are correctly named:\n```\n0000000001c3dfb0 t pytorch_flash::run_mha_bwd(pytorch_flash::Flash_bwd_params&, CUstream_st*)::$_0::operator()() const::{lambda()#1}::operator()() const::{lambda()#1}::operator()() const::{lambda()#7}::operator()() const\n0000000001c36080 t pytorch_flash::run_mha_fwd(pytorch_flash::Flash_fwd_params&, CUstream_st*, bool)::$_0::operator()() const::{lambda()#2}::operator()() const::{lambda()#1}::operator()() const::{lambda()#6}::operator()() const\n0000000001c360e0 t pytorch_flash::run_mha_fwd(pytorch_flash::Flash_fwd_params&, CUstream_st*, bool)::$_0::operator()() const::{lambda()#2}::operator()() const::{lambda()#1}::operator()() const::{lambda()#7}::operator()() const\n0000000001c35fc0 t pytorch_flash::run_mha_fwd(pytorch_flash::Flash_fwd_params&, CUstream_st*, bool)::$_0::operator()() const::{lambda()#1}::operator()() const::{lambda()#1}::operator()() const::{lambda()#6}::operator()() const\n0000000001c36020 t pytorch_flash::run_mha_fwd(pytorch_flash::Flash_fwd_params&, CUstream_st*, bool)::$_0::operator()() const::{lambda()#1}::operator()() const::{lambda()#1}::operator()() const::{lambda()#7}::operator()() const\n```\n\nReviewed By: vkuzo\n\nDifferential Revision: D68502879\n\nPulled By: drisspg\n",
        "open",
        "2025-02-04T00:34:54Z",
        null,
        null,
        "fb-exported, ciflow/trunk, topic: not user facing, ciflow/inductor, module: sdpa",
        "main",
        "export-D68502879",
        30,
        121,
        4214,
        1,
        9,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146372"
    ],
    [
        146367,
        "[dynamo][EXPERIMENT] Prototype for `mark_traceable`",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146367\n* #146714\n* #146713\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-02-04T00:05:53Z",
        null,
        null,
        "module: dynamo, ciflow/inductor",
        "gh/StrongerXi/81/base",
        "gh/StrongerXi/81/head",
        8,
        366,
        30,
        2,
        2,
        1,
        "StrongerXi, zou3519, StrongerXi, StrongerXi, StrongerXi, StrongerXi, zou3519",
        "COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146367"
    ],
    [
        146364,
        "[DeviceMesh] Add some documentation for `from_group` API and add a 2D test",
        "Fixes #ISSUE_NUMBER\r\n\n\ncc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wconstab @d4l3k @c-p-i-o @tianyu-l @XilunWu",
        "open",
        "2025-02-03T22:54:40Z",
        null,
        null,
        "oncall: distributed, module: dtensor, release notes: distributed (dtensor)",
        "main",
        "add_from_group_doc_and_test",
        2,
        102,
        11,
        1,
        1,
        2,
        "fduwjj, fduwjj, wz337, fegin, wz337",
        "COMMENTED, APPROVED, COMMENTED, COMMENTED, COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146364"
    ],
    [
        146356,
        "[cutlass backend] fix bug for accuminator dtype",
        "Will add unit tests for accuracy. \r\n\r\nStack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #146743\n* __->__ #146356\n\r\n\r\n\r\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @desertfire @chauhang @aakhundov",
        "open",
        "2025-02-03T22:03:13Z",
        null,
        null,
        "Merged, Reverted, ciflow/trunk, topic: not user facing, module: inductor, ciflow/inductor, ci-no-td",
        "gh/henrylhtsang/2/base",
        "gh/henrylhtsang/2/head",
        2,
        7,
        74,
        5,
        10,
        0,
        "Chillee",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146356"
    ],
    [
        146355,
        "[dynamo] replace hardcoded eval frame control flags skip_code_recursive_flag/cache_limit_hit_flag",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146355\n* #145603\n\r\nThis PR and the previous:\r\n- Moves parts of `eval_frame.c` to C++.\r\n- Reduces code duplication in `dynamo__custom_eval_frame` and makes the control flow more clear.\r\n- Enables `convert_frame` to signal to `eval_frame.cpp` in a general manner how to evaluate this frame, recursive frames, and future frames with the same code object (default/compile, skip, run-only). e.g. this will allow us to change skipping/cache limit hit eval_frame behavior directly from convert_frame without requiring changes to C/C++.\r\n\r\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-02-03T22:01:46Z",
        null,
        null,
        "topic: not user facing, module: dynamo, ciflow/inductor",
        "gh/williamwen42/204/base",
        "gh/williamwen42/204/head",
        9,
        215,
        172,
        5,
        1,
        0,
        "williamwen42, jansel, anijain2305, williamwen42, jansel",
        "COMMENTED, CHANGES_REQUESTED, COMMENTED, COMMENTED, COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146355"
    ],
    [
        146352,
        "Build a storage reader/writer to write checkpoints in HF format",
        "Summary: Title - we want to write checkpoints in HF format with DCP, this diff allows this for the non-distributed use case.\r\n\r\nTest Plan:\r\nbuck2 test 'fbcode//mode/dev-nosan' fbcode//caffe2/test/distributed/checkpoint:test_hf_torchtune_storage\r\n\r\nN6476188 --> able to save and load tensor in hf format\r\n\r\nDifferential Revision: D68444967\r\n\r\n\r\n\r\n\r\ncc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o @LucasLLC @MeetVadakkanchery @mhorowitz @pradeepfn @ekr0",
        "open",
        "2025-02-03T21:20:40Z",
        null,
        null,
        "oncall: distributed, fb-exported, ciflow/trunk, module: distributed_checkpoint",
        "main",
        "export-D68444967",
        4,
        316,
        5,
        1,
        6,
        0,
        "saumishr, ankitageorge, ankitageorge, ankitageorge, ankitageorge, ankitageorge, fegin, ankitageorge, ankitageorge",
        "COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146352"
    ],
    [
        146341,
        "Only call triton in worker process; run async_compile.triton ahead of time in Scheduler.codegen",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* (to be filled)\n\n### Big idea\nThis PR extends https://github.com/pytorch/pytorch/pull/144288 by combining calling triton in worker processes with the future cache: we kick off triton compilation in the worker processes earlier, during inductor codegen. Basically instead of calling async_compile.triton for the first time only after the entire code has been generated, we start compiling as soon as we know we'll need to compile the kernel. Then, when loading the generated inductor code, we can simply read from our in memory future cache, considerably increasing the parallelism.\n\n### Implementation Overview\nIn total, the diff does the following:\n- Converts TritonFuture to LambdaFuture, only calling triton.compile on worker processes\n- Now that triton.compile() isn't called on the main process, we call TritonBundler on all compiled kernels when we get them back from workers\n- Extend @eellison's future cache to a class, mostly as a refactor\n- Finally, call async_compile.triton ahead of time in Scheduler.codegen if workers are warmed up. This causes the subsequent async_compile.triton call that occurs after codegen to cache hit on cold start.\n\nIn the diffs after this, I will add more to CompiledTritonKernels so that TritonBundler, on a warm start, automatically populates the in memory cache on warm start with the existing triton kernels, avoiding calling triton altogether on warm starts.\n\nBecause LambdaFutures are much faster to kick off than TritonFutures, due to not needing to load from TritonCodeCache at all, the time spent kicking off these worker jobs is pretty minimal for inductor codegen.\n\n### Can we split the diff for easier review?\nIt's best if this diff lands atomically with all of these changes, as doing the ahead of time codegen compile is only performant if we replace TritonFuture with LambdaFuture(as we don't need to load the triton kernel on the main process). However, I've made a diff stack for easier reviewing here:\nD69070048 - Run async_compile.triton ahead of time in Scheduler.codegen\nD68633454 - Only call triton in worker process\n\nDifferential Revision: [D69013710](https://our.internmc.facebook.com/intern/diff/D69013710/)\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @desertfire @chauhang @aakhundov",
        "open",
        "2025-02-03T20:46:41Z",
        null,
        null,
        "module: inductor, ciflow/inductor",
        "gh/jamesjwu/102/base",
        "gh/jamesjwu/102/head",
        6,
        108,
        71,
        1,
        2,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146341"
    ],
    [
        146335,
        "[dynamic shapes][WIP] mark backed size symbols as size-like",
        "experimental, to apply upper-bound / maxsize size-oblivious semantics to backed symbols\r\n\r\n\r\ncc @ezyang @SherlockNoMad @EikanWang @jgong5 @wenzhe-nrv @voznesenskym @penguinwu @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-02-03T19:56:25Z",
        null,
        null,
        "ciflow/trunk, release notes: fx, fx, module: dynamo, ciflow/inductor",
        "main",
        "pianpwk/treat_sizes_as_size_like",
        5,
        31,
        8,
        7,
        2,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146335"
    ],
    [
        146333,
        "Add optional generator to distribution sampler/rsample methods.",
        "Fixes part of #45115 and #11340\r\nAdds a generator parameter to all the sample/rsample methods of torch distribution classes\n\ncc @fritzo @neerajprad @alicanb @nikitaved",
        "open",
        "2025-02-03T19:47:59Z",
        null,
        null,
        "module: distributions, triaged, open source, topic: not user facing",
        "main",
        "features/distribution_generator",
        30,
        160,
        138,
        1,
        4,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146333"
    ],
    [
        146324,
        "[torch][amdsmi] Avoid ODR violation when loading amdsmi",
        "Summary:\namdsmi bundles its own copy of `libamd_smi.so`. When you're interacting with `amdsmi` from *only* python that's fine, but when you try to interact with `libamd_smi.so` from native code too this poses a problem, because from native code you'll be linking against the copy of `libamd_smi.so` from the SDK.\n\nThis means you'll end up with 2 copies of `libamd_smi.so` in your process, and potentially (Murphey's law says you will, as does our CI) violate ODR.\n\nIn order to avoid this issue from the PT side of the world we can hook the `dlopen(\"path/to/bundled/libamd_smi.so\")` and try to use the already loaded/SDK version of `libamd_smi.so` first, before proceeding to use the `path/to/bundled/libamd_smi.so`.\n\nTest Plan: CI, inspect process using libamd_smi.so from native + python and observe only a single copy loaded\n\nDifferential Revision: D69064038\n\n\n",
        "open",
        "2025-02-03T18:59:18Z",
        null,
        null,
        "fb-exported, ciflow/trunk, topic: not user facing",
        "main",
        "export-D69064038",
        1,
        41,
        1,
        1,
        14,
        1,
        "malfet, danzimm, malfet, malfet",
        "COMMENTED, COMMENTED, APPROVED, COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146324"
    ],
    [
        146321,
        "[ONNX] Support custom axis name through dynamic_shapes",
        "Fixes #143443\r\n\r\nThis PR aims to support custom dynamic axis naming through dynamic_shapes. Currently, _Dim and _DimHint do not support dynamic axis naming (#144273).\r\n\r\n1. **the original dynamic shapes guarantee**\r\nThe axis renaming is only applied when dynamic shapes include string instead of all _Dim and _DimHint. Thus, there will not be any inconsistent behavior to dynamic_shapes with torch.export.export if the given dynamic shapes follow torch.export.export format.\r\n2. _DimHint.AUTO is applied to the axes that are specified with custom names to avoid exporter crash. (_DimHint.DYNAMIC crashes when the export fails.)\r\n3.  There's no need to handle cases where kwargs are out of order with the model signature,\r\n    as torch.export.export supports dynamism only when kwargs and dynamic_shapes are provided in order.\r\n    https://github.com/pytorch/pytorch/blob/49082f9dba3b79a344cb03652972ddbe7c3729cc/torch/export/_trace.py#L2034\r\n4. If `torch.onnx.ExportedProgram` finds the axes share the same constraints, they will have the same name (e.g. s0, s1, ...). Therefore, even if the ONNX users specify them with different custom names, they won't be respected.\r\n\r\nExample model:\r\n```python\r\n        class NestedModel(torch.nn.Module):\r\n            def forward(\r\n                self,\r\n                x: torch.Tensor,\r\n                ys: list[torch.Tensor],\r\n                zs: dict[str, torch.Tensor],\r\n                c: torch.Tensor,\r\n            ):\r\n                y = ys[0] + ys[1] + zs[\"a\"] + zs[\"b\"]\r\n                w = 5\r\n                if x.shape[0] < 3 and c.shape[0] != 4:\r\n                    return x + w, x + y, c\r\n                else:\r\n                    return x - w, x - y, c\r\n\r\n        input = (\r\n            torch.ones(5),\r\n            [torch.zeros(5), torch.ones(5)],\r\n            {\"a\": torch.zeros(5), \"b\": torch.ones(5)},\r\n            torch.ones(6),\r\n        )\r\n\r\n        dynamic_shapes = (\r\n            {0: torch.export.Dim(\"dim_x\", min=3)},  # _Dim\r\n            [(\"custom_name_axis_ys_0\",), (torch.export.Dim.AUTO,)],  # custom name\r\n            {\r\n                \"a\": {0: torch.export.Dim.AUTO},\r\n                \"b\": (\"custom_name_axis_zs_b_0\",),\r\n            },  # _DimHint\r\n            {0: \"custom_name_axis_c_0\"},  # custom name\r\n        )\r\n\r\n```",
        "open",
        "2025-02-03T18:00:09Z",
        null,
        null,
        "open source, release notes: onnx, topic: new features",
        "main",
        "titaiwang/support_axis_name",
        6,
        722,
        247,
        9,
        1,
        0,
        "justinchuby, justinchuby, justinchuby, justinchuby, justinchuby, justinchuby, justinchuby, xadupre, xadupre, xadupre, xadupre, xadupre, justinchuby, titaiwangms, titaiwangms, titaiwangms, titaiwangms, titaiwangms, titaiwangms, titaiwangms, titaiwangms, titaiwangms, justinchuby, justinchuby",
        "COMMENTED, APPROVED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146321"
    ],
    [
        146319,
        "[export][dynamic shapes] use size-oblivious upper bound reasoning for backed symbols",
        "cc @ezyang @SherlockNoMad @EikanWang @jgong5 @wenzhe-nrv",
        "open",
        "2025-02-03T17:31:50Z",
        null,
        null,
        "release notes: fx, fx, ciflow/inductor",
        "main",
        "pianpwk/backed_symint_endofbounds",
        2,
        28,
        11,
        1,
        1,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146319"
    ],
    [
        146318,
        "Hack AC to not clear recomputed activations",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #146633\n* __->__ #146318\n* #145399\n* #145533\n* #145531\n* #145520\n\n",
        "open",
        "2025-02-03T17:29:36Z",
        null,
        null,
        "",
        "gh/soulitzer/351/base",
        "gh/soulitzer/351/head",
        1,
        2,
        2,
        1,
        2,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146318"
    ],
    [
        146313,
        "[Dynamo] fix torch._dynamo.assume_constant_result when used on class method",
        "This PR fixes `torch._dynamo.assume_constant_result` when it is used on a class method, and the class was instantiated inside of the dynamo traced code.\r\n\r\nThe issue: Currently when an object is modified by storing attributes on it in dynamo, the side effects of those attribute stores are tracked using the `SideEffect` system, and the modifications to the class are not performed in the first place.  For example, in\r\n```python\r\nclass A:\r\n    def __init__(self):\r\n        self.value = 123\r\n```\r\nThe `self.value` field will not be set on the class `A` but rather it will only be tracked within the `SideEffect` system.\r\n\r\nThis causes a problem with `torch._dynamo.assume_constant_result` as it converts the value in dynamo back into a normal python value and invokes the function as normal python.  However, it currently does not find the `self.value` field (as it was never set on the underlying object).  This PR checks if the object passed to the `torch._dynamo.assume_constant_result` has any pending mutations from the `SideEffect` system and applies them before calling the `torch._dynamo.assume_constant_result` function.\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-02-03T16:00:09Z",
        null,
        null,
        "triaged, open source, module: dynamo",
        "main",
        "dynamo-fixes-6",
        2,
        37,
        8,
        2,
        8,
        1,
        "anijain2305, StrongerXi, jansel, anijain2305",
        "APPROVED, COMMENTED, CHANGES_REQUESTED, COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146313"
    ],
    [
        146310,
        "Fix type stubs for SymmetricMemory",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146310\n* #146308\n\n",
        "open",
        "2025-02-03T14:11:18Z",
        null,
        null,
        "release notes: distributed (c10d)",
        "gh/lw/7/base",
        "gh/lw/7/head",
        1,
        35,
        4,
        1,
        1,
        1,
        "yifuwang",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146310"
    ],
    [
        146308,
        "Support SymmetricMemory's signaling kernels on sm60 and sm70",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #146310\n* __->__ #146308\n\nBy leveraging libcudacxx's utilities: https://nvidia.github.io/cccl/libcudacxx/extended_api/synchronization_primitives/atomic_ref.html\n\ncc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o",
        "open",
        "2025-02-03T13:35:20Z",
        null,
        null,
        "oncall: distributed, release notes: distributed (c10d)",
        "gh/lw/6/base",
        "gh/lw/6/head",
        3,
        36,
        58,
        1,
        1,
        0,
        "yifuwang",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146308"
    ],
    [
        146290,
        "[ c10d ] modify API to get device string from device with torch.device",
        "Modify the ```get_default_backend_for_device()``` API to extract the device string using ```torch.device()```\r\n\n\ncc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o",
        "open",
        "2025-02-03T03:24:49Z",
        null,
        null,
        "oncall: distributed, triaged, open source, release notes: distributed (c10d)",
        "main",
        "c10d_api_modification",
        1,
        1,
        1,
        1,
        2,
        0,
        "guangyey",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146290"
    ],
    [
        146289,
        "Use device agnostic APIs for device_count and backend in common_fsdp",
        "Replace device specific APIs with device abstracted API\r\n",
        "open",
        "2025-02-03T03:10:30Z",
        null,
        null,
        "triaged, open source, topic: not user facing",
        "main",
        "fsdp_common_cleanup",
        1,
        8,
        8,
        1,
        6,
        0,
        "guangyey",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146289"
    ],
    [
        146288,
        "[Trace PyDispatcher] Capture Vmapped autograd function as graph",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146288\n* #146272\n* #146271\n* #146270\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-02-03T02:14:22Z",
        null,
        null,
        "topic: not user facing, module: dynamo, ciflow/inductor",
        "gh/yanboliang/62/base",
        "gh/yanboliang/62/head",
        5,
        275,
        3,
        3,
        1,
        2,
        "yanboliang, zou3519, zou3519, zou3519, zou3519, zou3519, yanboliang, yanboliang, yanboliang, yanboliang, yanboliang, yanboliang, yanboliang, zou3519, zou3519, zou3519",
        "COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146288"
    ],
    [
        146285,
        "[scan] Autograd with partial gradient support",
        "This PR introduces the Autograd feature for scan with partial gradient support. It is a combination of the already opened PRs: https://github.com/pytorch/pytorch/pull/135631 and https://github.com/bohnstingl/pytorch/pull/4\r\n\r\ncc @ydwu4 ",
        "open",
        "2025-02-03T00:36:42Z",
        null,
        null,
        "triaged, open source, topic: not user facing",
        "main",
        "scan_autograd22",
        2,
        1324,
        212,
        4,
        2,
        1,
        "ydwu4",
        "COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146285"
    ],
    [
        146280,
        "[Inductor-CPU] Avoid redundant compute of index in AVX512 FP32 acc GEMM micro-kernel",
        "`constexpr int idx` doesn't seem to help here since `idx` is equal to `i`:\r\n\r\n```cpp\r\n        constexpr int row = i / COLS;\r\n        constexpr int col = i % COLS;\r\n\r\n       // some other code\r\n\r\n        constexpr int idx = row * COLS + col;\r\n        vc[idx] = at::vec::fmadd(va, vb[col], vc[idx]);\r\n```\r\n\r\nTODO\r\n- [ ] Although it's known at the time of compilation as to what various values of `i` would be due to forced-unrolling of `compute` lambda calls, check if the compiler really computes values of `row`, `col` and `idx` corresponding to each value of `i` at compile-time.\r\n \r\n\r\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @desertfire @chauhang @aakhundov",
        "open",
        "2025-02-02T20:19:37Z",
        null,
        null,
        "open source, topic: not user facing, module: inductor, ciflow/inductor",
        "main",
        "sanchitintel/modify_fp32_micro_gemm",
        1,
        1,
        2,
        2,
        1,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146280"
    ],
    [
        146275,
        "Correctly handle duplicated arguments when merging input views.",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146275\n\nFix: #135099\n\nThis PR changes how we map the original inputs into the new set of\ninputs that take in the tensor input's base instead of their aliases.\n\n**Problem:** in order to create this mapping, we had a dictionary that\nmapped the hashed arguments into their respective indices. However, if\nthere's a group of equal arguments, we will have only one mapping for\nsuch an argument. This breaks the assumption that there will be one\nmapping for each argument.\n\n**Solution:** map the hashed arguments into a list of indices. Then, we\nwill be able to correctly reconstruct the parameters for the new calling\nconvention.",
        "open",
        "2025-02-02T17:20:19Z",
        null,
        null,
        "open source, topic: not user facing, ciflow/inductor",
        "gh/ysiraichi/82/base",
        "gh/ysiraichi/82/head",
        2,
        28,
        4,
        2,
        2,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146275"
    ],
    [
        146273,
        "[dcp] Minor improvements to filesystem writer",
        "- Apply same check to `_SerialCpuLoader ` from `_OverlappingCpuLoader`  for determining when to clone non-contiguous cpu tensors\r\n- Add minor helper function to avoid iterating over `WriteItem`s twice to collect bytes and tensor write items\r\n- Use the metadata filename constant instead of harcoding \r\n\r\ncc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o @LucasLLC @MeetVadakkanchery @mhorowitz @pradeepfn @ekr0",
        "open",
        "2025-02-02T07:47:24Z",
        null,
        null,
        "oncall: distributed, triaged, open source, topic: not user facing, module: distributed_checkpoint",
        "main",
        "dist-ckpt-clone-patch",
        1,
        16,
        6,
        4,
        2,
        0,
        "Skylion007, ananthsub, ananthsub",
        "COMMENTED, COMMENTED, COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146273"
    ],
    [
        146267,
        "Format tests by PYFMT",
        "Fixes #ISSUE_NUMBER\r\n\n\ncc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o @LucasLLC @MeetVadakkanchery @mhorowitz @pradeepfn @ekr0",
        "open",
        "2025-02-02T04:02:51Z",
        null,
        null,
        "oncall: distributed, triaged, open source, topic: not user facing, ciflow/inductor, module: distributed_checkpoint",
        "main",
        "ruff_import",
        8,
        104,
        78,
        2,
        1,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146267"
    ],
    [
        146265,
        "Add libtorch CUDA 12.8 ",
        "Trying removing sm50 and sm60 to get around the ld --relink error\r\nTest will fail but testing the build.\r\n\r\nhttps://github.com/pytorch/pytorch/issues/145570\r\n\n\ncc @ptrblck @msaroufim @eqy",
        "open",
        "2025-02-02T02:42:46Z",
        null,
        null,
        "module: cuda, triaged, open source, ciflow/binaries, topic: not user facing",
        "main",
        "cu128-libtorch-build",
        3,
        66,
        4,
        1,
        4,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146265"
    ],
    [
        146264,
        "[ROCm] opportunistic fastatomics for ReduceAdd operations for MI300 GPUs",
        "In this approach, we are catching any lane within a wave that is doing fastatomics to the same destination address and computing the sum on the CU. This is leading to 3x improvement in scatter_add performance and 2x improvement in index_select.\r\n\r\nco-authored by: @amd-hhashemi\r\n\r\nFixes #ISSUE_NUMBER\r\n\r\n\r\ncc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd",
        "open",
        "2025-02-02T00:23:33Z",
        null,
        null,
        "module: rocm, open source, topic: not user facing, ciflow/periodic, ciflow/unstable, ciflow/rocm",
        "main",
        "pg-scatter-add-dup-fix",
        5,
        63,
        1,
        6,
        3,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146264"
    ],
    [
        146248,
        "Make fx.node.map_arg() and .map_aggregate() generic",
        "## What's the problem?\r\n\r\nThe popular `fx.node.map_arg()` and `fx.node.map_aggregate()` apply operations recursively on `dict`s, `tuples`, `list`s, etc, and return a new collection of the same type.\r\n\r\nUnfortunately, their base input type is `Argument`, which is [very unspecific indeed](https://github.com/pytorch/pytorch/blob/5d55a6585d5806c2743e92118e663f5abb261895/torch/fx/node.py#L48-L58): most type information is just thrown away at the call site of either of these functions, as far as the type checker goes.\r\n\r\nAs `torch` moves to a more typed code base, this would force innocent, unsuspecting developers to add logically unnecessary casts or `# type: ignore` statements.\r\n\r\n## What's the solution?\r\n\r\nMaking these two `node.map_*` functions generic on the first argument and return type means that type information is preserved for the type checker. (The signature of the other parameter, the function that visits the nodes and subnodes, has not changed, nor should it.)\r\n\r\n## Won't it break everything?\r\n\r\nIt doesn't break the type checker - one place needed an extra hint.\r\n\r\nThere have been code breakages, resolved one, at least one new one... we'll see!\r\n\r\nStack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #146626\n* __->__ #146248\n\r\n\r\n\r\ncc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o @ezyang @malfet @xuzhao9 @gramster @SherlockNoMad @EikanWang @jgong5 @wenzhe-nrv @voznesenskym @penguinwu @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-02-01T18:09:17Z",
        null,
        null,
        "oncall: distributed, module: typing, open source, better-engineering, ciflow/trunk, release notes: fx, topic: not user facing, fx, module: dynamo, ciflow/inductor, suppress-api-compatibility-check, merging, suppress-bc-linter",
        "gh/rec/129/base",
        "gh/rec/129/head",
        4,
        53,
        56,
        14,
        7,
        0,
        "Skylion007, Skylion007, rec, Skylion007, XuehaiPan, XuehaiPan, Skylion007, rec, Skylion007, rec, rec, Skylion007",
        "COMMENTED, COMMENTED, COMMENTED, COMMENTED, CHANGES_REQUESTED, APPROVED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146248"
    ],
    [
        146247,
        "torch/nn/modules/conv.py: docs: improvements",
        "Fix highlighting in generated documentation (`torch/nn/modules/conv.py`):\r\n\r\n* attrs should be `:attrs:`,\r\n* constants should be constants,\r\n* text in math should be '\\text{}`.\r\n\r\nReborn of #136218.\r\n\r\n/cc @albanD, @jbschlosser, @mikaylagawarecki",
        "open",
        "2025-02-01T18:06:45Z",
        null,
        null,
        "triaged, open source, topic: not user facing",
        "main",
        "patch-2",
        1,
        106,
        106,
        1,
        2,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146247"
    ],
    [
        146244,
        "Simplify CUDA version checking on tests",
        "Since we require CUDA >=11.0\r\n\r\n\r\ncc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o",
        "open",
        "2025-02-01T14:37:48Z",
        null,
        null,
        "oncall: distributed, triaged, open source, release notes: distributed (c10d)",
        "main",
        "windows_tests2",
        5,
        7,
        17,
        1,
        1,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146244"
    ],
    [
        146237,
        "[2/N] Fix cppcoreguidelines-init-variables suppression",
        "Fixes #ISSUE_NUMBER\r\n",
        "open",
        "2025-02-01T09:07:08Z",
        null,
        null,
        "triaged, open source, topic: not user facing",
        "main",
        "fix_init_variable",
        9,
        10,
        19,
        2,
        2,
        0,
        "Skylion007, cyyever, cyyever",
        "COMMENTED, COMMENTED, COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146237"
    ],
    [
        146234,
        "[1/N] Fix F401 errors in tests",
        "Fixes #ISSUE_NUMBER\r\n",
        "open",
        "2025-02-01T05:05:54Z",
        null,
        null,
        "triaged, open source, topic: not user facing",
        "main",
        "win_tests",
        5,
        29,
        48,
        2,
        2,
        0,
        "Skylion007, cyyever, soulitzer, cyyever, albanD, cyyever",
        "COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146234"
    ],
    [
        146228,
        "[PT2][Inductor][reland] Add runtime numeric check for the post grad pass",
        "Summary: We observed compilation time regression with previous diff implementation D63438718. Here we fix the issue and reland the diff\n\nTest Plan:\n### numeric check enablement test\n\n```\nbuck2 run mode/opt //scripts/jackiexu0313/pt2:local_model_with_pt2 -- --test_mode batch  --use_synthetic_data --flow_id 685229965 -n\n```\n\n\n### compilation time check\n\n```\nbuck2 run mode/opt //caffe2/benchmarks/dynamo/fb:torchbench_run_nanogpt_training -- -m nanogpt -t training\n```\n\n```\ntorchbench_run\n    duration_ms: 219528\n    defaults-batch_size: 1\n    defaults-speedup-x1000: 1408\n    defaults-abs_latency-x1000: 29068\n    defaults-compilation_latency-x1000: 93996\n    defaults-compression_ratio-x1000: 924\n    defaults-eager_peak_mem-x1000: 2473\n    defaults-dynamo_peak_mem-x1000: 2675\n    defaults-calls_captured: 1156\n    defaults-unique_graphs: 3\n    defaults-graph_breaks: 8\n    defaults-unique_graph_breaks: 6\n    defaults-autograd_captures: 0\n    defaults-autograd_compiles: 0\n    defaults-cudagraph_skips: 0\n    cudagraphs-batch_size: 1\n    cudagraphs-speedup-x1000: 5065\n    cudagraphs-abs_latency-x1000: 7983\n    cudagraphs-compilation_latency-x1000: 76961\n    cudagraphs-compression_ratio-x1000: 1485\n    cudagraphs-eager_peak_mem-x1000: 4473\n    cudagraphs-dynamo_peak_mem-x1000: 3012\n    cudagraphs-calls_captured: 1154\n    cudagraphs-unique_graphs: 2\n    cudagraphs-graph_breaks: 4\n    cudagraphs-unique_graph_breaks: 4\n    cudagraphs-autograd_captures: 0\n    cudagraphs-autograd_compiles: 0\n    cudagraphs-cudagraph_skips: 0\n    cudagraphs_dynamic-batch_size: 1\n    cudagraphs_dynamic-speedup-x1000: 5038\n    cudagraphs_dynamic-abs_latency-x1000: 8334\n    cudagraphs_dynamic-compilation_latency-x1000: 22521\n    cudagraphs_dynamic-compression_ratio-x1000: 893\n    cudagraphs_dynamic-eager_peak_mem-x1000: 4017\n    cudagraphs_dynamic-dynamo_peak_mem-x1000: 4493\n    cudagraphs_dynamic-calls_captured: 1154\n    cudagraphs_dynamic-unique_graphs: 2\n    cudagraphs_dynamic-graph_breaks: 4\n    cudagraphs_dynamic-unique_graph_breaks: 4\n    cudagraphs_dynamic-autograd_captures: 0\n    cudagraphs_dynamic-autograd_compiles: 0\n    cudagraphs_dynamic-cudagraph_skips: 0\n```\n\n\n```\nservicelab create benchmark_torchbench_run_nanogpt_training -d D68979204\n```\n\nSuccessfully submitted experiment: https://www.internalfb.com/servicelab/experiment/4800587892/\n\nDifferential Revision: D68979204\n\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @desertfire @chauhang @aakhundov",
        "open",
        "2025-02-01T01:13:15Z",
        null,
        null,
        "fb-exported, module: inductor, ciflow/inductor, release notes: inductor, ci-no-td",
        "main",
        "export-D68979204",
        5,
        92,
        25,
        1,
        5,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146228"
    ],
    [
        146227,
        "[ROCm][TunableOp] Add bias data type to TunableOp signature.",
        "Fixes #ISSUE_NUMBER\r\n\n\ncc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang",
        "open",
        "2025-02-01T01:09:32Z",
        null,
        null,
        "module: rocm, open source, topic: not user facing, ciflow/rocm",
        "main",
        "tunableop_fp8_bias",
        1,
        1,
        1,
        1,
        1,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146227"
    ],
    [
        146224,
        "[ONNX] Migrate torchlib into PyTorch",
        "Migrate torchlib and fix relevant implementation.\r\n\r\n1. Use the current ONNX IR based graph builder to run unit tests. Removed the eager evaluator mode tests because they are not relevant to pytorch\r\n2. Updated the builder to handle `Split` nodes correctly by supporting the num_outputs argument\r\n3. Simplified the torchlib registry to directly produce a list of OnnxDecompMeta, which can be consumed directly by the dispatcher\r\n4. Remove handling of traceable functions because all traceable functions are now trace only\r\n5. `torchvision` and `quantized_decomposed` ops are not included.\r\n\r\nFixes https://github.com/pytorch/pytorch/issues/139301\r\n\r\n## Next steps\r\nThe follow up PRs will decouple the implementation from ONNX Script type system\r\n",
        "open",
        "2025-02-01T00:54:16Z",
        null,
        null,
        "module: onnx, triaged, open source, ciflow/trunk, release notes: onnx, topic: new features, merging",
        "main",
        "justinchu/ghstack/torchlib",
        30,
        20652,
        435,
        30,
        6,
        0,
        "justinchuby, justinchuby, justinchuby, justinchuby, justinchuby, justinchuby, justinchuby, xadupre, xadupre, xadupre, xadupre, xadupre, xadupre, justinchuby, justinchuby, justinchuby, justinchuby, justinchuby, xadupre, justinchuby, justinchuby, justinchuby, justinchuby, justinchuby, justinchuby, titaiwangms, justinchuby, justinchuby, justinchuby, justinchuby",
        "COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146224"
    ],
    [
        146223,
        "What will happen?",
        null,
        "open",
        "2025-02-01T00:25:10Z",
        null,
        null,
        "topic: not user facing",
        "main",
        "malfet-patch-10",
        1,
        4,
        14,
        2,
        1,
        0,
        "malfet, Skylion007",
        "COMMENTED, COMMENTED",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146223"
    ],
    [
        146222,
        "[while_loop][inductor] support sym expression as cond_fn output",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146222\n\r\n\r\nAs titled. Previously, we only support tensor output of cond_fn, this PR changes to also allow a shape expr to be returned in cond_fn.\r\n\r\naoti generated output code looks like:\r\n```\r\nV0203 11:28:05.750000 2611693 torch/_inductor/compile_fx.py:1091] [1/0] [__output_code]     bool buf7_cond_result;\r\n....\r\n(while_loop_cond_graph_0_arg2_1_handle);\r\nV0203 11:27:59.336000 2611693 torch/_inductor/compile_fx.py:1091] [1/0] [__output_code]         buf7_cond_result = u0 + u1 < 10L;\r\nV0203 11:27:59.336000 2611693 torch/_inductor/compile_fx.py:1091] [1/0] [__output_code]         if (!buf7_cond_result) break;\r\n```\r\n\r\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @desertfire @chauhang @aakhundov",
        "open",
        "2025-02-01T00:23:46Z",
        null,
        null,
        "Merged, Reverted, ciflow/trunk, topic: not user facing, module: inductor, ciflow/inductor, ci-no-td",
        "gh/ydwu4/205/base",
        "gh/ydwu4/205/head",
        6,
        81,
        13,
        2,
        9,
        0,
        "desertfire",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146222"
    ],
    [
        146218,
        "[FSDP2][DEBUG] enforcing ReduceOp.SUM to avoid bug in ReduceOp.AVG",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\r\n* __->__ #146218\r\n\r\nworkaround for https://github.com/pytorch/pytorch/issues/144045 , but not sure if we should land\r\n\r\n\r\ncc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o",
        "open",
        "2025-01-31T23:34:19Z",
        null,
        null,
        "oncall: distributed, release notes: distributed (fsdp), ciflow/inductor",
        "gh/weifengpy/21/base",
        "gh/weifengpy/21/head",
        1,
        4,
        4,
        1,
        3,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146218"
    ],
    [
        146215,
        "Update Dependencies.cmake",
        "fix cmake if check error:\r\n\u201cUnknown arguments specified\u201d\r\n\r\nFixes #ISSUE_NUMBER\r\n",
        "open",
        "2025-01-31T23:14:29Z",
        null,
        null,
        "triaged, open source",
        "main",
        "patch-1",
        1,
        1,
        1,
        1,
        3,
        0,
        "Skylion007, longlene",
        "COMMENTED, COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146215"
    ],
    [
        146202,
        "[ROCm] follow up to #138964, remove work-around",
        "PR #138964 used #ifdef to skip non-contig tensor copies on ROCm due to failing tests.\r\n\n\ncc @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd",
        "open",
        "2025-01-31T21:42:15Z",
        null,
        null,
        "module: rocm, open source, release notes: cuda, ciflow/rocm",
        "main",
        "rocm_followup_128964",
        1,
        0,
        4,
        1,
        2,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146202"
    ],
    [
        146199,
        "docs: change log to ln in Softplus function and class",
        "Updated the math formula in the softplus function in torch.nn.functional.py and the Softplus class in torch.nn.modules.activation.py from log to ln for correctness and accuracy.\r\n\r\n",
        "open",
        "2025-01-31T20:47:48Z",
        null,
        null,
        "triaged, open source",
        "main",
        "docs",
        2,
        2,
        2,
        1,
        3,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146199"
    ],
    [
        146192,
        "torch.check distributions",
        "Fixes #ISSUE_NUMBER\r\n",
        "open",
        "2025-01-31T19:46:52Z",
        null,
        null,
        "",
        "main",
        "angelayi/distribution",
        1,
        6,
        3,
        1,
        2,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146192"
    ],
    [
        146191,
        "[cuda] Speed up layernorm backward by ~13% by using warp shuffles for the 16x32 kernel invocation",
        "Before this PR we had 2 kernels:\r\n1. For blocksize=32x32, this kernel *only* used warp shuffles to do the reduction\r\n2. For blocksize=16x32, this kernel *only* used shared memory to do the reduction\r\n\r\nThis PR replaces those two kernels with a single generic kernel with template parameters for the block size.\r\n\r\n1. Uses template parameters for blockDim.x and blockDim.y.\r\n1. Uses those template parameters to do a partial final reduction in shared memory if needed (i.e. if blockDim.y > 32).\r\n1. Then, for the final 32 rows, it uses warp shuffles when we need to reduce 32 rows down to a single row.\r\n1. Uses slightly more shared memory to reduce bank conflicts when reading the transposed data in both cases\r\n\r\nWhen compared to the baseline 16x32 kernel, ncu shows lower latency:\r\n\r\n![image](https://github.com/user-attachments/assets/df4fe13a-31b8-42ef-bc5d-348b39ec21e5)\r\n\r\nncu shows much lower shared memory loads and stores:\r\n\r\n![image](https://github.com/user-attachments/assets/c233c712-e2b2-4038-a5e9-9acc45c6e5b9)\r\n\r\nncu shows lower cycle count:\r\n\r\n![image](https://github.com/user-attachments/assets/83e7e236-66f1-4d8f-a1e4-11368fb69d09)\r\n\r\nncu shows lower sync instructions:\r\n\r\n![image](https://github.com/user-attachments/assets/f5caba87-417b-43a0-a304-0abaecb093dc)\r\n\r\nFor the 32x32 kernel, nvcc in theory should optimize away the shared memory reduction loop completely and performance should be identical to the previous specialized kernel.",
        "open",
        "2025-01-31T19:45:47Z",
        null,
        null,
        "release notes: cuda",
        "main",
        "ln1",
        3,
        62,
        129,
        8,
        1,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146191"
    ],
    [
        146189,
        "[test]",
        "Fixes #ISSUE_NUMBER\r\n",
        "open",
        "2025-01-31T19:04:04Z",
        null,
        null,
        "release notes: releng",
        "main",
        "csl/build_test_more_procs",
        7,
        276,
        362,
        30,
        1,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146189"
    ],
    [
        146182,
        "[export] Allow bypassing version check with unsafe API.",
        "Summary:\nas title.\nhttps://fb.workplace.com/groups/1028545332188949/permalink/10024343514259357/\n\nTest Plan:\n```\nwith torch.export._unsafe_skip_version_check():\n    ep = torch.export.load(...)\n```\nCI\n\nDifferential Revision: D68791202\n\n\n",
        "open",
        "2025-01-31T18:22:22Z",
        null,
        null,
        "fb-exported, ciflow/trunk, release notes: export",
        "main",
        "export-D68791202",
        1,
        23,
        6,
        1,
        2,
        0,
        "angelayi",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146182"
    ],
    [
        146180,
        "[AOTI] Improve readability of package_cpp_only",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146180\n\nSummary: Made two improvements here: 1) Emit interface.cpp into a separate file instead of embedding it to the model code; 2) Add prefix to mark the generated files as model code or weights(constants).\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov",
        "open",
        "2025-01-31T17:53:29Z",
        null,
        null,
        "topic: improvements, module: inductor, ciflow/inductor, release notes: inductor",
        "gh/desertfire/535/base",
        "gh/desertfire/535/head",
        4,
        28,
        23,
        1,
        1,
        0,
        "angelayi",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146180"
    ],
    [
        146176,
        "[executorch hash update] update the pinned executorch hash",
        "Based on latest green in HUD https://hud.pytorch.org/hud/pytorch/executorch/main/1?per_page=50\n",
        "open",
        "2025-01-31T17:35:35Z",
        null,
        null,
        "ciflow/trunk, topic: not user facing, ciflow/inductor",
        "main",
        "et_pin_bump",
        2,
        2,
        2,
        1,
        6,
        0,
        "huydhn",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146176"
    ],
    [
        146173,
        "[CI] Get rid of UCC builds",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146173\n\nThere hasn't been any active development/testing of those in last 2 years",
        "open",
        "2025-01-31T17:09:55Z",
        null,
        null,
        "topic: not user facing",
        "gh/malfet/159/base",
        "gh/malfet/159/head",
        5,
        0,
        99,
        1,
        4,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146173"
    ],
    [
        146172,
        "Factory function support for NestedTensor",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146172\n* #146101\n* #145922\n* #141842\n* #141841\n* #146052\n\r\nRebase of https://github.com/pytorch/pytorch/pull/117904 removing unnecessary bits now that python nested int already holds the necessary metadata.\r\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-01-31T17:03:29Z",
        null,
        null,
        "release notes: nested tensor, module: dynamo, ciflow/inductor",
        "gh/soulitzer/350/base",
        "gh/soulitzer/350/head",
        16,
        229,
        7,
        6,
        1,
        0,
        "ezyang, Skylion007",
        "APPROVED, COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146172"
    ],
    [
        146171,
        "Noob attempt at tensor_pointer_to_tensor_handle accepting const",
        "Fairly certain this will fail lint but is there a reason creating an AtenTensorHandle is not const preserving?\r\n\r\nStack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\r\n* __->__ #146171\r\n\r\n",
        "open",
        "2025-01-31T16:35:50Z",
        null,
        null,
        "ciflow/inductor, release notes: inductor",
        "gh/janeyx99/221/base",
        "gh/janeyx99/221/head",
        1,
        5,
        0,
        1,
        1,
        0,
        "desertfire",
        "APPROVED",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146171"
    ],
    [
        146145,
        "[CUDAEvent.h] support cuda events in cudagraphs",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146145\n\n",
        "open",
        "2025-01-31T09:36:23Z",
        null,
        null,
        "release notes: cuda",
        "gh/nmacchioni/39/base",
        "gh/nmacchioni/39/head",
        5,
        104,
        5,
        9,
        7,
        0,
        "ngimel, nmacchioni, galv",
        "COMMENTED, COMMENTED, COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146145"
    ],
    [
        146143,
        "Fix C++20 build errors",
        "Without breaking C++17.\r\n\r\n\r\ncc @EikanWang @jgong5 @wenzhe-nrv @sanchitintel",
        "open",
        "2025-01-31T08:21:24Z",
        null,
        null,
        "oncall: jit, triaged, open source, NNC, release notes: jit",
        "main",
        "cxx20_error",
        5,
        30,
        2,
        4,
        3,
        1,
        "albanD, cyyever, albanD, swolchok, cyyever, cyyever, malfet, malfet, malfet, cyyever",
        "COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, CHANGES_REQUESTED, COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146143"
    ],
    [
        146142,
        "Fix condition number invertible input(s) documented results",
        "`torch.linalg.cond` documentation states a singular input raises a RuntimeError, though unit tests show it in fact returns `inf` (https://github.com/pytorch/pytorch/blob/main/test/test_linalg.py#L1576).\r\n\r\n Fixes the documentation and adds an example.\r\n\r\nIt appears earlier documentation reflected this behavior (https://github.com/pytorch/pytorch/pull/45832/files/9008c10d63e7f5ddd0f06bbd5c7f1548c945d917#diff-316ce439a56491298e2d98deeca82606c52e5bde2f1ceb16c534ec03386c817eR358) \r\n\r\nand then got updated here: https://github.com/pytorch/pytorch/commit/d578e8cfa2db71e45c3565b42ff2b10d13643402.\r\n",
        "open",
        "2025-01-31T07:29:14Z",
        null,
        null,
        "triaged, open source, release notes: linalg_frontend",
        "main",
        "redwrasse/linalg-cond-non-invertible-err",
        1,
        1,
        2,
        2,
        8,
        2,
        "lezcano, redwrasse, lezcano",
        "CHANGES_REQUESTED, COMMENTED, COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146142"
    ],
    [
        146135,
        "WIP: async compile",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146135\n* #146134\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @desertfire @chauhang @aakhundov",
        "open",
        "2025-01-31T05:36:47Z",
        null,
        null,
        "module: inductor, ciflow/inductor",
        "gh/aorenste/214/base",
        "gh/aorenste/214/head",
        2,
        340,
        0,
        5,
        2,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146135"
    ],
    [
        146134,
        "Subprocess compile",
        "Add a mode to `fx_codegen_and_compile()` to compile in a separate process. This is to prepare for async compile where we'll compile and run eager in parallel (and also be able to move the compile phase to a remote computer).\r\n\r\nAdded a test based which runs the test_torchinductor tests with subprocess compiling turned on.\r\n\r\nStack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\r\n* __->__ #146134\r\n\r\n\r\n\r\ncc @ezyang @SherlockNoMad @EikanWang @jgong5 @wenzhe-nrv @voznesenskym @penguinwu @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @desertfire @chauhang @aakhundov",
        "open",
        "2025-01-31T05:36:42Z",
        null,
        null,
        "release notes: fx, fx, module: inductor, ciflow/inductor",
        "gh/aorenste/213/base",
        "gh/aorenste/213/head",
        7,
        615,
        58,
        6,
        1,
        1,
        "jamesjwu, jamesjwu",
        "COMMENTED, APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146134"
    ],
    [
        146133,
        "Apply ruff fixes to torch/**/*py",
        "Fixes #ISSUE_NUMBER\r\n\n\ncc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o @EikanWang @jgong5 @wenzhe-nrv @sanchitintel @ezyang @SherlockNoMad @voznesenskym @penguinwu @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @desertfire @chauhang @aakhundov",
        "open",
        "2025-01-31T05:10:03Z",
        null,
        null,
        "oncall: distributed, oncall: jit, triaged, open source, release notes: quantization, fx, module: inductor, module: dynamo, ciflow/inductor, release notes: export",
        "main",
        "ruff_fix",
        30,
        92,
        123,
        1,
        1,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146133"
    ],
    [
        146113,
        "Fix logging and test files which misspell \"precision\"",
        "Noticed this while working on something, decided to submit a quick fix.",
        "open",
        "2025-01-31T00:51:57Z",
        null,
        null,
        "ciflow/trunk, release notes: linalg_frontend, merging",
        "main",
        "spell",
        2,
        2,
        2,
        1,
        7,
        0,
        "drisspg",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146113"
    ],
    [
        146110,
        "[scan] Corrections for scan",
        "This PR resolves some minor issues with the scan HOP and unifies the handling of the additional_inputs in the same way as for associative_scan.\r\n\r\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @ydwu4 ",
        "open",
        "2025-01-31T00:26:28Z",
        null,
        null,
        "triaged, open source, topic: not user facing, module: dynamo",
        "main",
        "scan_hop_fixes",
        3,
        25,
        39,
        3,
        4,
        0,
        "ydwu4, bohnstingl, bohnstingl, Skylion007",
        "COMMENTED, COMMENTED, COMMENTED, COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146110"
    ],
    [
        146109,
        "cpp_wrapper: fix inductor triton tests",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #146452\n* #146706\n* #146424\n* __->__ #146109\n* #146449\n* #144349\n* #144293\n* #144002\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @desertfire @chauhang @aakhundov",
        "open",
        "2025-01-31T00:12:22Z",
        null,
        null,
        "open source, topic: not user facing, module: inductor, ciflow/inductor",
        "gh/benjaminglass1/63/base",
        "gh/benjaminglass1/63/head",
        5,
        75,
        44,
        7,
        1,
        0,
        "benjaminglass1",
        "COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146109"
    ],
    [
        146104,
        "[DO NOT MERGE] Testing C2 MI300 cluster.",
        "This PR is to test the stability of the C2 MI300x cluster.\n\ncc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd",
        "open",
        "2025-01-30T23:49:22Z",
        null,
        null,
        "module: rocm, open source, topic: not user facing, ciflow/unstable",
        "main",
        "test-c2",
        3,
        11,
        8,
        1,
        1,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146104"
    ],
    [
        146101,
        "(WIP) Update NJT ops to check data for raggedness check",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #146172\n* __->__ #146101\n* #145922\n* #141842\n* #141841\n* #146052\n\r\n\r\nNext:\r\n   - make sure guards are okay\r\n\r\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-01-30T23:22:38Z",
        null,
        null,
        "release notes: nested tensor, module: dynamo, ciflow/inductor",
        "gh/soulitzer/349/base",
        "gh/soulitzer/349/head",
        11,
        430,
        74,
        7,
        2,
        0,
        "albanD, soulitzer, soulitzer, soulitzer, soulitzer",
        "COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146101"
    ],
    [
        146098,
        "Move get accelerator to use build time flags when possible",
        "This PR does two main things (they are in a single PR to show how the newly added APIs are used).\r\n\r\n- Add isBuilt and isAvailable APIs to the AcceleratorHook interface. See inline doc for their exact semantic\r\n- Use the newly added isBuilt for accelerator check to ensure it does not poison fork\r\n\r\n\r\ncc @egienvalue we should do an MTIA patch for this and move to compile-time check once we figure out the CUDA+MTIA binary situation\r\ncc @guangyey we would need to add these APIs to the HPU backend (which I don't have access to) and we can move it to be compile time as well to avoid initialization.",
        "open",
        "2025-01-30T23:13:45Z",
        null,
        null,
        "Merged, Reverted, ciflow/trunk, release notes: python_frontend, topic: bug fixes, ciflow/mps, ciflow/xpu, ci-no-td",
        "main",
        "acc_clean",
        7,
        78,
        28,
        11,
        12,
        0,
        "ngimel, janeyx99, janeyx99, janeyx99, janeyx99, malfet, egienvalue, albanD, EikanWang",
        "APPROVED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, APPROVED, COMMENTED, COMMENTED, APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146098"
    ],
    [
        146093,
        "TEST3",
        "Fixes #ISSUE_NUMBER\r\n\n\ncc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o",
        "open",
        "2025-01-30T22:36:55Z",
        null,
        null,
        "oncall: distributed, release notes: releng, ciflow/inductor",
        "main",
        "zainr/mypy-break-test3",
        8,
        23,
        18,
        4,
        1,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146093"
    ],
    [
        146092,
        "TEST2",
        "Fixes #ISSUE_NUMBER\r\n\n\ncc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o",
        "open",
        "2025-01-30T22:36:11Z",
        null,
        null,
        "oncall: distributed, release notes: releng, ciflow/inductor",
        "main",
        "zainr/mypy-break-test2",
        8,
        25,
        18,
        2,
        1,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146092"
    ],
    [
        146091,
        "[WIP] TEST 1",
        "Fixes #ISSUE_NUMBER\r\n\n\ncc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o",
        "open",
        "2025-01-30T22:35:18Z",
        null,
        null,
        "oncall: distributed, release notes: releng, ciflow/inductor",
        "main",
        "zainr/mypy-break-test",
        8,
        26,
        18,
        1,
        1,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146091"
    ],
    [
        146090,
        "Fix TestDataLoader.test_segfault unexpected success on Aarch64",
        "TestDataLoader.test_segfault gives unexpected success on linux Aarch64",
        "open",
        "2025-01-30T22:35:15Z",
        null,
        null,
        "triaged, open source, release notes: dataloader",
        "main",
        "dataloader",
        3,
        18,
        8,
        2,
        2,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146090"
    ],
    [
        146073,
        "Nccl update to 2.25.1 for cuda 11.8-12.6",
        "Should resolve: https://github.com/pytorch/pytorch/issues/144768\r\nWe build one common nccl version for all cuda builds: ``NCCL_VERSION=v2.25.1-1``\r\nFor CUDA 11.8 we package this wheel instead of installing it from pypi.\r\n\r\n",
        "open",
        "2025-01-30T21:09:07Z",
        null,
        null,
        "topic: not user facing, ciflow/binaries_wheel",
        "main",
        "nccl_module_test",
        15,
        174,
        177,
        14,
        4,
        1,
        "Skylion007, atalman, atalman, malfet",
        "APPROVED, COMMENTED, COMMENTED, APPROVED",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146073"
    ],
    [
        146069,
        "Set /NODEFAULTLIB:vcomp for MSVC when linking caffe2::mkl with libiomp5md.lib ",
        "Fixes:\r\n- https://github.com/pytorch/pytorch/issues/113490\r\n\r\nThe PR sets `/NODEFAULTLIB:vcomp` link flag when linking caffe2::mkl with libiomp5md.lib.\r\n\r\nThe changes have been verified by checking build output with `VERBOSE=1`, for example:\r\n```\r\nC:\\PROGRA~1\\MICROS~1\\2022\\COMMUN~1\\VC\\Tools\\MSVC\\1442~1.344\\bin\\Hostx64\\x64\\link.exe /nologo caffe2\\CMakeFiles\\torch_global_deps.dir\\__\\torch\\csrc\\empty.c.obj /out:bin\\torch_global_deps.dll /implib:lib\\torch_global_deps.lib /pdb:bin\\torch_global_deps.pdb /dll /version:0.0 /machine:x64 /ignore:4049 /ignore:4217 /ignore:4099 /debug /INCREMENTAL:NO /NODEFAULTLIB:vcomp -LIBPATH:\\lib -LIBPATH:\\lib\\intel64 -LIBPATH:\\lib\\intel64_win -LIBPATH:\\lib\\win-x64 C:\\lib\\mkl_intel_lp64.lib C:\\lib\\mkl_intel_thread.lib C:\\lib\\mkl_core.lib C:\\lib\\libiomp5md.lib kernel32.lib user32.lib gdi32.lib winspool.lib shell32.lib ole32.lib oleaut32.lib uuid.lib comdlg32.lib advapi32.lib /MANIFEST:EMBED,ID=2\r\n```\r\n\r\n\r\ncc @malfet @seemethere @peterjc123 @mszhanyi @skyline75489 @nbcsm @iremyux @Blackhex",
        "open",
        "2025-01-30T20:33:33Z",
        null,
        null,
        "module: build, module: windows, triaged, open source, ciflow/trunk, release notes: build, topic: bug fixes",
        "main",
        "pytorch-113490",
        1,
        3,
        0,
        1,
        4,
        1,
        "malfet",
        "CHANGES_REQUESTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146069"
    ],
    [
        146068,
        "Error handling for launcher method in CachingAutotuner",
        "Fixes #146018\r\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang @aakhundov",
        "open",
        "2025-01-30T20:25:52Z",
        null,
        null,
        "triaged, open source, function request, topic: not user facing, module: inductor",
        "main",
        "error_handling_caching_autotuner",
        2,
        178,
        1,
        2,
        10,
        1,
        "eellison",
        "CHANGES_REQUESTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146068"
    ],
    [
        146064,
        "[PT2] Support add/remove passes in pre_grad",
        "Summary:\nsupport the same functionality with acc_tracer disabled, add a new config for pre_grad add/remove_passes, at the front end it still uses the same interface\n\nsome minor updates in pre_grad passes to make sure the passes are run in desired order, after added passes, still run pass like remove_noops at the end\n\nTest Plan: add new UT, please see stacked diff for add pass tests (TODO: update diff link)\n\nDifferential Revision: D68909278\n\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang @aakhundov",
        "open",
        "2025-01-30T19:17:34Z",
        null,
        null,
        "fb-exported, ciflow/trunk, topic: not user facing, module: inductor, ciflow/inductor",
        "main",
        "export-D68909278",
        3,
        77,
        94,
        1,
        3,
        0,
        "frank-wei",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146064"
    ],
    [
        146063,
        "[wip] torch._dynamo.disable on the CA graph",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146063\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @yf225",
        "open",
        "2025-01-30T19:14:34Z",
        null,
        null,
        "module: dynamo, ciflow/inductor, module: compiled autograd",
        "gh/xmfan/166/base",
        "gh/xmfan/166/head",
        2,
        122,
        0,
        1,
        2,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146063"
    ],
    [
        146061,
        "[inductor][triton] Fix average pool nd for int64 dtype",
        "The eager mode implementation of average pool nd returns an integer tensor if the input is also an integer tensor. This should also be preserved in inductor.\r\n\r\nFixes pytest -k test_comprehensive_nn_functional_avg_pool2d_cpu_int64 error: Triton compilation failed: triton_poi_fused_avg_pool2d_0\r\n\r\nSee WIP https://github.com/pytorch/pytorch/pull/145865#issuecomment-26200289890 to potentially enable such tests as they aren't enabled yet.\r\n\r\nFixes #ISSUE_NUMBER\r\n\r\n\r\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang @aakhundov",
        "open",
        "2025-01-30T18:54:43Z",
        null,
        null,
        "triaged, open source, topic: not user facing, module: inductor, ciflow/inductor",
        "main",
        "mwizak/fix-avg-pool-int64-dtype",
        3,
        29,
        5,
        1,
        3,
        1,
        "eellison",
        "COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146061"
    ],
    [
        146055,
        "[Win][CD] Install cmake and setuptools from PyPI",
        "And also avoid repeating the same command over and over\r\n",
        "open",
        "2025-01-30T18:14:34Z",
        null,
        null,
        "topic: not user facing, ciflow/binaries_wheel",
        "main",
        "malfet-patch-8",
        1,
        10,
        9,
        3,
        1,
        2,
        "Skylion007, atalman, seemethere",
        "APPROVED, APPROVED, APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146055"
    ],
    [
        146051,
        "inductor.config.descriptive_names = False is not actually supported (#145523) (#145523)",
        "Summary:\nThis config is not supported (it throws an error when set), and doesn't really make sense imo.\n\nApproved by: https://github.com/eellison\n\nTest Plan: contbuild & OSS CI, see https://hud.pytorch.org/commit/pytorch/pytorch/edf266e9bbbf6063f7c4a336ffb50234e11a0a82\n\nDifferential Revision: D68846308\n\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang @aakhundov",
        "open",
        "2025-01-30T17:52:47Z",
        null,
        null,
        "fb-exported, ciflow/trunk, topic: docs, module: inductor, ciflow/inductor, release notes: inductor",
        "main",
        "export-D68846308",
        1,
        2,
        3,
        1,
        4,
        0,
        "Skylion007",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146051"
    ],
    [
        145999,
        "[CUDA] Optimize CUDA occupancy for indexing operators like index_select, index_add and index_reduce",
        "### Background\r\nFor indexing operations such as **torch.index_select**, **torch.index_add**, and **torch.index_reduce**, GPU performance is relatively low when handling large input sizes. On an A100 GPU, `torch.index_select` achieves only about 30% of the theoretical memory bandwidth for large inputs. Notably, `torch.index_select` and `torch.index_add` are among the most time-consuming operations in the forward and backward passes of `torch.nn.Embedding`, which is widely used in NLP and deep learning ranking models (DLRM).\r\n\r\nUpon analysis, I identified the following line of code`dim3 largeIndexGrid(std::min(ceil_div(sourceTotalSize, (uint64_t)128), (uint64_t)(mpc * 8)))`. Here, the blockSize is fixed at 128, and the number of blocks is set to `mpc * 8`, where mpc represents the number of streaming multiprocessors (SMs). This configuration results in only 1024 threads per SM (128 \u00d7 8). However, on an A100 GPU, each SM supports up to 2048 concurrent threads, meaning that the theoretical occupancy is limited to 50%.\r\n\r\nThis commit improves grid dimension calculation by leveraging maxThreadsPerMultiProcessor and blockSize, aligning with best practices commonly used in other GPU kernels in PyTorch.\r\n\r\n### Tests Performed\r\n1. Correctness testing: All tests in `test/test_torch.py` passed, including numerous tests covering `torch.index_select`, `torch.index_add` and `torch.index_reduce`. \r\n2. Performance testing: I run performance testing with the  following [script ](https://github.com/YyWangCS/FairySpeed/blob/main/embedding/bench_index_ops.py)on A100, the performance number is as follows. \r\n\r\n#### torch.index_select\r\n\r\n| num_embedding | embedding_dim | input_size | kernel latency before optimization (\u00b5s) | kernel latency after optimization (\u00b5s) |\r\n| ------------- | ------------- | ---------- | --------------------------------------- | -------------------------------------- |\r\n| 1000000       | 128           | 307200     | 518.3                                   | 359.4                                  |\r\n| 1000000       | 32            | 307200     | 141.4                                   | 97.3                                   |\r\n| 1000000       | 128           | 204800     | 347.5                                   | 242.4                                  |\r\n| 1000000       | 32            | 204800     | 96.2                                    | 66.2                                   |\r\n| 128000        | 4096          | 4096       | 219.2                                   | 158.6                                  |\r\n\r\n#### torch.index_add\r\n\r\n| num_embedding | embedding_dim | input_size | kernel latency before optimization (\u00b5s) | kernel latency after optimization (\u00b5s) |\r\n| ------------- | ------------- | ---------- | --------------------------------------- | -------------------------------------- |\r\n| 1000000       | 128           | 307200     | 526.8                                   | 379.4                                  |\r\n| 1000000       | 32            | 307200     | 143.4                                   | 103.3                                  |\r\n| 1000000       | 128           | 204800     | 352.2                                   | 256.1                                  |\r\n| 1000000       | 32            | 204800     | 98.9                                    | 69.9                                   |\r\n| 128000        | 4096          | 4096       | 222.8                                   | 165.0                                  |\r\n\r\n#### torch.index_reduce\r\n\r\n| num_embedding | embedding_dim | input_size | kernel latency before optimization (\u00b5s) | kernel latency after optimization (\u00b5s) |\r\n| ------------- | ------------- | ---------- | --------------------------------------- | -------------------------------------- |\r\n| 1000000       | 128           | 307200     | 732.5                                   | 470.7                                  |\r\n| 1000000       | 32            | 307200     | 197.5                                   | 126.6                                  |\r\n| 1000000       | 128           | 204800     | 490.8                                   | 316.4                                  |\r\n| 1000000       | 32            | 204800     | 133.7                                   | 86.1                                   |\r\n### Reference\r\n[Performance Optimization of Embedding Computation on GPU Part 1: GPU Occupancy Optimization](https://yywangcs.notion.site/Performance-Optimization-of-Embedding-Computation-on-GPU-Part-1-GPU-Occupancy-Optimization-178fc9f5d805800e91b6d4490afcc665)",
        "open",
        "2025-01-30T01:14:03Z",
        null,
        null,
        "triaged, open source, release notes: cuda",
        "main",
        "YyWangCS/opt_embedding",
        1,
        25,
        7,
        4,
        2,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145999"
    ],
    [
        145992,
        "fix indirect broadcast",
        "Fixes #142250\r\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang @aakhundov",
        "open",
        "2025-01-30T00:27:28Z",
        null,
        null,
        "topic: not user facing, module: inductor, ciflow/inductor",
        "main",
        "findhao/fix-indirect-access",
        2,
        37,
        1,
        2,
        1,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/145992"
    ],
    [
        145990,
        "[Profiler] Add Full PG ranks to Metadata",
        "Summary: We only add a shortened list of PG ranks if there is a job distributed across multiple nodes. Let's add the PG ranks to the JSON metadata\n\nTest Plan:\nhttps://www.internalfb.com/intern/perfdoctor/trace_view?filepath=tree/traces/dynocli/devvm2185.cco0.facebook.com/rank-0.Jan_29_16_19_52.2285734.pt.trace.json.gz&bucket=gpu_traces\n{F1974810517}\n\nDifferential Revision: D68867518\n\n\n",
        "open",
        "2025-01-30T00:25:11Z",
        null,
        null,
        "enhancement, fb-exported, release notes: profiler",
        "main",
        "export-D68867518",
        1,
        6,
        0,
        1,
        7,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145990"
    ],
    [
        145979,
        "[draft_export] better stack logging for strict mode",
        "Strict-mode draft export tends to log unhelpful stack traces for guards/data-dependent errors, relying on `CapturedTraceback.extract()`, which is only accurate for non-strict. For dynamo, it's better to use `TracingContext.extract_stack()` and fallback to the former when this is empty, avoiding traces pointing to the top-level export call, or lambdas (in the case of `torch._check` calls).\r\n\r\ne.g. before, for `test_draft_export.py -k test_offsets`:\r\n```\r\n    This occurred at the following stacktrace: \r\n        File /data/users/pianpwk/pytorch/test/export/test_draft_export.py, lineno 259, in test_offsets:\r\n        `ep, report = draft_export(M(), inp, strict=True)`\r\n```\r\nafter:\r\n```\r\n    This occurred at the following stacktrace: \r\n        File /data/users/pianpwk/pytorch/test/export/test_draft_export.py, lineno 254, in forward:\r\n            `if a == 0:`\r\n```\n\ncc @ezyang @SherlockNoMad @EikanWang @jgong5 @wenzhe-nrv",
        "open",
        "2025-01-29T23:04:26Z",
        null,
        null,
        "ciflow/trunk, fx, ciflow/inductor, release notes: export",
        "main",
        "pianpwk/draft_strict_stack",
        2,
        44,
        30,
        1,
        2,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145979"
    ],
    [
        145969,
        "Test of triton.compile in worker processes",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #145969\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang @aakhundov",
        "open",
        "2025-01-29T20:57:45Z",
        null,
        null,
        "module: inductor, ciflow/inductor",
        "gh/jamesjwu/100/base",
        "gh/jamesjwu/100/head",
        5,
        66,
        58,
        3,
        2,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/145969"
    ],
    [
        145966,
        "[BE] Upgrade to mypy 1.14",
        "Upgrade mypy version\r\n\r\ncc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o",
        "open",
        "2025-01-29T20:48:32Z",
        null,
        null,
        "oncall: distributed, release notes: releng, ciflow/inductor",
        "main",
        "zainr/mypy-update",
        11,
        45,
        31,
        7,
        2,
        0,
        "Skylion007, Skylion007, Skylion007, ZainRizvi, Skylion007, Skylion007, Skylion007, Skylion007",
        "COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, CHANGES_REQUESTED, COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145966"
    ],
    [
        145957,
        "Fix invalid nested int guarding in broadcast_shapes()",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #145957\n\r\nFixes #145874\r\n\r\nThis PR takes the approach of updating the logic determining whether multiple shapes broadcast together to handle nested ints specially.\r\n\r\nPossible alternative approach: don't update `broadcast_shapes()` + indicate that e.g. `Ne(j0, 1)` should statically evaluate to False. I briefly tried this but it wasn't straightforward. Is it better?",
        "open",
        "2025-01-29T19:40:37Z",
        null,
        null,
        "topic: not user facing",
        "gh/jbschlosser/227/base",
        "gh/jbschlosser/227/head",
        2,
        57,
        7,
        2,
        6,
        0,
        "bobrenjc93, jbschlosser, soulitzer",
        "COMMENTED, COMMENTED, COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145957"
    ],
    [
        145955,
        "Add MPS OpInfo db, rework test_mps to use OpInfo",
        "Infrastructure changes that will help enable: https://github.com/pytorch/pytorch/pull/142202",
        "open",
        "2025-01-29T19:29:09Z",
        null,
        null,
        "triaged, open source, release notes: mps, ciflow/mps, keep-going",
        "main",
        "dev/skotapati/mps_op_db",
        5,
        877,
        1037,
        17,
        1,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145955"
    ],
    [
        145946,
        "[ROCm][TunableOp] hipblaslt tf32 support",
        "TF32 is supported by hipblaslt. Support added by #143549.  This PR expands integration to the TunableOp feature.\n\ncc @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd",
        "open",
        "2025-01-29T18:06:41Z",
        null,
        null,
        "module: rocm, open source, ciflow/trunk, topic: not user facing, ciflow/rocm",
        "main",
        "rocm_tunableop_tf32",
        2,
        15,
        2,
        3,
        2,
        0,
        "pruthvistony",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145946"
    ],
    [
        145942,
        "Enable fast qlinear_dynamic path for AArch64 through ACL directly",
        "This enables a fast path for eager mode dynamic quantization for AArch64 through Arm Compute Library (ACL) directly.\r\n\r\nContext: PR #126687 enabled an optimized implementation for `qlinear_dynamic` for AArch64 through ideep \u2192 oneDNN \u2192 ACL which improved performance by ~10x compared to the previous implementation. \r\n\r\nHowever, the current `qlinear_dynamic` path (ideep \u2192 oneDNN \u2192 ACL) suffers from high overhead due to the API friction between the stateless oneDNN API and the stateful ACL low-precision GEMM (`lowp_gemm`) API - for example, ACL's `lowp_gemm` objects cache information like weights reduction or weights in optimized memory format which oneDNN does not allow due to its stateless nature. Hence, ACL currently runs a (redundant) sum of columns and pre-transposition (to the gemm kernel's optimal format) for each GEMM operation. \r\n\r\nThis PR addresses the sub-optimalities above by integrating ACL directly with `qlinear_dynamic`. This approach yields an average speedup (averaged over context_lengths of 2^3 up to 2^9) of ~ 50% for `bert-base-uncased`, `bert-large-uncased`, `roberta-base`, `distilbert-base-uncased` with 16 threads on a Neoverse-V1 (with `transformers==4.48`) - See benchmark code below. To achieve this, we:\r\n* Use ACL which is already built with PyTorch as a shared library when `USE_MKLDNN_ACL` is set.\r\n* Add ACL to ATen's CPU include and dependency libs\r\n* Introduce `PackedLinearWeightsACL` (as a subclasses of `PackedLinearWeightsOnednn`) with an implementation of `qlinear_dynamic` that uses ACL directly, while `qlinear` still follows the oneDNN path.\r\n* A future PR will introduce a direct ACL implementation `qlinear` and will allow us to remove the dependence on `PackedLinearWeightsOnednn`\r\n\r\nNote, that the ACL `lowp_gemm` API changed slightly between v24.09 and v24.12 which will be the new version after #138889 - Hence, this PR targets the new version - v24.12.\r\n\r\nThe following code was used to benchmark `qlinear_dynamic` performance:\r\n```\r\n# SPDX-FileCopyrightText: Copyright 2025 Arm Limited and/or its affiliate <open-source-office@arm.com>\r\n# SPDX-License-Identifier: BSD-3-Clause\r\nimport torch\r\nfrom transformers import AutoModel, AutoConfig\r\nimport time\r\nimport numpy as np\r\nfrom argparse import ArgumentParser\r\n\r\nclass ModelArgumentParser(ArgumentParser):\r\n    def __init__(self) -> None:\r\n        super().__init__(description=\"huggingface model\")\r\n        self.add_argument(\"--context_length\",\r\n                            help=\"context length - number of input tokens\",\r\n                            type=int,\r\n                            default=64\r\n        )\r\n        self.add_argument(\"--model\",\r\n                            help=\"model checkpoint - i.e. 'bert-base-uncased'\",\r\n                            type=str,\r\n                            default=None)\r\n        self.add_argument(\"--iters\",\r\n                          help=\"benchmark iterations\",\r\n                          default=500)\r\n\r\nif __name__ == \"__main__\":\r\n    parser = ModelArgumentParser()\r\n    args = parser.parse_args()\r\n    model_name = args.model\r\n    config = AutoConfig.from_pretrained(model_name)\r\n    batch_size = 1\r\n    model = AutoModel.from_pretrained(model_name)\r\n    model = torch.quantization.quantize_dynamic(model, {torch.nn.Linear}, dtype=torch.qint8)\r\n    model.eval()\r\n    inputs = torch.randint(config.vocab_size, (batch_size, args.context_length), dtype=torch.long, device=\"cpu\")\r\n    times = []\r\n    with torch.no_grad():\r\n        # warmup\r\n        for _ in range(10):\r\n            model(inputs)\r\n        # benchmark\r\n        for _ in range(args.iters):\r\n            s = time.time_ns()\r\n            model(inputs)\r\n            times.append((time.time_ns() - s) / 1e6)\r\n\r\n    print(\"Model = \", model_name)         \r\n    print(\"Context Length = \", args.context_length)\r\n    print(\"Min (ms) = \", min(times))\r\n    print(\"Mean (ms) = \", np.mean(times))  \r\n```\r\n\r\n\r\n\r\nFixes #ISSUE_NUMBER\r\n\r\n\r\ncc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @malfet @snadampal @milpuz01",
        "open",
        "2025-01-29T17:19:32Z",
        null,
        null,
        "module: cpu, triaged, open source, module: arm, release notes: quantization, release notes: releng, ciflow/linux-aarch64",
        "main",
        "acl_qlinear_dynamic",
        9,
        471,
        15,
        2,
        6,
        1,
        "malfet",
        "COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145942"
    ],
    [
        145936,
        "`torch.tensordot`: performance improvements when contracting to a scalar.",
        "As per title.\r\nFixes https://github.com/pytorch/pytorch/issues/145731\r\n\r\nTouches only compute. The CPU overhead can potentially be further reduced.\r\n\r\nBefore:\r\n```python\r\nIn [3]: n = 512\r\n\r\nIn [4]: A = torch.rand(n, n)\r\n\r\nIn [5]: B = torch.rand(n, n)\r\n\r\nIn [6]: %timeit torch.tensordot(A, B, [[0, 1], [0, 1]])\r\n2.04 ms \u00b1 70 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\r\n\r\nIn [7]: %timeit torch.tensordot(A, B, [[0, 1], [1, 0]])\r\n2.85 ms \u00b1 191 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\r\n\r\nIn [8]: %timeit torch.tensordot(A, B, [[1, 0], [0, 1]])\r\n2.9 ms \u00b1 133 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\r\n\r\nIn [9]: %timeit torch.tensordot(A, B, [[1, 0], [1, 0]])\r\n4.07 ms \u00b1 262 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\r\n```\r\n\r\nAfter\r\n```python\r\nIn [2]: n = 512\r\n\r\nIn [3]: A = torch.rand(n, n)\r\n\r\nIn [4]: B = torch.rand(n, n)\r\n\r\nIn [5]: %timeit torch.tensordot(A, B, [[0, 1], [0, 1]])\r\n30.7 \u00b5s \u00b1 2.51 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10,000 loops each)\r\n\r\nIn [6]: %timeit torch.tensordot(A, B, [[0, 1], [1, 0]])\r\n141 \u00b5s \u00b1 6.52 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10,000 loops each)\r\n\r\nIn [7]: %timeit torch.tensordot(A, B, [[1, 0], [0, 1]])\r\n142 \u00b5s \u00b1 4.03 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10,000 loops each)\r\n\r\nIn [8]: %timeit torch.tensordot(A, B, [[1, 0], [1, 0]])\r\n62.8 \u00b5s \u00b1 4.31 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10,000 loops each)\r\n\r\n```\n\ncc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o",
        "open",
        "2025-01-29T16:18:48Z",
        null,
        null,
        "oncall: distributed, open source, ciflow/trunk, release notes: python_frontend, topic: performance, ciflow/inductor",
        "main",
        "nikitaved/tensordot",
        4,
        38,
        8,
        1,
        9,
        2,
        "albanD, albanD, nikitaved, nikitaved, nikitaved",
        "APPROVED, APPROVED, COMMENTED, COMMENTED, COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145936"
    ],
    [
        145935,
        "[CPU Stream] Add noop for CPU stream record_event() and wait_event()",
        "Summary: Adds wait_event and record_event endpoints to CPU stream in order to facilitate device-agnostic code. Both methods are noops.\n\nTest Plan: CI\n\nDifferential Revision: D68833927\n\n\n\n\ncc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10",
        "open",
        "2025-01-29T15:45:43Z",
        null,
        null,
        "module: cpu, fb-exported",
        "main",
        "export-D68833927",
        1,
        6,
        0,
        1,
        3,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145935"
    ],
    [
        145930,
        "[Test][Linalg][CUDA] Increase niter in test_svd_lowrank_cuda_float64",
        "A recent PR #143049 attempted to increase tolerances to make test passable. However, we are still seeing errors like:\r\n```\r\nTraceback (most recent call last):\r\n  File \"~git/pytorch/test/test_linalg.py\", line 2540, in test_svd_lowrank\r\n    run_subtest(None, size, (), device, torch.svd_lowrank, density=density)\r\n  File \"~git/pytorch/test/test_linalg.py\", line 2505, in run_subtest\r\n    self.assertEqual(A, a, rtol=1e-7, atol=2e-7)\r\n  File \"~git/pytorch/torch/testing/_internal/common_utils.py\", line 4044, in assertEqual\r\n    raise error_metas.pop()[0].to_error(  # type: ignore[index]\r\nAssertionError: Tensor-likes are not close!\r\n\r\nMismatched elements: 90 / 1000000 (0.0%)\r\nGreatest absolute difference: 7.795904016052784e-07 at index (176, 930) (up to 2e-07 allowed)\r\nGreatest relative difference: inf at index (6, 179) (up to 1e-07 allowed)\r\n```\r\nIncreasing `niter` parameter actually decreases numerical differences.\n\ncc @ptrblck @msaroufim @eqy @jianyuh @nikitaved @pearu @mruberry @walterddr @xwang233 @Lezcano",
        "open",
        "2025-01-29T14:22:18Z",
        null,
        null,
        "module: cuda, triaged, open source, module: linear algebra, topic: not user facing",
        "main",
        "increase_niter_in_test_svd_lowrank_cuda_float64",
        1,
        1,
        1,
        1,
        1,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145930"
    ],
    [
        145923,
        "Update mi300 labels to account for multiple clusters.",
        "We now have multiple Kubernetes clusters of mi300x resources, and this commit updates labels accordingly to target both clusters evenly.\n\ncc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd",
        "open",
        "2025-01-29T08:51:26Z",
        null,
        null,
        "module: rocm, triaged, open source, Merged, Reverted, topic: not user facing, ci-no-td",
        "main",
        "mi300-labels",
        2,
        20,
        20,
        1,
        11,
        0,
        "jeffdaily",
        "DISMISSED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145923"
    ],
    [
        145922,
        "Update NestedInt equality to take into account all metadata",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #146172\n* #146101\n* __->__ #145922\n* #141842\n* #141841\n* #146052\n\n\n\ncc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @ezyang @SherlockNoMad @EikanWang @wenzhe-nrv @voznesenskym @penguinwu @Guobing-Chen @zhuhaozhe @blzheng @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-01-29T08:50:27Z",
        null,
        null,
        "module: cpu, release notes: fx, fx, module: dynamo, ciflow/inductor",
        "gh/soulitzer/347/base",
        "gh/soulitzer/347/head",
        8,
        107,
        35,
        17,
        1,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145922"
    ],
    [
        145920,
        "Add basic Gaudi support to benchmarks/dynamo",
        "This PR adds basic Gaudi support to benchmarks/dynamo\r\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-01-29T08:28:00Z",
        null,
        null,
        "triaged, open source, topic: not user facing, oncall: pt2, module: dynamo",
        "main",
        "kfojcik/basic_hpu_dynamo_benchmark",
        1,
        9,
        1,
        1,
        4,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145920"
    ],
    [
        145917,
        "Draft: fix: Some smaller mingw fixes",
        "Fixes #ISSUE_NUMBER\r\n",
        "open",
        "2025-01-29T05:27:38Z",
        null,
        null,
        "open source",
        "main",
        "fix-mingw",
        2,
        4,
        4,
        1,
        3,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/145917"
    ],
    [
        145911,
        "Add future lazy clone setting and deprecate `torch.reshape` view",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #145911\n\n\n\ncc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @voznesenskym @penguinwu @EikanWang @Guobing-Chen @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @desertfire @chauhang @aakhundov @ColinPeppler",
        "open",
        "2025-01-29T03:27:56Z",
        null,
        null,
        "oncall: distributed, module: cpu, open source, release notes: python_frontend, module: inductor, module: dynamo, ciflow/inductor",
        "gh/kurtamohler/31/base",
        "gh/kurtamohler/31/head",
        30,
        470,
        48,
        6,
        2,
        0,
        "kurtamohler",
        "COMMENTED",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/145911"
    ],
    [
        145910,
        "Fix redundant move",
        "Fixes #ISSUE_NUMBER\r\n\n\ncc @EikanWang @jgong5 @wenzhe-nrv @sanchitintel",
        "open",
        "2025-01-29T03:21:22Z",
        null,
        null,
        "oncall: jit, open source, NNC, release notes: jit",
        "main",
        "move",
        1,
        2,
        2,
        1,
        1,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/145910"
    ],
    [
        145903,
        "[NJT] Add cumsum support for nested tensors",
        "Summary: - Add cumsum support for NT\n\nTest Plan: - added unit tests\n\nDifferential Revision: D68307097\n\n\n",
        "open",
        "2025-01-29T01:24:45Z",
        null,
        null,
        "fb-exported, topic: improvements, release notes: nested tensor",
        "main",
        "export-D68307097",
        2,
        62,
        0,
        1,
        8,
        0,
        "jbschlosser, jbschlosser, ketansingh, jbschlosser",
        "COMMENTED, COMMENTED, COMMENTED, COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145903"
    ],
    [
        145885,
        "Hacky solution to bad interaction between AOTAutogradcache and Triton 3.1",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #145885\n\n",
        "open",
        "2025-01-28T22:50:12Z",
        null,
        null,
        "topic: not user facing, ciflow/inductor",
        "gh/jamesjwu/97/base",
        "gh/jamesjwu/97/head",
        2,
        28,
        1,
        2,
        1,
        0,
        "bdhirsh, jamesjwu",
        "COMMENTED, COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145885"
    ],
    [
        145873,
        "[WIP] Allow generation of inductor backend specific tests using instantiate_device_type_tests ",
        "This allows the creation of inductor backend specific test classes. Since this is an extension point for out of tree backends, it also allows out of tree backends to customise test instantiation to fit their backend / device.\r\n\r\nTo maintain backwards compatibility, `only_inductor_backends` defaults to `None` so that the behaviour of test class instantiation matches the incumbent behaviour. If `only_inductor_backends` is not None, inductor backend specific test classes will be created from a test template, e.g. `TestInductorOpInfo -> TestInductorOpInfoTritonCUSTOMDEVICE, TestInductorOpInfoHalideCUSTOMDEVICE`\r\n\r\nAn illustration of the before/after changes:\r\n\r\n```python\r\n\r\n# in test_inductor.py\r\n# Inductor test template\r\nclass TestInductor:\r\n    def test_comprehensive(...)\r\n    \r\n# Original\r\ninstantiate_device_type_tests(TestSuiteTemplate)\r\n# Generates something like this:\r\n# TestInductorCPU\r\n# TestInductorCUDA\r\n\r\n# After changes\r\ninstantiate_device_type_tests(TestSuiteTemplate, enable_inductor_backend_classes=True, only_inductor_backends=[\"cpp\", \"triton\"])\r\n# TestInductorCppCPU\r\n# TestInductorCppTriton\r\n# TestInductorTritonCUDA\r\n\r\n# Additionally, the new test classes if a native inductor backend is used are guarded\r\n# e.g.  TestInductorCppCPU\r\n# is equivalent to the following class definition\r\n# @skipUnless(HAS_CPU, \"Requires C++ compiler\")\r\n# @config.patch(\"cpu_backend\", \"cpp\")\r\n# class TestInductorCppCPU(CPUTestBase)\r\n#   ...\r\n\r\n\r\n\r\n```\r\n\r\nAn illustration of the before/after changes:\r\n\r\n\r\nFixes #ISSUE_NUMBER\r\n\r\n\r\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang @aakhundov",
        "open",
        "2025-01-28T20:54:32Z",
        null,
        null,
        "open source, module: inductor",
        "main",
        "mwizak/extend-device-agnostic-testing-with-inductor",
        4,
        195,
        71,
        5,
        2,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/145873"
    ],
    [
        145866,
        "[pytorch][cuda] Improve softmax backward pass native CUDA implementation",
        "This PR is similar to https://github.com/pytorch/pytorch/pull/122970, but works on the softmax backward pass.\r\n\r\nSpecifically, it uses shared memory to cache the gradOutput when it can fit in shared memory. Before this PR we were reading gradOutput twice.\r\n\r\nOn my H100 this seems to improve the softmax backward pass performance by about 5% for problem sizes that fit within shared memory. (Note that this is not the only kernel that runs when you call softmax backward pass -- there is an elementwise kernel that runs before this; optimizing that can be a separate PR).\r\n\r\n**Important Note**: Currently the softmax backward pass consists of an [element-wise multiply operator](https://github.com/pytorch/pytorch/blob/7f65a208848205b38445423b7e2e93a2b4994e5e/aten/src/ATen/native/cuda/SoftMax.cu#L1216), followed by [this function](https://github.com/pytorch/pytorch/blob/7f65a208848205b38445423b7e2e93a2b4994e5e/aten/src/ATen/native/cuda/SoftMax.cu#L1062) which calls the `cunn_SoftMaxBackward` kernel. With my change the kernel time reduces by about 12% (see screenshot below), while the total time (including the elementwise) reduces by about 5%.\r\n\r\n```\r\nBaseline\t\t\t\t\t\tThis PR\r\nN\tsize\tFP32 bandwidth\tFP16 bandwidth\t\tN\tsize\tFP32 bandwidth\tFP16 bandwidth\t\tfp32 diff\tfp16 diff\r\n0\t256\t134.340966\t70.042039\t\t0\t256\t133.70146\t70.342753\t\t-0.48%\t0.43%\r\n1\t512\t233.501185\t129.945803\t\t1\t512\t234.057145\t132.933066\t\t0.24%\t2.30%\r\n2\t1024\t340.667966\t229.280464\t\t2\t1024\t338.833265\t226.441699\t\t-0.54%\t-1.24%\r\n3\t2048\t379.643726\t337.452058\t\t3\t2048\t399.559017\t338.432284\t\t5.25%\t0.29%\r\n4\t4096\t416.597537\t383.625364\t\t4\t4096\t428.252403\t396.137506\t\t2.80%\t3.26%\r\n5\t6000\t431.198241\t384.384384\t\t5\t6000\t457.744577\t406.06275\t\t6.16%\t5.64%\r\n6\t8192\t462.811252\t427.292573\t\t6\t8192\t474.791032\t428.281563\t\t2.59%\t0.23%\r\n7\t10000\t464.258731\t429.050294\t\t7\t10000\t483.7643\t446.849381\t\t4.20%\t4.15%\r\n8\t10013\t465.199701\t429.824179\t\t8\t10013\t464.904407\t428.72184\t\t-0.06%\t-0.26%\r\n9\t10240\t477.07359\t428.853737\t\t9\t10240\t485.317024\t444.902586\t\t1.73%\t3.74%\r\n10\t11000\t473.038785\t430.778663\t\t10\t11000\t488.161438\t453.462162\t\t3.20%\t5.27%\r\n11\t12000\t474.342475\t432.594814\t\t11\t12000\t490.532418\t458.427653\t\t3.41%\t5.97%\r\n12\t16384\t487.468854\t473.611576\t\t12\t16384\t488.154406\t476.264631\t\t0.14%\t0.56%\r\n13\t20000\t482.029793\t465.666186\t\t13\t20000\t482.147092\t483.886193\t\t0.02%\t3.91%\r\n14\t24000\t478.368093\t474.159464\t\t14\t24000\t478.364948\t491.447921\t\t0.00%\t3.65%\r\n15\t32000\t476.523796\t473.18868\t\t15\t32000\t476.523796\t474.398962\t\t0.00%\t0.26%\r\n16\t32768\t476.104723\t477.493634\t\t16\t32768\t476.704463\t477.330606\t\t0.13%\t-0.03%\r\n17\t36864\t477.900663\t475.472787\t\t17\t36864\t477.973279\t475.728454\t\t0.02%\t0.05%\r\n18\t40960\t477.707561\t475.559064\t\t18\t40960\t478.445017\t476.088067\t\t0.15%\t0.11%\r\n19\t45056\t479.169812\t475.865134\t\t19\t45056\t479.143266\t475.878202\t\t-0.01%\t0.00%\r\n20\t49152\t477.804907\t475.382982\t\t20\t49152\t477.868404\t475.976377\t\t0.01%\t0.12%\r\n21\t65536\t481.274125\t478.171806\t\t21\t65536\t481.537733\t478.703926\t\t0.05%\t0.11%\r\n22\t66000\t481.64652\t480.095457\t\t22\t66000\t481.856013\t480.466388\t\t0.04%\t0.08%\r\n23\t68608\t481.745774\t479.034704\t\t23\t68608\t481.917596\t478.856209\t\t0.04%\t-0.04%\r\n24\t80000\t483.409361\t480.356529\t\t24\t80000\t483.330481\t480.375277\t\t-0.02%\t0.00%\r\n25\t98304\t480.736301\t481.396882\t\t25\t98304\t480.789858\t481.320143\t\t0.01%\t-0.02%\r\n```\r\n\r\nNCU profiler shows lower DRAM fetches with the new kernel:\r\n\r\n![image](https://github.com/user-attachments/assets/f3606725-d8fc-4ea5-ae6d-9c188bf32d72)\r\n\r\nNCU reports about 12% elapsed time reduction in this kernel alone compared to baseline (and because of other kernels that are run, the overall backward pass time as seen by the user gets reduced by 5%).\r\n\r\nI compared the binary size increase by running `python setup.py develop` before and after and diffing the .so files:\r\n\r\n![image](https://github.com/user-attachments/assets/8e6cee2e-3c7a-4fa4-8836-954047ce8ffc)\r\n\r\nlibtorch_cuda.so goes from 274,752,224 bytes to 274,787,072 bytes. The increase in size is 34kB which is about 0.01%.\r\n\r\nI measured the compilation time for incremental development:\r\n\r\n```\r\ntouch ./aten/src/ATen/native/cuda/SoftMax.cu\r\ntime python setup.py develop\r\nreal    0m10.083s\r\nuser    0m8.197s\r\nsys     0m3.149s\r\n```\r\n\r\nNote that this uses `ccache` and does a bunch of copies and is not just measuring the `nvcc` time. I measured the `nvcc` time separately by capturing the `nvcc` command shown in [1] below and running it on the baseline and modified kernels:\r\n\r\n```\r\n# baseline nvcc time for SoftMax.cu\r\nreal    0m35.341s\r\nuser    0m33.801s\r\nsys     0m1.289s\r\n\r\n# this PR's nvcc time for SoftMax.cu\r\nreal    0m36.513s\r\nuser    0m34.722s\r\nsys     0m1.408s\r\n```\r\n\r\nSo the `nvcc` time increases by about 1 second, or ~3% of the baseline.\r\n\r\n[1] `nvcc` command is here:\r\n```\r\n# This is the nvcc command\r\n/usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DAT_PER_OPERATOR_HEADERS -DFLASHATTENTION_DISABLE_ALIBI -DFMT_HEADER_ONLY=1 -DHAVE_MALLOC_USABLE_SIZE=1 -DHAVE_MMAP=1 -DHAVE_SHM_OPEN=1 -DHAVE_SHM_UNLINK=1 -DMINIZ_DISABLE_ZIP_READER_CRC32_CHECKS -DONNXIFI_ENABLE_EXT=1 -DONNX_ML=1 -DONNX_NAMESPACE=onnx_torch -DTORCH_CUDA_BUILD_MAIN_LIB -DTORCH_CUDA_USE_NVTX3 -DUSE_C10D_GLOO -DUSE_C10D_NCCL -DUSE_CUDA -DUSE_DISTRIBUTED -DUSE_EXTERNAL_MZCRC -DUSE_FLASH_ATTENTION -DUSE_MEM_EFF_ATTENTION -DUSE_NCCL -DUSE_RPC -DUSE_TENSORPIPE -D_FILE_OFFSET_BITS=64 -Dtorch_cuda_EXPORTS -I/home/ahmads/personal/pytorch/build/aten/src -I/home/ahmads/personal/pytorch/aten/src -I/home/ahmads/personal/pytorch/build -I/home/ahmads/personal/pytorch -I/home/ahmads/personal/pytorch/cmake/../third_party/benchmark/include -I/home/ahmads/personal/pytorch/third_party/onnx -I/home/ahmads/personal/pytorch/build/third_party/onnx -I/home/ahmads/personal/pytorch/nlohmann -I/home/ahmads/personal/pytorch/aten/src/THC -I/home/ahmads/personal/pytorch/aten/src/ATen/cuda -I/home/ahmads/personal/pytorch/third_party/fmt/include -I/home/ahmads/personal/pytorch/aten/src/ATen/../../../third_party/cutlass/include -I/home/ahmads/personal/pytorch/aten/src/ATen/../../../third_party/cutlass/tools/util/include -I/home/ahmads/personal/pytorch/build/caffe2/aten/src -I/home/ahmads/personal/pytorch/aten/src/ATen/.. -I/home/ahmads/personal/pytorch/build/nccl/include -I/home/ahmads/personal/pytorch/c10/cuda/../.. -I/home/ahmads/personal/pytorch/c10/.. -I/home/ahmads/personal/pytorch/third_party/tensorpipe -I/home/ahmads/personal/pytorch/build/third_party/tensorpipe -I/home/ahmads/personal/pytorch/third_party/tensorpipe/third_party/libnop/include -I/home/ahmads/personal/pytorch/torch/csrc/api -I/home/ahmads/personal/pytorch/torch/csrc/api/include -isystem /home/ahmads/personal/pytorch/build/third_party/gloo -isystem /home/ahmads/personal/pytorch/cmake/../third_party/gloo -isystem /home/ahmads/personal/pytorch/cmake/../third_party/tensorpipe/third_party/libuv/include -isystem /home/ahmads/personal/pytorch/cmake/../third_party/googletest/googlemock/include -isystem /home/ahmads/personal/pytorch/cmake/../third_party/googletest/googletest/include -isystem /home/ahmads/personal/pytorch/third_party/protobuf/src -isystem /home/ahmads/personal/pytorch/third_party/XNNPACK/include -isystem /home/ahmads/personal/pytorch/third_party/ittapi/include -isystem /home/ahmads/personal/pytorch/cmake/../third_party/eigen -isystem /usr/local/cuda/include -isystem /home/ahmads/personal/pytorch/torch/include -isystem /home/ahmads/personal/pytorch/third_party/ideep/include -isystem /home/ahmads/personal/pytorch/torch/include/oneapi/dnnl -isystem /home/ahmads/personal/pytorch/INTERFACE -isystem /home/ahmads/personal/pytorch/third_party/nlohmann/include -isystem /home/ahmads/personal/pytorch/third_party/NVTX/c/include -isystem /home/ahmads/personal/pytorch/cmake/../third_party/cudnn_frontend/include -DLIBCUDACXX_ENABLE_SIMPLIFIED_COMPLEX_OPERATIONS -D_GLIBCXX_USE_CXX11_ABI=1 -Xfatbin -compress-all -DONNX_NAMESPACE=onnx_torch -gencode arch=compute_90,code=sm_90 -Xcudafe --diag_suppress=cc_clobber_ignored,--diag_suppress=field_without_dll_interface,--diag_suppress=base_class_has_different_dll_interface,--diag_suppress=dll_interface_conflict_none_assumed,--diag_suppress=dll_interface_conflict_dllexport_assumed,--diag_suppress=bad_friend_decl --expt-relaxed-constexpr --expt-extended-lambda  -Wno-deprecated-gpu-targets --expt-extended-lambda -DCUB_WRAPPED_NAMESPACE=at_cuda_detail -DCUDA_HAS_FP16=1 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -O3 -DNDEBUG -std=c++17 -Xcompiler=-fPIC -DTORCH_USE_LIBUV -DCAFFE2_USE_GLOO -Xcompiler -Wall -Wextra -Wdeprecated -Wno-unused-parameter -Wno-missing-field-initializers -Wno-array-bounds -Wno-unknown-pragmas -Wno-strict-overflow -Wno-strict-aliasing -Wunused-function -Wunused-variable -Wunused-but-set-variable -Wno-maybe-uninitialized -MD -MT caffe2/CMakeFiles/torch_cuda.dir/__/aten/src/ATen/native/cuda/SoftMax.cu.o -MF caffe2/CMakeFiles/torch_cuda.dir/__/aten/src/ATen/native/cuda/SoftMax.cu.o.d -x cu -c /home/ahmads/personal/pytorch/aten/src/ATen/native/cuda/SoftMax.cu -o caffe2/CMakeFiles/torch_cuda.dir/__/aten/src/ATen/native/cuda/SoftMax.cu.o\r\n```\r\n\r\n\r\n\r\n\r\n\r\n",
        "open",
        "2025-01-28T20:26:34Z",
        null,
        null,
        "release notes: cuda",
        "main",
        "softmax1",
        4,
        157,
        12,
        13,
        2,
        1,
        "ngimel, ahmadsharif1",
        "COMMENTED, COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145866"
    ],
    [
        145865,
        "[WIP] Add test_torchinductor_opinfo.py to triton-cpu tests",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #145865\n\nI'm guessing this isn't going to pass, but I want to see how long the CI takes.",
        "open",
        "2025-01-28T20:21:09Z",
        null,
        null,
        "ciflow/inductor, keep-going",
        "gh/davidberard98/335/base",
        "gh/davidberard98/335/head",
        1,
        1,
        1,
        1,
        5,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/145865"
    ],
    [
        145863,
        "Cleanup VS 2019 refs in pytorch",
        "Related to: https://github.com/pytorch/pytorch/issues/128835\r\nFollow up on PR: https://github.com/pytorch/pytorch/pull/145319",
        "open",
        "2025-01-28T19:26:12Z",
        null,
        null,
        "ciflow/binaries, ciflow/trunk, release notes: releng, test-config/default",
        "main",
        "cleanup_vs_2019",
        10,
        9,
        130,
        4,
        8,
        1,
        "Skylion007, huydhn, malfet, huydhn, huydhn, atalman",
        "APPROVED, COMMENTED, APPROVED, APPROVED, COMMENTED, APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145863"
    ],
    [
        145854,
        "[c10d][UCC] Support coalesing in `c10d::ProcessGroupUCC` through alltoallv",
        "# What \r\nAdd support for coalescing communication in `ProcessGroupUCC`, by implementing the methods `startCoalescing` and `endCoalescing`.\r\n\r\nWe need to impose several restrictions to the coalesced group:\r\n1) we can only coalesce `send` and `recv` ops\r\n2) we can only coalesce one `send` and one `recv` maximum per pair of ranks\r\n3) all ranks must participate in the `startCoalescing` and `endCoalescing` calls, even if the group is empty.\r\n4) we do not support tags for coalesced groups.\r\n\r\n# Why\r\n\r\nDespite the above restrictions, we cover a number of useful data patterns, such as ring p2p, allgather, broadcast, alltoall, etc. Those data patterns or other custom ones are conveniently written in terms of coalesced send/recv calls, which this patch makes possible.\r\n\r\nRecall that for a p2p bidirectional transfer, the send and recv need to be coalesced to enjoy full bidirectional bandwidth\r\n\r\n# How\r\n\r\nSince UCC does not natively support Coalescing, we implement it at the ProcessGroup level. The implementation relies on calling UCC's alltoallv, setting the count to `0` and displacement to `nullptr` for ranks that do not exchange data.\n\ncc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o",
        "open",
        "2025-01-28T17:16:51Z",
        null,
        null,
        "oncall: distributed, triaged, open source, release notes: distributed (c10d)",
        "main",
        "ucc_coalesced_a2av",
        3,
        151,
        21,
        2,
        2,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145854"
    ],
    [
        145853,
        "cmake: fix detection logic when using system XNNPACK",
        "This commit makes the following improvements:\r\n\r\n* The \"elseif\" branch now only runs when USE_XNNPACK is set.\r\n* Use \"REQUIRED\" to enforce the existence of XNNPACK libraries, and remove the erroneous if statement ('or' should be 'OR').\r\n* libmicrokernels-prod is built statically in XNNPACK [1], change in pytorch side accordingly.\r\n\r\n[1]: https://github.com/google/XNNPACK/blob/d7f398ee5e135ef4f7045802eea973cc6cb26c6c/CMakeLists.txt#L819\r\n\r\n",
        "open",
        "2025-01-28T17:05:50Z",
        null,
        null,
        "triaged, open source, topic: not user facing",
        "main",
        "fix-system-xnnpack",
        1,
        4,
        7,
        1,
        2,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145853"
    ],
    [
        145850,
        "Skip search for MKL on ARM cpus",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #145871\n* #145870\n* __->__ #145850\n\nIt will not find it anyway and makes a bit easier parsing thru CMake log on non-x86 systems",
        "open",
        "2025-01-28T16:40:35Z",
        null,
        null,
        "topic: not user facing",
        "gh/malfet/155/base",
        "gh/malfet/155/head",
        1,
        5,
        0,
        1,
        1,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145850"
    ],
    [
        145847,
        "Improve the guards on some cpp wrapper tests ",
        "Since they're the CPU CPP wrapper tests, they should only run if the CPU backend we're using is the CPP one.\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang @aakhundov",
        "open",
        "2025-01-28T16:22:39Z",
        null,
        null,
        "triaged, open source, topic: not user facing, module: inductor",
        "main",
        "charliew/test-skip",
        1,
        1,
        1,
        1,
        5,
        0,
        "desertfire",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145847"
    ],
    [
        145834,
        "[inductor] Add features to docstring_linter (see #142496)",
        "## Improvements to `docstring_linter`\r\n\r\n* Add a \"grandfather list\" of existing undocumented classes and functions (`--grandfather`, `--grandfather-tolerance`, `--no-grandfather`, `--write-grandfather`)\r\n* In classes, now just one of the class itself or its `__init__()` method needs to be documented (`--lint-init` turns the old behavior back on)\r\n* Now classes and functions defined local to other functions do not need to be documented (`--lint-local` turns the old behavior back on)\r\n* New `--report` flag produces a compact report of long, undocumented classes or function definitions: see attached example run over all pytorch: [pytorch-docs.json](https://github.com/user-attachments/files/18455981/pytorch-docs.json)\r\n\r\n## Help text\r\n\r\n```\r\n$ python tools/linter/adapters/docstring_linter.py --help\r\nusage: docstring_linter.py [-h] [-l] [-v] [--grandfather GRANDFATHER] [--grandfather-tolerance GRANDFATHER_TOLERANCE] [--lint-init]\r\n                           [--lint-local] [--lint-protected] [--max-class MAX_CLASS] [--max-def MAX_DEF]\r\n                           [--min-docstring MIN_DOCSTRING] [--no-grandfather] [--report] [--write-grandfather]\r\n                           [files ...]\r\n\r\n`docstring_linter` reports on long functions, methods or classes without docstrings\r\n\r\npositional arguments:\r\n  files                 A list of files or directories to lint\r\n\r\noptional arguments:\r\n  -h, --help            show this help message and exit\r\n  -l, --lintrunner      Run for lintrunner and print LintMessages which aren't edits\r\n  -v, --verbose         Print more debug info\r\n  --grandfather GRANDFATHER, -g GRANDFATHER\r\n                        Set the grandfather list\r\n  --grandfather-tolerance GRANDFATHER_TOLERANCE, -t GRANDFATHER_TOLERANCE\r\n                        Tolerance for grandfather sizes, in percent\r\n  --lint-init, -i       Lint __init__ and class separately\r\n  --lint-local, -o      Lint definitions inside other functions\r\n  --lint-protected, -p  Lint functions, methods and classes that start with _\r\n  --max-class MAX_CLASS, -c MAX_CLASS\r\n                        Maximum number of lines for an undocumented class\r\n  --max-def MAX_DEF, -d MAX_DEF\r\n                        Maximum number of lines for an undocumented function\r\n  --min-docstring MIN_DOCSTRING, -s MIN_DOCSTRING\r\n                        Minimum number of characters for a docstring\r\n  --no-grandfather, -n  Disable the grandfather list\r\n  --report, -r          Print a report on all classes and defs\r\n  --write-grandfather, -w\r\n                        Rewrite the grandfather list\r\n```\r\n\r\n---\r\n\r\nStack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #144622\n* #144621\n* __->__ #145834\n* #144620\n\r\n",
        "open",
        "2025-01-28T12:03:29Z",
        null,
        null,
        "module: lint, open source, better-engineering, topic: not user facing, suppress-api-compatibility-check, suppress-bc-linter",
        "gh/rec/128/base",
        "gh/rec/128/head",
        9,
        1199,
        191,
        5,
        2,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145834"
    ],
    [
        145833,
        "Implement KL for studentT",
        "Fixes #145729\r\n",
        "open",
        "2025-01-28T10:37:07Z",
        null,
        null,
        "triaged, open source",
        "main",
        "fix/student_t_kl",
        1,
        66,
        0,
        1,
        2,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145833"
    ],
    [
        145832,
        "[DO NOT MERGE] [TESTING] [ROCm] Triton cherry-picks for AMD backend perf optimisation",
        "Testing for rc/3.2.x PR\n\ncc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @hongxiayang @naromero77amd",
        "open",
        "2025-01-28T10:21:32Z",
        null,
        null,
        "module: rocm, open source, ciflow/trunk, topic: not user facing, ciflow/periodic, ciflow/inductor, ciflow/inductor-perf-compare, ciflow/rocm, ciflow/inductor-rocm, ciflow/inductor-periodic",
        "main",
        "rc32-cps",
        2,
        2,
        2,
        2,
        1,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/145832"
    ],
    [
        145822,
        "Remove unneeded CUDA logic from _create_build_env",
        "Because FindCUDAToolkit.cmake has that logic.\r\n",
        "open",
        "2025-01-28T03:34:42Z",
        null,
        null,
        "triaged, open source, topic: not user facing",
        "main",
        "cmake_find",
        1,
        0,
        10,
        1,
        1,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145822"
    ],
    [
        145819,
        "Replace distutils.version with  copied looseversion",
        "distutils was deprecated.\r\n",
        "open",
        "2025-01-28T03:17:56Z",
        null,
        null,
        "triaged, open source, topic: not user facing",
        "main",
        "looseversion",
        1,
        92,
        2,
        1,
        2,
        1,
        "albanD",
        "COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145819"
    ],
    [
        145812,
        "[cutlass backend] check against arch >= 100",
        "Summary:\nWant to add a guard against silent fallback to SM90.\n\nGenerateSM100 was just added 3 days ago. https://github.com/NVIDIA/cutlass/blame/main/python/cutlass_library/generator.py#L8896\n\nIt should show up in CUTLASS 3.8 (not pinned yet).\n\nTest Plan: ci\n\nDifferential Revision: D68748705\n\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang @aakhundov",
        "open",
        "2025-01-28T02:03:04Z",
        null,
        null,
        "fb-exported, ciflow/trunk, topic: not user facing, module: inductor, ciflow/inductor",
        "main",
        "export-D68748705",
        1,
        21,
        2,
        1,
        7,
        0,
        "chenyang78, ColinPeppler, ColinPeppler, Aidyn-A, henrylhtsang, Aidyn-A, henrylhtsang",
        "APPROVED, COMMENTED, APPROVED, CHANGES_REQUESTED, COMMENTED, COMMENTED, COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145812"
    ],
    [
        145811,
        "[AsyncMM] re-enable and adapt to cutlass 3.6.0 (#144011)",
        "Summary:\n\n\n\n\ncc H-Huang awgu kwen2501 wanchaol fegin fduwjj wz337 wconstab d4l3k c-p-i-o\n\nimported-using-ghimport\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D68734003\n\nPulled By: yifuwang\n\n\n\n\ncc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o",
        "open",
        "2025-01-28T01:59:16Z",
        null,
        null,
        "oncall: distributed, fb-exported, release notes: distributed (c10d)",
        "main",
        "export-D68734003",
        2,
        262,
        119,
        1,
        4,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145811"
    ],
    [
        145798,
        "[will-not-merge] tuning",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* (to be filled)\n\n\n\ncc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o",
        "open",
        "2025-01-27T23:43:34Z",
        null,
        null,
        "oncall: distributed, release notes: distributed (c10d)",
        "gh/yifuwang/192/base",
        "gh/yifuwang/192/head",
        3,
        49,
        59,
        1,
        1,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/145798"
    ],
    [
        145797,
        "[Async-TP] improve algo selection",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #145798\n* __->__ #145797\n* #145796\n* #145795\n* #145794\n\n",
        "open",
        "2025-01-27T23:43:30Z",
        null,
        null,
        "oncall: distributed",
        "gh/yifuwang/191/base",
        "gh/yifuwang/191/head",
        2,
        134,
        87,
        1,
        2,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/145797"
    ],
    [
        145796,
        "[Async-TP] _pipelined_multi_all_gather_and_consume reduce overhead",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #145798\n* #145797\n* __->__ #145796\n* #145795\n* #145794\n\n",
        "open",
        "2025-01-27T23:43:25Z",
        null,
        null,
        "oncall: distributed",
        "gh/yifuwang/190/base",
        "gh/yifuwang/190/head",
        1,
        15,
        18,
        1,
        2,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/145796"
    ],
    [
        145795,
        "[AsyncMM] preliminary tuning",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #145798\n* #145797\n* #145796\n* __->__ #145795\n* #145794\n\n",
        "open",
        "2025-01-27T23:43:21Z",
        null,
        null,
        "oncall: distributed, release notes: distributed (c10d)",
        "gh/yifuwang/189/base",
        "gh/yifuwang/189/head",
        1,
        57,
        4,
        1,
        1,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/145795"
    ],
    [
        145794,
        "[Async-TP] Port _fused_all_gather_matmul_native to cpp to reduce launching overhead",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\r\n* #145798\r\n* #145797\r\n* #145796\r\n* #145795\r\n* __->__ #145794\r\n\r\n`_fused_all_gather_matmul_native` schedules multiple tasks (e.g., kernel, copy engine transfers, and stream_write_value32_) onto the GPU. Previously, `fused_all_gather_matmul_native` was implemented in Python, and issuing most of these tasks incurred dispatcher overhead. When the problem size is small, the CPU overhead can exceed the GPU\u2019s execution time. While this may be acceptable in workloads where the CPU runs ahead of the GPU, it still isn\u2019t ideal.\r\n\r\nThis PR reduces CPU overhead by porting `_fused_all_gather_matmul_native` to C++. Specifically, it eliminates dispatcher overhead for:\r\n- `aten.split` (calling `aten.narrow` \u00d7 `world_size` times)\r\n- `symm_mem::stream_write_value32_` \u00d7 `world_size` times\r\n\r\n<img width=\"842\" alt=\"image\" src=\"https://github.com/user-attachments/assets/176ebc89-a2e1-4c07-b340-c2d4422def09\" />\r\n<img width=\"455\" alt=\"image\" src=\"https://github.com/user-attachments/assets/3768f0c1-876f-4c66-bd22-89aa80d0889c\" />",
        "open",
        "2025-01-27T23:43:16Z",
        null,
        null,
        "oncall: distributed, release notes: distributed (c10d)",
        "gh/yifuwang/188/base",
        "gh/yifuwang/188/head",
        4,
        135,
        46,
        1,
        1,
        1,
        "lw",
        "APPROVED",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/145794"
    ],
    [
        145779,
        "[CUDNN][CUDNN V8 API] Allow user-specified CUDNN V8 API benchmarking technique",
        "Useful for debugging apparent \"regressions\" when using cuDNN autotuning (\"benchmarking\")\n\ncc @csarofeen @ptrblck @xwang233",
        "open",
        "2025-01-27T21:33:12Z",
        null,
        null,
        "module: cudnn, triaged, open source, topic: not user facing",
        "main",
        "cudnnv8technique",
        1,
        73,
        6,
        3,
        1,
        0,
        "Skylion007, eqy",
        "COMMENTED, COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145779"
    ],
    [
        145778,
        "NJT support for cat() on the ragged dim",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\r\n* __->__ #145778\r\n\r\nRequested [here](https://github.com/pytorch/pytorch/issues/118107#issuecomment-2615705795).\r\n\r\nThere's still a fair amount of work left. TODO:\r\n* Fix the backwards pass (need NJT-specific derivative formula, possibly `narrow()` on the ragged dim)\r\n* Fix data-dependency errors in forward + torch.compile() due to `unbind()` usage",
        "open",
        "2025-01-27T21:28:00Z",
        null,
        null,
        "topic: not user facing",
        "gh/jbschlosser/226/base",
        "gh/jbschlosser/226/head",
        3,
        161,
        6,
        1,
        1,
        0,
        "Skylion007",
        "COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145778"
    ],
    [
        145776,
        "Update to NCCL 2.25.1 for 12.8",
        "https://github.com/pytorch/pytorch/issues/145570\r\n\r\nfollow up for https://github.com/pytorch/pytorch/pull/145567/files",
        "open",
        "2025-01-27T21:16:42Z",
        null,
        null,
        "triaged, open source, topic: not user facing",
        "main",
        "nccl-update-12.8",
        2,
        4,
        0,
        1,
        3,
        1,
        "eqy, Skylion007",
        "APPROVED, CHANGES_REQUESTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145776"
    ],
    [
        145774,
        "[POC] flat_apply HOP",
        "[no-ci]\r\n\r\nFixes #ISSUE_NUMBER\r\n\n\ncc @ezyang @SherlockNoMad @EikanWang @jgong5 @wenzhe-nrv",
        "open",
        "2025-01-27T20:46:12Z",
        null,
        null,
        "release notes: fx, fx",
        "main",
        "flat_apply",
        3,
        183,
        0,
        2,
        1,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/145774"
    ],
    [
        145772,
        "Implement serializable getattr support for tensor subclasses",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #145772\n\r\nbuiltins.getattr is not serializable, so we replace it with a custom op that has more refined schema. \r\n\r\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames\n\nDifferential Revision: [D68899421](https://our.internmc.facebook.com/intern/diff/D68899421)",
        "open",
        "2025-01-27T20:19:13Z",
        null,
        null,
        "ciflow/trunk, module: dynamo, ciflow/inductor, release notes: export",
        "gh/tugsbayasgalan/288/base",
        "gh/tugsbayasgalan/288/head",
        5,
        49,
        30,
        8,
        4,
        1,
        "bdhirsh, bdhirsh, tugsbayasgalan, bdhirsh",
        "COMMENTED, COMMENTED, COMMENTED, APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145772"
    ]
][
    [
        146764,
        "Fix standalone runner for CUTLASS auto-tuning backend",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146764\n* #146755\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov",
        "open",
        "2025-02-08T17:23:45Z",
        null,
        null,
        "open source, topic: not user facing, module: inductor, ciflow/inductor",
        "gh/alexsamardzic/26/base",
        "gh/alexsamardzic/26/head",
        1,
        64,
        8,
        2,
        2,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146764"
    ],
    [
        146763,
        "[Break XPU] Align meta calculation for fft_r2c with _fft_r2c_mkl",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146763\n* #145248\n* #146762\n\nFix #146761",
        "open",
        "2025-02-08T16:20:39Z",
        null,
        null,
        "open source, ciflow/trunk, topic: not user facing, ciflow/inductor, ciflow/xpu",
        "gh/etaf/98/base",
        "gh/etaf/98/head",
        1,
        3,
        1,
        1,
        1,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146763"
    ],
    [
        146762,
        "[Break XPU][Inductor UT] Fix XPU Inductor UT introduced from community.",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #146763\n* #145248\n* __->__ #146762\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov",
        "open",
        "2025-02-08T16:20:35Z",
        null,
        null,
        "open source, topic: not user facing, module: inductor, ciflow/inductor",
        "gh/etaf/97/base",
        "gh/etaf/97/head",
        4,
        10,
        2,
        1,
        1,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146762"
    ],
    [
        146756,
        "[Inductor][CPU] Add GEMM tamplates for _weight_int4pack_mm_for_cpu with AVX512",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146756\n* #145250\n* #145245\n\r\n**Summary**\r\nIt's part of the task to enable max-autotune with GEMM template for WoQ INT4 GEMM on CPU. Early work: #133846\r\n\r\nThis PR adds GEMM templates for `torch.ops.aten_weight_int4pack_mm_for_cpu`. The micro kernel used for the templates is based on AVX512 and it's a copy of the ATen implementation of `torch.ops.aten_weight_int4pack_mm_for_cpu` with minor changes.\r\n\r\nDue to better blocking and loop schedule, the GEMM template based implementation outperforms the ATen implementation in all cases we tested.\r\n\r\n**Test plan**\r\n```\r\npython test/inductor/test_cpu_select_algorithm.py -k test_int4_woq_mm_avx512\r\n```\r\n\r\ncc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @voznesenskym @penguinwu @EikanWang @Guobing-Chen @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov",
        "open",
        "2025-02-08T13:51:09Z",
        null,
        null,
        "open source, ciflow/trunk, topic: not user facing, intel, module: inductor, ciflow/inductor",
        "gh/Xia-Weiwen/30/base",
        "gh/Xia-Weiwen/30/head",
        8,
        468,
        21,
        2,
        1,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146756"
    ],
    [
        146755,
        "Fix CUTLASS 2.x kernels for auto-tuning",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #146764\n* __->__ #146755\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov",
        "open",
        "2025-02-08T12:01:14Z",
        null,
        null,
        "open source, topic: not user facing, module: inductor, ciflow/inductor",
        "gh/alexsamardzic/25/base",
        "gh/alexsamardzic/25/head",
        3,
        12,
        30,
        1,
        1,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146755"
    ],
    [
        146754,
        "[MPS] fix inverse bug for N>1024",
        "Fixes #138200 \r\n",
        "open",
        "2025-02-08T10:24:29Z",
        null,
        null,
        "open source, release notes: mps",
        "main",
        "mps-inverse-bugfix",
        4,
        26,
        27,
        1,
        1,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146754"
    ],
    [
        146753,
        "[MPS] fix lu factor for large tensors with bs>1",
        "Try this:\r\n```\r\nimport torch\r\n\r\nbatch_size = 2\r\nA = torch.eye(256, device=\"mps\")[None, :, :].expand(batch_size, -1, -1) + 0.1 * torch.randn((batch_size, 256, 256), device=\"mps\")\r\nA_cpu = A.cpu()\r\nLU_cpu, pivots_cpu = torch.linalg.lu_factor(A_cpu)\r\nLU, pivots = torch.linalg.lu_factor(A)\r\ntorch.testing.assert_close(LU.cpu(), LU_cpu)\r\n```\r\nYou'll get huge difference in LU tensors\r\n<img width=\"706\" alt=\"Screenshot 2025-02-08 at 12 14 39\" src=\"https://github.com/user-attachments/assets/b45f2b3c-e0a5-49c8-aa07-42792150b781\" />\r\n",
        "open",
        "2025-02-08T08:15:29Z",
        null,
        null,
        "open source, release notes: mps",
        "main",
        "lu-factor-fix-batches",
        2,
        7,
        5,
        1,
        1,
        0,
        "Isalia20",
        "COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146753"
    ],
    [
        146752,
        "realize stride symbols in estimate_runtime",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\r\n* __->__ #146752\r\n\r\nUnfortuanlty could not create a local repo, or unit test.\r\nfix https://github.com/pytorch/pytorch/issues/146686\r\n\r\n",
        "open",
        "2025-02-08T07:30:03Z",
        null,
        null,
        "topic: not user facing, ciflow/inductor",
        "gh/laithsakka/107/base",
        "gh/laithsakka/107/head",
        1,
        2,
        1,
        1,
        1,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146752"
    ],
    [
        146751,
        "[MTIA] (4/n) Implement PyTorch APIs to query/reset device peak memory usage",
        "Summary: Public summary (shared with Github): This diff updates the unit test for the PyTorch API \"reset_peak_memory_stats\".\n\nTest Plan:\n```\nbuck2 test //mtia/host_runtime/torch_mtia/tests:test_torch_mtia_api -- -r test_reset_peak_memory_stats\n```\n\nhttps://www.internalfb.com/intern/testinfra/testrun/9007199321947161\n\nReviewed By: yuhc\n\nDifferential Revision: D68989900\n\n\n",
        "open",
        "2025-02-08T07:16:36Z",
        null,
        null,
        "fb-exported",
        "main",
        "export-D68989900",
        2,
        17,
        0,
        1,
        3,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146751"
    ],
    [
        146750,
        "Update instructions about faster linker",
        "This PR adds instructions to specify linker via cmake env `CMAKE_LINKER_TYPE` and also adds `mold` as a linker alternative.\r\n\r\nSince 3.29, cmake introduced [`CMAKE_LINKER_TYPE`](https://cmake.org/cmake/help/latest/variable/CMAKE_LINKER_TYPE.html) that can specify linker without overwriting `ld` file or changing build script.\r\n\r\n`mold` is already stable and **the fastest** (afaict) linker out there, and also easier to install compared with `lld`. So I added it here. After switching to `mold`, the time of linking `libtorch_cuda.so` has been reduced from ~7s to ~0.6s locally.\r\n\r\nAlso note `gold` has been marked deprecated recently[1].\r\n\r\n[1] https://lwn.net/Articles/1007541/",
        "open",
        "2025-02-08T06:30:17Z",
        null,
        null,
        "open source, topic: not user facing",
        "main",
        "update-linker",
        1,
        6,
        8,
        2,
        1,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146750"
    ],
    [
        146748,
        "Update strided test to float32",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\r\n* __->__ #146748\r\n\r\nFixes #146377\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov",
        "open",
        "2025-02-08T04:48:51Z",
        null,
        null,
        "ciflow/trunk, topic: not user facing, module: inductor, ciflow/inductor",
        "gh/drisspg/122/base",
        "gh/drisspg/122/head",
        1,
        3,
        3,
        1,
        6,
        1,
        "BoyuanFeng, leijurv",
        "APPROVED, APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146748"
    ],
    [
        146747,
        "Add hint message for `pack_padded_sequence`",
        "Fixes #144207\r\n\r\nAdd truncate hint message in docs [torch.nn.utils.rnn.pack_padded_sequence](https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pack_padded_sequence.html)\r\n\r\n## Test Result\r\n\r\n![image](https://github.com/user-attachments/assets/46258f36-f6c7-4f11-9213-8513e52a9001)\r\n\r\n",
        "open",
        "2025-02-08T04:28:15Z",
        null,
        null,
        "open source, topic: not user facing",
        "main",
        "opt/nn/rnn",
        1,
        5,
        1,
        1,
        2,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146747"
    ],
    [
        146746,
        "[Inductor] Fix the lowering of squeeze when input is not contiguous",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146746\n\r\n**Summary**\r\nFix issue https://github.com/pytorch/pytorch/issues/143498. The issue happens when we lowering `select = torch.ops.aten.select.int(cat, 1, 0)`. \r\n\r\nFor example, when `cat` is contiguous with size[2, 2] stride[2,1]\r\n\r\n- for eager, it returns a view of size[2,] stride[2,]\r\n- for Inductor lowering, it returns wrong stride 1 instead of 2\r\n```\r\nTensorBox(\r\n  ReinterpretView(\r\n    StorageBox(\r\n      ConcatKernel(name='buf10', layout=FixedLayout('cpu', torch.int64, size=[u0, 2], stride=[2, 1]), inputs=[ComputedBuffer(name='buf8', layout=NonOwningLayout('cpu', torch.int64, size=[u0, 1], stride=[2, 1]), data=Pointwise(device=device(type='cpu'), dtype=torch.int64, inner_fn=<function ReinterpretView.make_loader.<locals>.loader at 0x7f6b856449d0>, ranges=[u0, 1])), ComputedBuffer(name='buf9', layout=NonOwningLayout('cpu', torch.int64, size=[u0, 1], stride=[2, 1]), data=Pointwise(device=device(type='cpu'), dtype=torch.int64, inner_fn=<function ReinterpretView.make_loader.<locals>.loader at 0x7f6b85644790>, ranges=[u0, 1]))])\r\n    ),\r\n    FixedLayout('cpu', torch.int64, size=[u0], stride=[**1**]),\r\n    origins=OrderedSet([select])\r\n  )\r\n)\r\n```\r\n\r\nTo fix this issue, we give the right stride when lowering of `squeeze`.\r\n\r\n**Test Plan**\r\n```\r\npython -u -m pytest -s -v test/inductor/test_unbacked_symints.py -k test_issue_143498\r\n```\r\n\r\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov",
        "open",
        "2025-02-08T03:50:25Z",
        null,
        null,
        "open source, ciflow/trunk, topic: not user facing, module: inductor, ciflow/inductor",
        "gh/leslie-fang-intel/180/base",
        "gh/leslie-fang-intel/180/head",
        2,
        16,
        4,
        2,
        1,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146746"
    ],
    [
        146743,
        "[cutlass backend] refactor tests to remove duplicate logic",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146743\n* #146356\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov",
        "open",
        "2025-02-08T01:36:44Z",
        null,
        null,
        "topic: not user facing, module: inductor, ciflow/inductor",
        "gh/henrylhtsang/3/base",
        "gh/henrylhtsang/3/head",
        1,
        57,
        81,
        1,
        1,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146743"
    ],
    [
        146742,
        "[Dynamo][autograd.Function] Relax backward speculation strict mode: support .requires_grad",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146742\n* #146741\n* #146571\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-02-08T01:30:53Z",
        null,
        null,
        "topic: not user facing, module: dynamo, ciflow/inductor",
        "gh/yanboliang/65/base",
        "gh/yanboliang/65/head",
        2,
        27,
        1,
        1,
        1,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146742"
    ],
    [
        146741,
        "[Dynamo][autograd.Function] Relax backward speculation strict mode: support .data",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #146742\n* __->__ #146741\n* #146571\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-02-08T01:30:49Z",
        null,
        null,
        "topic: not user facing, module: dynamo, ciflow/inductor",
        "gh/yanboliang/64/base",
        "gh/yanboliang/64/head",
        2,
        29,
        1,
        1,
        1,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146741"
    ],
    [
        146739,
        "Testing",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146739\n* #145748\n\nThis reverts commit 5cd5b4d2d54c0220b92ee488dd36d789c9b60af3.",
        "open",
        "2025-02-08T00:30:57Z",
        null,
        null,
        "release notes: releng, ciflow/binaries_wheel",
        "gh/mikaylagawarecki/312/base",
        "gh/mikaylagawarecki/312/head",
        8,
        25,
        80,
        2,
        1,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146739"
    ],
    [
        146738,
        "[audio hash update] update the pinned audio hash",
        "This PR is auto-generated nightly by [this action](https://github.com/pytorch/pytorch/blob/main/.github/workflows/nightly.yml).\nUpdate the pinned audio hash.",
        "open",
        "2025-02-08T00:23:09Z",
        null,
        null,
        "open source, ciflow/trunk, topic: not user facing, ciflow/inductor, merging",
        "main",
        "update-audio-commit-hash/13210264744-1454-1",
        1,
        1,
        1,
        1,
        4,
        0,
        "pytorchbot",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146738"
    ],
    [
        146737,
        "[dynamo][user-defined] Unify standard and non-standard __new__ codebase",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #146678\n* __->__ #146737\n* #146677\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-02-08T00:21:25Z",
        null,
        null,
        "ciflow/trunk, topic: not user facing, module: dynamo, ciflow/inductor, keep-going",
        "gh/anijain2305/676/base",
        "gh/anijain2305/676/head",
        1,
        17,
        28,
        2,
        1,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146737"
    ],
    [
        146736,
        "Document dynamo",
        "Many files in dynamo are currently lacking file/module-level documentation, which makes it hard to know what they do at a glance and without digging into the code. This fixes that.\r\n\r\nNote: documentation was AI-generated and could be incorrect, please review carefully.\r\n\r\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @xmfan @svekars @brycebortree @sekyondaMeta @AlannaBurke",
        "open",
        "2025-02-08T00:15:41Z",
        null,
        null,
        "better-engineering, ciflow/trunk, topic: not user facing, module: inductor, module: dynamo, ciflow/inductor, module: compiled autograd",
        "main",
        "gh/raymo/document-dynamo",
        30,
        601,
        45,
        1,
        4,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146736"
    ],
    [
        146735,
        "[ca] log graph before reodering passes",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146735\n* #146720\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @yf225",
        "open",
        "2025-02-07T23:39:50Z",
        null,
        null,
        "module: dynamo, ciflow/inductor, module: compiled autograd",
        "gh/xmfan/177/base",
        "gh/xmfan/177/head",
        1,
        12,
        0,
        1,
        2,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146735"
    ],
    [
        146734,
        "[CUDA][CUDNN][SDPA] Pass dropout seed and offset to cuDNN in `int64`",
        "Workaround for limitation in cuDNN that does not accept dropout seed/offset in `int32` for SM 10.0 kernels.\n\ncc @csarofeen @ptrblck @xwang233 @msaroufim",
        "open",
        "2025-02-07T23:30:33Z",
        null,
        null,
        "module: cudnn, module: cuda, triaged, open source, topic: not user facing, module: sdpa",
        "main",
        "cudnnsm100dropoutworkaround",
        1,
        31,
        12,
        1,
        1,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146734"
    ],
    [
        146733,
        "[CUDA][SDPA] Don't dispatch to mem eff attn for batch_size >= 65536",
        "#146704\n\ncc @ptrblck @msaroufim",
        "open",
        "2025-02-07T23:29:27Z",
        null,
        null,
        "module: cuda, open source, topic: not user facing, module: sdpa",
        "main",
        "memeff65536",
        2,
        23,
        0,
        1,
        2,
        0,
        "drisspg, drisspg, nikitaved",
        "COMMENTED, APPROVED, COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146733"
    ],
    [
        146731,
        "dont specialize symints when testing truthiness",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #133044\n* #146729\n* #146642\n* __->__ #146731\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-02-07T22:48:49Z",
        null,
        null,
        "module: dynamo, ciflow/inductor",
        "gh/bdhirsh/641/base",
        "gh/bdhirsh/641/head",
        2,
        20,
        1,
        1,
        3,
        1,
        "bobrenjc93",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146731"
    ],
    [
        146730,
        "[BaseHOP] change hop(subgraph, operands) to hop(subgraph, *operands)",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146730\n* #146727\n\nOur three main users are OK with this, with two of them (foreach_map,\ninvoke_quant) prefering it like this.\n\nI was originally worried about BC issues (this now means you cannot add\nany positional args) but I think that's not a concern -- one can always\nadd kwonly args.\n\nTest Plan\n- tests\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov",
        "open",
        "2025-02-07T22:42:43Z",
        null,
        null,
        "release notes: foreach_frontend, module: inductor, module: dynamo, ciflow/inductor",
        "gh/zou3519/1130/base",
        "gh/zou3519/1130/head",
        6,
        35,
        48,
        1,
        1,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146730"
    ],
    [
        146729,
        "support meta_tensor.to(device='cpu') under fake_mode",
        "Fixing this is actually a bit annoying:\r\n\r\n(1) FakeTensorMode sees a function where all of its inputs are real tensors, so it tries to run the real compute before converting the output to a FakeTensor\r\n\r\n(2) we don't actually want this, because the \"real compute\" is support to error normally, when you do `meta_tensor.to(device='cpu')`. Instead, we want FakeTensor to actually skip constant prop and run the normal FakeTensor implementation, which will not error\r\n\r\nStack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #133044\n* __->__ #146729\n* #146642\n* #146731\n\r\n",
        "open",
        "2025-02-07T22:19:32Z",
        null,
        null,
        "ciflow/inductor",
        "gh/bdhirsh/640/base",
        "gh/bdhirsh/640/head",
        2,
        12,
        0,
        2,
        2,
        0,
        "zou3519, zou3519, bdhirsh",
        "COMMENTED, APPROVED, COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146729"
    ],
    [
        146728,
        "[StaticRuntime] Fix a bug that memory planner ignores subblocks",
        "Summary: When Static Runtime graph node has sub-blocks, the memory planner does not consider sub-blocks' inputs as a node's input in memory planner. As the result, such nodes' inputs' lifetime is incorrect and corresponding tensor memory is released earlier than required and causes errors.\n\nDifferential Revision: D69195886\n\n\n\n\ncc @EikanWang @jgong5 @wenzhe-nrv @sanchitintel",
        "open",
        "2025-02-07T22:13:28Z",
        null,
        null,
        "oncall: jit, fb-exported, release notes: jit",
        "main",
        "export-D69195886",
        3,
        75,
        8,
        1,
        7,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146728"
    ],
    [
        146727,
        "Rename PrimHOPBase to BaseHOP + minor changes",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #146730\n* __->__ #146727\n\nThis PR:\n- renames PrimHOPBase to BaseHOP\n- changes the backward pass to always return a tuple (to match the\n  forward pass).\n\nTest Plan:\n- tests\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-02-07T22:00:22Z",
        null,
        null,
        "release notes: foreach_frontend, module: dynamo, ciflow/inductor",
        "gh/zou3519/1129/base",
        "gh/zou3519/1129/head",
        6,
        18,
        18,
        1,
        1,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146727"
    ],
    [
        146726,
        "[ez][BE] get rid of the extra printf('\\n')",
        "Summary: as title\n\nTest Plan:\n```\nAOT_INDUCTOR_DEBUG_INTERMEDIATE_VALUE_PRINTER=3  TORCHINDUCTOR_FORCE_DISABLE_CACHES=1 TORCHINDUCTOR_ABI_COMPATIBLE=1 TORCH_COMPILE_DEBUG=1 TORCH_LOGS=\"+graph, inductor, +schedule, output_code\" buck2 run -c fbcode.enable_gpu_sections=true -c fbcode.nvcc_arch=h100a @//mode/opt fbcode//caffe2/test/inductor:test_aot_inductor -- -r test_addmm_cuda\n```\n\nDifferential Revision: D69328701\n\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov",
        "open",
        "2025-02-07T21:57:49Z",
        null,
        null,
        "fb-exported, ciflow/trunk, topic: not user facing, module: inductor, ciflow/inductor, merging",
        "main",
        "export-D69328701",
        1,
        1,
        2,
        1,
        5,
        0,
        "ColinPeppler",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146726"
    ],
    [
        146723,
        "torch: Log a unified waitcounter for torch.compile and triton.autotune",
        "Summary: Add a second more generic waitcounter to torch.compile. We'll keep expanding this as new generic pytorch compilation sites show up.\n\nTest Plan: Waitcounter only change, relying on existing tests.\n\nDifferential Revision: D69215401\n\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov",
        "open",
        "2025-02-07T20:59:32Z",
        null,
        null,
        "fb-exported, ciflow/trunk, topic: not user facing, module: inductor, ciflow/inductor",
        "main",
        "export-D69215401",
        1,
        3,
        1,
        1,
        3,
        0,
        "davidberard98",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146723"
    ],
    [
        146722,
        "Test on in-graph constructed NJTs",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146722\n* #146721\n\r\nA recent set of bugs has been cropping up related to NJTs that constructed in-graph within a compiled function. This exercises different paths related to symbolic nested ints, etc. Some examples:\r\n* #145874\r\n* #146644\r\n\r\nTo get ahead of these, we should do NJT testing for this case as well.\r\n\r\nThis PR parametrizes the OpInfo tests for compile + forward to cover both in-graph constructed NJT and normal input cases. TBD what fails..\r\n\r\nTODO:\r\n* Do this for compile + backward tests also (?)",
        "open",
        "2025-02-07T20:09:15Z",
        null,
        null,
        "topic: not user facing",
        "gh/jbschlosser/229/base",
        "gh/jbschlosser/229/head",
        2,
        44,
        6,
        2,
        1,
        0,
        "jbschlosser, cpuhrsch, soulitzer",
        "COMMENTED, APPROVED, APPROVED",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146722"
    ],
    [
        146721,
        "Use inductor backend for NJT compile tests",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #146722\n* __->__ #146721\n\r\nWe've been using `backend=\"aot_eager_decomp_partition\"` for NJT compile testing, but this can let inductor bugs slip through. This PR switches the compile tests to use `backend=\"inductor\"`; let's see if test runtime is an issue after this.",
        "open",
        "2025-02-07T20:09:11Z",
        null,
        null,
        "topic: not user facing",
        "gh/jbschlosser/228/base",
        "gh/jbschlosser/228/head",
        1,
        3,
        7,
        2,
        1,
        0,
        "soulitzer",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146721"
    ],
    [
        146720,
        "[ca] remove private API: _compiled_autograd_should_lift",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #146735\n* __->__ #146720\n\r\nSince the functional autograd + compiled autograd migration, we don't trace into nodes anymore, and everything is lifted. We can't support this flag which tries to inline make_fx style in CA initial pass. There's no more usage internally.",
        "open",
        "2025-02-07T20:06:08Z",
        null,
        null,
        "ciflow/trunk, ciflow/inductor, release notes: dynamo",
        "gh/xmfan/176/base",
        "gh/xmfan/176/head",
        4,
        0,
        15,
        1,
        7,
        0,
        "zou3519",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146720"
    ],
    [
        146718,
        "[CUDAGraph] add skip message for unbacked symint",
        "Add explicit skip message for unbacked symint in cudagraph, as suggested by @bdhirsh.\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov",
        "open",
        "2025-02-07T19:29:37Z",
        null,
        null,
        "ciflow/trunk, topic: not user facing, module: inductor, module: dynamo, ciflow/inductor",
        "main",
        "bf/cg-skip-unbacked-symint-msg",
        4,
        31,
        14,
        1,
        1,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146718"
    ],
    [
        146717,
        "[BE][cuDNN] cuDNN to 9.7.1.26 for CUDA 12.8",
        "cuDNN 9.7.1 is out now and is expected to be the longer-lived branch with more potential backports vs. 9.7.0\r\n\r\nCC @nWEIdia @tinglvv \n\ncc @malfet @seemethere @csarofeen @ptrblck @xwang233",
        "open",
        "2025-02-07T19:25:00Z",
        null,
        null,
        "module: build, module: cudnn, triaged, open source, topic: not user facing, topic: build",
        "main",
        "cudnn971",
        6,
        14,
        12,
        1,
        1,
        0,
        "drisspg",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146717"
    ],
    [
        146716,
        "[BE] Remove outdated RPC benchmark",
        "We have lots of outdated unused + uncalled code in our codebase, namely in our benchmarks and examples folders among others. The last change to this directory was 4 years ago and this code looks dead. cc @albanD @H-Huang for feedback\r\n\r\nStack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\r\n* __->__ #146716\r\n\r\n",
        "open",
        "2025-02-07T18:52:53Z",
        null,
        null,
        "release notes: distributed (rpc), skip-pr-sanity-checks",
        "gh/janeyx99/223/base",
        "gh/janeyx99/223/head",
        29,
        0,
        2535,
        1,
        1,
        0,
        "Skylion007, H-Huang",
        "APPROVED, APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146716"
    ],
    [
        146715,
        "[export][ez] Allow math.trunc for serialization.",
        "Summary: as title.\n\nTest Plan: CI\n\nDifferential Revision: D69317084\n\n\n",
        "open",
        "2025-02-07T18:24:55Z",
        null,
        null,
        "fb-exported, ciflow/trunk, ciflow/inductor, release notes: export",
        "main",
        "export-D69317084",
        1,
        1,
        0,
        1,
        2,
        0,
        "angelayi",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146715"
    ],
    [
        146714,
        "[hop] Support more output types for `flat_apply`",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #146367\n* __->__ #146714\n* #146713\n\nThis patch enables `flat_apply` to support certain non-Tensor output\ntypes like containers and graphable types. This will in turn enable the\nupcoming `mark_traceable` to support more output types.\n\nThe patch also exposes a `func_to_graphable` rather than having the\nusers calling the lower level `pytree.flatten(ConstantFunction(...))`.\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-02-07T17:43:34Z",
        null,
        null,
        "module: dynamo, ciflow/inductor",
        "gh/StrongerXi/83/base",
        "gh/StrongerXi/83/head",
        2,
        54,
        21,
        1,
        2,
        0,
        "zou3519",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146714"
    ],
    [
        146713,
        "[dynamo][fx] Support dataclass whose fields have `init=False`",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* (to be filled)\n\nPreviously Dynamo and FX have code paths that reconstruct a dataclass\ninstance based on its type and fields; however they weren't taking\n`init=False` into account (which is supposed to exclude the field from\nconstructor).\n\nThis patch fixes that, and also updates `pytree.LeafSpec` so that its\n`__init__` conforms with the `init` attribute of its fields. Without\nthis change, the aforementioned reconstruction logic would fail.\n\ncc @ezyang @SherlockNoMad @EikanWang @jgong5 @wenzhe-nrv @voznesenskym @penguinwu @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-02-07T17:43:29Z",
        null,
        null,
        "release notes: fx, fx, module: dynamo, ciflow/inductor",
        "gh/StrongerXi/82/base",
        "gh/StrongerXi/82/head",
        4,
        61,
        9,
        1,
        1,
        0,
        "Skylion007",
        "COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146713"
    ],
    [
        146710,
        "[MTIA] (3/n) Implement PyTorch APIs to query/reset device peak memory usage",
        "Summary: Public summary (shared with Github): This diff implements a C++-Python binding to enable `reset_peak_memory_stats`.\n\nTest Plan: The test is implemented in the following diff.\n\nReviewed By: yuhc\n\nDifferential Revision: D68988673\n\n\n",
        "open",
        "2025-02-07T16:52:00Z",
        null,
        null,
        "fb-exported, ciflow/trunk, topic: not user facing, merging",
        "main",
        "export-D68988673",
        3,
        9,
        1,
        1,
        6,
        0,
        "nautsimon",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146710"
    ],
    [
        146709,
        "FSDP: avoid resetting version counter of all_gather_output in inference_mode",
        "Summary:\nFSDP needs to hide VC bumps on its allgather buffer, but it does not need to do this is the allgather buffer was generated under inference mode.\n\nmore details here: https://www.internalfb.com/diff/D69115649?dst_version_fbid=1316814572779281&transaction_fbid=849120230625711\n\nTest Plan: CI\n\nDifferential Revision: D69311496\n\n\n\n\ncc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o",
        "open",
        "2025-02-07T16:39:54Z",
        null,
        null,
        "oncall: distributed, fb-exported, ciflow/trunk, release notes: distributed (fsdp), ciflow/inductor, merging",
        "main",
        "export-D69311496",
        1,
        9,
        1,
        1,
        6,
        0,
        "awgu",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146709"
    ],
    [
        146706,
        "cpp_wrapper: persist autotune example tensors until last use",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\r\n* #146452\r\n* __->__ #146706\r\n* #146424\r\n* #146109\r\n* #146449\r\n* #144349\r\n* #144293\r\n* #144002\r\n\r\nPatches over an issue where randomly generated example tensors can cause kernel autotuning to fail, when those tensors would not be possible outputs from previous kernels in the sequence. This fixes a failure in `test_torchinductor_opinfo.py` when run with compile-time autotuning, `test_comprehensive_nanquantile_cuda_float64`.\r\n\r\nFor clarity, the situation triggering this PR looks like kernels `A -> BCDE -> F` (`BCDE` is fused), where one of the outputs from `A` is a boolean tensor describing some of the input data. Previously, we randomly regenerated that boolean tensor and the input data before passing them to `BCDE`, so that they no longer matched. This caused a `tl.device_assert` call in `BCDE` to fail. With this PR, we reuse the random data input to `A` and the output Boolean tensor, such that they match and pass the device assertion in `BCDE`.\r\n\r\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @desertfire @chauhang @aakhundov",
        "open",
        "2025-02-07T16:08:21Z",
        null,
        null,
        "open source, topic: not user facing, module: inductor, ciflow/inductor",
        "gh/benjaminglass1/67/base",
        "gh/benjaminglass1/67/head",
        1,
        28,
        8,
        2,
        2,
        0,
        "benjaminglass1",
        "COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146706"
    ],
    [
        146705,
        "Remove NO_MULTIPROCESSING_SPAWN checks",
        "py 3.9 has spawn.\r\n\r\ncc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o",
        "open",
        "2025-02-07T15:19:32Z",
        null,
        null,
        "oncall: distributed, triaged, open source, release notes: distributed (torchelastic)",
        "main",
        "NO_MULTIPROCESSING_SPAWN",
        11,
        23,
        156,
        1,
        1,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146705"
    ],
    [
        146696,
        "Add full_like.default to list of ops with kwargs",
        "The _maybe_insert_input_observers_for_node function expects ops, except a few exceptions, to have zero kwargs. full_like.default seems to be one of these cases and should therefore be added to the list.\r\n\r\nAddresses https://github.com/pytorch/pytorch/issues/146621\r\n\r\nFixes #146621 \r\n",
        "open",
        "2025-02-07T11:34:31Z",
        null,
        null,
        "triaged, open source, release notes: quantization, release notes: AO frontend",
        "main",
        "main",
        2,
        3,
        2,
        1,
        2,
        1,
        "Xia-Weiwen",
        "COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146696"
    ],
    [
        146695,
        "Enable Windows tests",
        "Fixes #ISSUE_NUMBER\r\n\n\ncc @peterjc123 @mszhanyi @skyline75489 @nbcsm @iremyux @Blackhex",
        "open",
        "2025-02-07T11:34:26Z",
        null,
        null,
        "module: windows, triaged, open source, topic: not user facing",
        "main",
        "win_test294",
        3,
        1,
        23,
        2,
        1,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146695"
    ],
    [
        146692,
        "[clang-tidy] Add suppression clang-diagnostic-shadow",
        "Summary:\r\nReviewed By: varun2784\r\n\r\nDifferential Revision: D69182465\r\n\r\n\r\n",
        "open",
        "2025-02-07T10:06:04Z",
        null,
        null,
        "fb-exported, topic: not user facing",
        "main",
        "export-D69182465",
        1,
        2,
        1,
        1,
        8,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146692"
    ],
    [
        146690,
        "Enable pt2e quantization path for arm",
        "**Title**: Enable PyTorch 2 Export Quantization path for ARM CPUs.\r\n\r\n**Description:**\r\n - This PR extends the PyTorch 2 Export Quantization (PT2E Quantization) workflow\u2014originally available only on x86 CPUs\u2014to support ARM platforms. PT2E Quantization is an automated, full-graph quantization solution in PyTorch that improves on Eager Mode Quantization by adding support for functionals and automating the overall process. It is part of the torch.ao module and fully supports quantization when using the compile mode.\r\n\r\n**Key Changes:**\r\n\r\n - Introduces ARM-specific support by leveraging oneDNN kernels for matmuls and convolution.\r\n\r\n - Integrates pre-defined configuration selection to automatically choose the best quantization settings based on the selected quantization method.\r\n\r\n**Provides customization options via two flags:**\r\n\r\n - **qat_state:** Indicates whether to use Quantization Aware Training (if set to True) or Post Training Quantization (if set to False). The default remains False.\r\n - **dynamic_state:** Selects between dynamic quantization (if True) and static quantization (if False). The default is also set to False.\r\n![Screenshot 2025-01-22 105543](https://github.com/user-attachments/assets/c611a1ce-9274-4b70-9c58-cae96000d06d)\r\n\r\nThese options allow users to tailor the quantization process for their specific workload requirements (e.g., using QAT for fine-tuning or PTQ for calibration-based quantization).\r\n\r\nTesting and Validation:\r\n\r\nThe new ARM flow has been thoroughly tested across a range of models with all combinations:\r\n**NLP**: Models such as BERT and T5.\r\n**Vision**: Models like ResNet and ViT.\r\n**Custom Models**: user defined models with various operators.\r\n\r\nexample script:\r\n```\r\nimport torch\r\nimport torchvision.models as models\r\nfrom torch.ao.quantization.quantize_pt2e import prepare_pt2e, convert_pt2e\r\nimport torch.ao.quantization.quantizer.arm_inductor_quantizer as armiq\r\nfrom torch.ao.quantization.quantizer.arm_inductor_quantizer import ArmInductorQuantizer\r\nfrom torch.profiler import profile, record_function, ProfilerActivity\r\n\r\nmodel_name = \"resnet50\"\r\nmodel = models.__dict__[model_name](pretrained=True)\r\n\r\n# Set the model to eval mode\r\nmodel = model.eval()\r\n\r\n# Create the data, using the dummy data here as an example\r\ntraced_bs = 500\r\nx = torch.randn(traced_bs, 3, 224, 224).contiguous(memory_format=torch.channels_last)\r\nexample_inputs = (x,)\r\n\r\nwith torch.no_grad():\r\n    exported_model = torch.export.export_for_training(model, example_inputs).module()\r\n    quantizer = armiq.ArmInductorQuantizer()\r\n    quantizer.set_global(armiq.get_default_arm_inductor_quantization_config(is_dynamic=False))\r\n    prepared_model = prepare_pt2e(exported_model, quantizer)\r\n    converted_model = convert_pt2e(prepared_model)\r\n\r\n    with torch.set_grad_enabled(False):\r\n        for _ in range(50):\r\n            converted_model(*example_inputs) #Warmup\r\n        print(\"Warmup over\")\r\n        with profile(activities=[ProfilerActivity.CPU], record_shapes=True) as prof:\r\n            with record_function(\"model_inference\"):\r\n                for _ in range(100):\r\n                    converted_model(*example_inputs)\r\n\r\n    print(prof.key_averages(group_by_input_shape=True).table(sort_by=\"self_cpu_time_total\"))\r\n\r\n```\n\ncc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @malfet @snadampal @milpuz01",
        "open",
        "2025-02-07T09:23:48Z",
        null,
        null,
        "module: cpu, triaged, open source, module: arm, release notes: quantization, release notes: AO frontend",
        "main",
        "devang/pt2e_quantization_arm",
        2,
        1592,
        0,
        1,
        7,
        1,
        "jerryzh168",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146690"
    ],
    [
        146689,
        "Update addbmm, addmm, addmv and baddbmm description",
        "Fixes #146611, following #146482\r\n\r\n## Test Result\r\n\r\n![image](https://github.com/user-attachments/assets/5c1749be-1f10-4e80-a284-b1929ca340eb)\r\n",
        "open",
        "2025-02-07T08:57:51Z",
        null,
        null,
        "triaged, open source, release notes: python_frontend",
        "main",
        "opt/docs/add",
        1,
        4,
        4,
        1,
        7,
        0,
        "mikaylagawarecki",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146689"
    ],
    [
        146687,
        "Make GetCPUAllocatorMaybePinned to be Device-Agnostic",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146687\n\n----\n\n- Keep cuda first to perserve BC\n- Remove cuda first if it is possible to have only one accelerator at a time in the future",
        "open",
        "2025-02-07T08:54:31Z",
        null,
        null,
        "open source, topic: not user facing",
        "gh/fffrog/38/base",
        "gh/fffrog/38/head",
        2,
        18,
        15,
        1,
        2,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146687"
    ],
    [
        146684,
        "Optimize LRScheduler docs",
        "Fixes #120735\r\n\r\nAdd more description about [`LRScheduler`](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.LRScheduler.html#torch.optim.lr_scheduler.LRScheduler)\r\n\r\n## Test Result\r\n\r\n### Before\r\n\r\n![image](https://github.com/user-attachments/assets/619c3ea8-5652-4e61-936f-0cb5aa5a326b)\r\n\r\n### After\r\n\r\n![image](https://github.com/user-attachments/assets/174a6ffc-5da2-4837-bf49-2f09f6c7b6ee)\r\n\r\n![image](https://github.com/user-attachments/assets/ae1bc984-49cc-4d5b-8d81-08f460b71361)\r\n\r\ncc @janeyx99\r\n",
        "open",
        "2025-02-07T08:42:57Z",
        null,
        null,
        "triaged, open source, release notes: optim",
        "main",
        "opt/docs/LRScheduler",
        1,
        21,
        2,
        1,
        1,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146684"
    ],
    [
        146678,
        "[dynamo][not ready] polyfill infra for classes",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146678\n* #146737\n* #146677\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-02-07T06:41:35Z",
        null,
        null,
        "module: dynamo, ciflow/inductor",
        "gh/anijain2305/675/base",
        "gh/anijain2305/675/head",
        7,
        247,
        41,
        3,
        2,
        0,
        "XuehaiPan",
        "COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146678"
    ],
    [
        146677,
        "[dynamo][user-defined] User class.__new__ instead of special casing",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #146678\n* #146737\n* __->__ #146677\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-02-07T06:41:32Z",
        null,
        null,
        "ciflow/trunk, topic: not user facing, module: dynamo, ciflow/inductor, keep-going",
        "gh/anijain2305/674/base",
        "gh/anijain2305/674/head",
        6,
        168,
        104,
        3,
        1,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146677"
    ],
    [
        146675,
        "[ROCm] Move ROCm unstable MI300 jobs back to stable",
        "Fixes #145790\r\n\r\nThis PR moves rocm unstable MI300 back to stable. The change to unstable was introduced through this [PR](https://github.com/pytorch/pytorch/pull/145790). This was because the MI300s were failing with a [docker daemon](https://github.com/pytorch/pytorch/actions/runs/13015957622/job/36306779536) issue which has been resolved.\r\n\r\ncc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd @ZainRizvi \r\n\r\ncc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd",
        "open",
        "2025-02-07T05:22:51Z",
        null,
        null,
        "module: rocm, open source, topic: not user facing, ciflow/inductor, ciflow/rocm, ciflow/inductor-rocm",
        "main",
        "patch-8",
        3,
        80,
        70,
        8,
        1,
        0,
        "jithunnair-amd, jithunnair-amd",
        "COMMENTED, COMMENTED",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146675"
    ],
    [
        146671,
        "Update torch-xpu-ops commit pin",
        "Update the torch-xpu-ops commit to [662837e722cbbc0701fbcf6ea9ad6383158cc44e](https://github.com/intel/torch-xpu-ops/commit/662837e722cbbc0701fbcf6ea9ad6383158cc44e), includes:\r\n\r\n- Aten operator coverage improvement\r\n- SYCL kernel optimization\r\n- Nested Tensor OPs support\r\n",
        "open",
        "2025-02-07T03:28:45Z",
        null,
        null,
        "triaged, open source, topic: not user facing, keep-going, ciflow/xpu",
        "main",
        "xyt/xpu_pin_662837e722cbbc0701fbcf6ea9ad6383158cc44e",
        1,
        1,
        1,
        1,
        2,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146671"
    ],
    [
        146669,
        "Optimize inductor `Self` typing",
        "Replace method return type with `Self` typing\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @desertfire @chauhang @aakhundov",
        "open",
        "2025-02-07T01:59:49Z",
        null,
        null,
        "triaged, open source, topic: not user facing, module: inductor",
        "main",
        "opt/inductor/typing",
        3,
        8,
        5,
        1,
        3,
        0,
        "jansel",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146669"
    ],
    [
        146668,
        "Make sure cutlass kernel .cu file has configuration name and nvcc compile command",
        "I think its good to have everything in the .cu file. Especially the nvcc compile command.\r\n\r\nTechnically, the configuration name can be found in the template already. So let me know if you think its not needed. \r\n\r\nDifferential Revision: D69281295\r\n\r\n\r\n\r\n\r\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @desertfire @chauhang @aakhundov",
        "open",
        "2025-02-07T01:54:45Z",
        null,
        null,
        "fb-exported, ciflow/trunk, topic: not user facing, module: inductor, ciflow/inductor, merging",
        "main",
        "export-D69281295",
        2,
        6,
        0,
        1,
        8,
        0,
        "chenyang78",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146668"
    ],
    [
        146664,
        "[Docs] Fix description of `input` in `torch.addbmm()`",
        "Fixes #146613\r\n",
        "open",
        "2025-02-07T01:27:52Z",
        null,
        null,
        "open source, ciflow/trunk, release notes: python_frontend, topic: docs, merging",
        "main",
        "docs/addbmm",
        1,
        1,
        1,
        1,
        10,
        1,
        "mikaylagawarecki",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146664"
    ],
    [
        146662,
        "[Optimus] Bug fix in the select cat aten pass",
        "Summary: Thanks to Shuai for reporting the bug in the pattern. We found there's a typo in the pass, where we should make sure all the selects will go to the cat node.\n\nTest Plan:\nbuck2 test 'fbcode//mode/dev-nosan' fbcode//caffe2/test/inductor:split_cat_fx_aten_passes -- test_select_cat_post_grad\n\n\nBuck UI: https://www.internalfb.com/buck2/2cd0888e-d803-43a8-8530-d97e6bc281b3\nTest UI: https://www.internalfb.com/intern/testinfra/testrun/6192449699305108\nNetwork: Up: 110KiB  Down: 35KiB  (reSessionID-687be0fa-031a-47a0-8780-5ab4cf4bbd94)\nExecuting actions. Remaining     0/4                                                                              6.6s exec time total\nCommand: test.     Finished 2 local\nTime elapsed: 2:12.0s\nTests finished: Pass 2. Fail 0. Fatal 0. Skip 0. Build failure 0\n\nDifferential Revision: D69278487\n\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @desertfire @chauhang @aakhundov",
        "open",
        "2025-02-07T00:50:59Z",
        null,
        null,
        "fb-exported, module: inductor, ciflow/inductor, release notes: inductor, inductor_pattern_match",
        "main",
        "export-D69278487",
        2,
        53,
        35,
        1,
        4,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146662"
    ],
    [
        146661,
        "[cond] Refactor cond_op's signature to take *operands.",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146661\n* #146660\n\r\n\r\nThis is a BC-breaking change for hop's IR schema. Previously, \r\n```python\r\ntorch.cond(pred, true_fn, false_fn, (a, b))\r\n# Old representation\r\ntorch.ops.higher_order.cond(pred, true_gm, false_gm, (a, b))\r\n# New representation:\r\ntorch.ops.higher_order.cond(pred, true_gm, false_gm, a, b)\r\n```\r\nThe benefits of this change is that it's much easier to construct the schema since the tuple is flattened. What's particularly troublesome about previous node is that it's hard to represent the mutation and alias information inside the tuple: we have to change the legacy schema parser and verify (maybe re-purpose) the aliasInfo to supports nested aliasInfo inside tuple/list.\r\n\r\nWe'll also refactor other control flow operators.\r\n\r\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @desertfire @chauhang @aakhundov\r\n\r\nDifferential Revision: [D69279033](https://our.internmc.facebook.com/intern/diff/D69279033)",
        "open",
        "2025-02-07T00:44:09Z",
        null,
        null,
        "ciflow/trunk, topic: not user facing, module: inductor, module: dynamo, ciflow/inductor, keep-going, release notes: export",
        "gh/ydwu4/208/base",
        "gh/ydwu4/208/head",
        13,
        119,
        181,
        3,
        3,
        0,
        "zou3519",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146661"
    ],
    [
        146660,
        "[hop][inductor] don't promote arg type for cond and while_loop",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #146661\n* __->__ #146660\n\r\nHop subgraph codegen assumes arguments's type are not promoted. Otherwise, we might generate wrong kernel.\r\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @desertfire @chauhang @aakhundov\n\nDifferential Revision: [D69279031](https://our.internmc.facebook.com/intern/diff/D69279031)",
        "open",
        "2025-02-07T00:44:03Z",
        null,
        null,
        "ciflow/trunk, topic: not user facing, module: inductor, ciflow/inductor, merging",
        "gh/ydwu4/207/base",
        "gh/ydwu4/207/head",
        1,
        2,
        2,
        2,
        6,
        1,
        "zou3519, zou3519, ydwu4",
        "COMMENTED, APPROVED, COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146660"
    ],
    [
        146658,
        "[HOP] Mutation and alias rework",
        "This PR reworks the way the input mutations and various aliases are checked\r\n\r\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @ydwu4 \r\n",
        "open",
        "2025-02-07T00:28:18Z",
        null,
        null,
        "open source, topic: not user facing, module: dynamo",
        "main",
        "mutation_alias_rework",
        12,
        116,
        100,
        5,
        2,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146658"
    ],
    [
        146656,
        "Optimize isclose() for CPU and GPU by adding specific implementations",
        "`isclose()` is currently quite slow, so this PR adds specific implementations for both CPU and cuda.\r\n\r\nCUDA implementation seeing ~4.9x improvement at 100m elements and ~18.7x improvement at 400m elements\r\n\r\nCPU implementation seeing ~5.7x improvements at 100m elements and ~5.9x improvements at 400m elements \r\n\r\nsimple benchmark used(adapt with CPU as needed):\r\n```python\r\nimport time\r\nimport numpy as np\r\nimport torch\r\n\r\ndef benchmark_isclose(shape1, shape2, num_runs=5):\r\n    tensor1 = torch.randn(shape1, device=\"cuda\")\r\n    tensor2 = tensor1.clone()\r\n    tensor2 += torch.randn_like(tensor2) * 0.001\r\n\r\n    # warn up\r\n    _ = torch.isclose(tensor1, tensor2)\r\n    torch.cuda.synchronize()\r\n\r\n    times = []\r\n    for _ in range(num_runs):\r\n        start_time = time.perf_counter()\r\n\r\n        _ = torch.isclose(tensor1, tensor2)\r\n        torch.cuda.synchronize()\r\n        end_time = time.perf_counter()\r\n        times.append(end_time - start_time)\r\n\r\n    mean_time = np.mean(times)\r\n    std_time = np.std(times)\r\n\r\n    return mean_time, std_time\r\n\r\n\r\ntest_shapes = [\r\n    (10000, 10000),  # 100M elements\r\n    (20000, 20000),  # 400M elements\r\n]\r\n\r\nprint(\"\\nBenchmarking torch.isclose():\")\r\nprint(\"-\" * 50)\r\n\r\nfor shape in test_shapes:\r\n    total_elements = np.prod(shape)\r\n    print(f\"\\nTensor shape: {shape} ({total_elements:,} elements)\")\r\n\r\n    mean_time, std_time = benchmark_isclose(shape, shape)\r\n    print(f\"Mean time: {mean_time*1000:.2f} ms +/- {std_time*1000:.2f} ms\")\r\n    print(f\"Elements per second: {total_elements/mean_time:,.0f}\")\r\n```\r\n\r\n```\r\n(optimized)\r\nTensor shape: (10000, 10000) (100,000,000 elements)\r\nMean time: 2.73 ms \u00b1 0.26 ms\r\nElements per second: 36,611,905,024\r\n\r\nTensor shape: (20000, 20000) (400,000,000 elements)\r\nMean time: 8.98 ms \u00b1 0.28 ms\r\nElements per second: 44,546,604,660\r\n\r\n(unoptimized)\r\nTensor shape: (10000, 10000) (100,000,000 elements)\r\nMean time: 13.48 ms \u00b1 0.28 ms\r\nElements per second: 7,420,814,236\r\n\r\nTensor shape: (20000, 20000) (400,000,000 elements)\r\nMean time: 166.90 ms \u00b1 4.71 ms\r\nElements per second: 2,396,711,992\r\n```\r\ncc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @albanD \r\n\r\n",
        "open",
        "2025-02-07T00:09:02Z",
        null,
        null,
        "module: cpu, triaged, open source",
        "main",
        "feature/isclose-kernels",
        4,
        171,
        57,
        3,
        2,
        1,
        "albanD",
        "COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146656"
    ],
    [
        146655,
        "torch._scaled_mm with MX dtypes",
        "making https://github.com/pytorch/pytorch/pull/145562 work with in-core dtypes\r\n\r\nnot ready for review yet",
        "open",
        "2025-02-07T00:02:24Z",
        null,
        null,
        "ciflow/inductor",
        "gh/vkuzo/2/head",
        "gh/vkuzo/3/head",
        8,
        291,
        79,
        3,
        2,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146655"
    ],
    [
        146654,
        "[DCP] Introduce modules metadata in the storage_meta",
        "Summary: Introduce the list of modules in the storage_meta which is shared between the planner and the storage writer. We will use it to let the storage writer know about the modules in the state dict and create module directories in the checkpoint.\n\nTest Plan: UTs\n\nDifferential Revision: D69154628\n\n\n\n\ncc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o @LucasLLC @MeetVadakkanchery @mhorowitz @pradeepfn @ekr0",
        "open",
        "2025-02-06T23:43:44Z",
        null,
        null,
        "oncall: distributed, fb-exported, module: distributed_checkpoint",
        "main",
        "export-D69154628",
        1,
        2,
        1,
        1,
        3,
        0,
        "mhorowitz",
        "COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146654"
    ],
    [
        146653,
        "windows Magma build for cu128",
        "https://github.com/pytorch/pytorch/issues/145570\r\n\r\nremoving `.ci/pytorch/windows/internal/cuda_install.bat` as it is a duplicate with` .github/scripts/windows/cuda_install.bat`. The later one is the one in use - https://github.com/pytorch/pytorch/pull/146653/files#diff-613791f266f2f7b81148ca8f447b0cd6c6544f824f5f46a78a2794006c78957bR8\r\n\r\ncc @atalman @ptrblck @nWEIdia ",
        "open",
        "2025-02-06T23:33:34Z",
        null,
        null,
        "open source, Merged, Reverted, release notes: releng, ciflow/binaries_wheel, ci-no-td",
        "main",
        "cu128-win-magma",
        5,
        95,
        223,
        4,
        6,
        1,
        "atalman",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146653"
    ],
    [
        146642,
        "[poc] force UntypedStorage.from_buffer(buf) to return meta storage under FakeTensorMode",
        "context here: https://fb.workplace.com/groups/326136610199609/permalink/495389539940981/\r\n\r\nThis PR is an attempt to make it such that if you create a tensor from an external buffer (using `UntypedStorage.from_buffer(buf)`, we can generate a proper fake tensor for you out of the box.\r\n\r\nThe annoying bit is that there are not any dispatcher ops to interpose on and change behavior. So instead, I took the manual C binding and tweaked the storage device to be \"meta' if we see an active fake mode.\r\n\r\nPut \"poc\" in the title since I... think this is hopefully reasonable, but I can be convinced that it's not :)\r\n\r\n```\r\nfrom torch._subclasses.fake_tensor import FakeTensorMode\r\nimport pickle\r\nimport io\r\nimport torch\r\nfrom contextlib import nullcontext\r\n\r\n\r\nuse_fake_tensor = True\r\nwith FakeTensorMode() if use_fake_tensor else nullcontext():\r\n    obj = [1, 2]\r\n    f = io.BytesIO()\r\n    pickle.Pickler(f).dump(obj)\r\n    byte_storage = torch.ByteStorage._from_buffer(f.getvalue())  # type: ignore[attr-defined]\r\n    \r\n    t = torch.ByteTensor(byte_storage)\r\n```\r\n\r\nStack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #133044\n* #146729\n* __->__ #146642\n* #146731\n\r\n",
        "open",
        "2025-02-06T21:50:27Z",
        null,
        null,
        "release notes: composability",
        "gh/bdhirsh/639/base",
        "gh/bdhirsh/639/head",
        2,
        27,
        8,
        4,
        2,
        1,
        "zou3519",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146642"
    ],
    [
        146641,
        "[dim order]  solve broken doc",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146641\n\nDifferential Revision: [D69265340](https://our.internmc.facebook.com/intern/diff/D69265340/)",
        "open",
        "2025-02-06T21:49:03Z",
        null,
        null,
        "fb-exported, ciflow/trunk, topic: not user facing, merging",
        "gh/gasoonjia/2/base",
        "gh/gasoonjia/2/head",
        1,
        3,
        0,
        3,
        8,
        0,
        "svekars, svekars, Jack-Khuu",
        "COMMENTED, APPROVED, APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146641"
    ],
    [
        146640,
        "POC for mixed prec optim frontend",
        "This PR is a prototype for what a frontend for asking for mixed precision can look like torch.optim through set_dtype_policy in optimizer.py.\r\n\r\nThis is not meant to be landable but to start some discussions on what people want/would like to see and to ask if there are things I haven't considered yet.\r\n\r\nThis currently only works with Adam(W)!\r\n\r\nA toy script for how to use:\r\n```\r\nimport torch\r\n\r\nmodel = torch.nn.Sequential(\r\n    torch.nn.Linear(2, 3),\r\n    torch.nn.Sigmoid(),\r\n    torch.nn.Linear(3, 1),\r\n    torch.nn.Sigmoid(),\r\n)\r\nmodel.to(\"cuda\")\r\n\r\noptim = torch.optim.AdamW(model.named_parameters(), foreach=False)\r\nmp_policy = {\r\n    \"exp_avg\": lambda _: torch.bfloat16,\r\n    \"exp_avg_sq\": lambda _: torch.bfloat16,\r\n    \"max_exp_avg_sq\": lambda _: torch.bfloat16,\r\n}\r\noptim.set_dtype_policy(mp_policy)\r\n\r\ni = torch.tensor([0.1, 0.2, 0.3, 0.4, 0.5, 0.6], device=\"cuda\").reshape(3, 2)\r\nl = model(i).sum()\r\nl.backward()\r\n\r\noptim.step()\r\n```\r\n\r\nStack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146640\n\r\n",
        "open",
        "2025-02-06T21:31:50Z",
        null,
        null,
        "release notes: optim",
        "gh/janeyx99/222/base",
        "gh/janeyx99/222/head",
        3,
        113,
        11,
        2,
        1,
        0,
        "janeyx99",
        "COMMENTED",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146640"
    ],
    [
        146638,
        "use None to slice when list has one element only",
        "When autotune_num_choices_displayed is None and the list of choices has length 1, slicing with `[:-1]` means getting all elements except the last one, which resulted in an empty list.\r\n\r\nSlicing with `[:None]` works. \r\n\r\nDifferential Revision: D69265168\r\n\r\n\r\n\r\n\r\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @desertfire @chauhang @aakhundov",
        "open",
        "2025-02-06T21:08:40Z",
        null,
        null,
        "fb-exported, ciflow/trunk, topic: not user facing, module: inductor, ciflow/inductor, merging",
        "main",
        "export-D69265168",
        1,
        2,
        5,
        1,
        8,
        0,
        "drisspg",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146638"
    ],
    [
        146637,
        "gloo: fix building system gloo with CUDA/HIP",
        "Fix incorrect linking of Gloo's libraries when building with system Gloo. Previously, either Gloo's native library or Gloo's CUDA library were linked. However, Gloo had changed such that all users of Gloo must link the native library, and can optionally link the CUDA or HIP library for Gloo + CUDA/HIP support.\r\nThis had been updated when building/linking with vendored Gloo, but not when using system Gloo.\r\n\r\nFixes: #146239\r\n\r\nReported-by: Adam J Stewart <ajstewart426@gmail.com>\r\n\r\n\n\ncc @malfet @seemethere @ptrblck @msaroufim @eqy",
        "open",
        "2025-02-06T21:04:17Z",
        null,
        null,
        "module: build, module: cuda, triaged, open source, topic: not user facing",
        "main",
        "gloo_cuda",
        2,
        25,
        25,
        1,
        2,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146637"
    ],
    [
        146636,
        "example repro failure",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146636\n\nSummary:\n\nTest Plan:\n\nReviewers:\n\nSubscribers:\n\nTasks:\n\nTags:\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-02-06T21:00:47Z",
        null,
        null,
        "module: dynamo, ciflow/inductor",
        "gh/c00w/37/base",
        "gh/c00w/37/head",
        2,
        5,
        0,
        1,
        2,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146636"
    ],
    [
        146634,
        "Add Structured Tracing for Traced Graph Edge Details for AC Debugging",
        "Summary:\nUpdating the structured trace infrastructure so that we are able to output to Zoomer and have an E2E solution.\n\nContext Doc: https://docs.google.com/document/d/1T6omIBEWVhbOiwDLSLffgQwjxiT2rQv8QvvQwXkw4fY/edit?usp=sharing\n\nTest Plan:\n### Testing Structured Log + tlparse locally\n\nCommand:\n```\nTORCH_TRACE=/data/users/basilwong/fbsource/fbcode/log_torch_trace buck2 run mode/opt //aps_models/ads/icvr:icvr_launcher -- mode=local_fb_fm_v4 launcher.num_workers=2\n```\n\nTorch Trace Logs (local then sent to paste): P1686419449\n```\ncat log_torch_trace/dedicated_log_torch_trace_rank_0_2lg012xo.log | pastry\nP1686419449\n```\n\ntlparse output: https://manifold.edge.x2p.facebook.net/v0/read/tree/logs/.tmpyiv5wj/rank_1/index.html?bucketName=tlparse_reports&apiKey=tlparse_reports-key&withPayload=1&timeoutMsec=100\n\ntlparse graph edge details output: https://manifold.edge.x2p.facebook.net/v0/read/tree/logs/.tmpyiv5wj/rank_1/9_0_0/joint_graph_information_397.txt?bucketName=tlparse_reports&apiKey=tlparse_reports-key&withPayload=1&timeoutMsec=100\n\nDifferential Revision: D61557220\n\n\n",
        "open",
        "2025-02-06T20:29:48Z",
        null,
        null,
        "fb-exported, topic: not user facing, ciflow/inductor",
        "main",
        "export-D61557220",
        2,
        137,
        38,
        1,
        4,
        1,
        "jansel",
        "CHANGES_REQUESTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146634"
    ],
    [
        146633,
        "[NJT] Fix inference mode for composite implicit ops without nested-specific kernel",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146633\n\n",
        "open",
        "2025-02-06T20:02:09Z",
        null,
        null,
        "ciflow/trunk, topic: bug fixes, release notes: nested tensor, merging",
        "gh/soulitzer/352/base",
        "gh/soulitzer/352/head",
        2,
        27,
        4,
        3,
        10,
        1,
        "jbschlosser",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146633"
    ],
    [
        146632,
        "[ROCm] OCP FP8 Support for new GPUs",
        "TLDR: Follow up/ Build on top of https://github.com/pytorch/pytorch/pull/144476. add OCP FP8 support for gfx950\r\nrefer to https://github.com/pytorch/ao/pull/1677\r\n\r\nThis pull request includes several changes to improve compatibility and support for new GPU architectures and data types, particularly for ROCm. The key updates involve adding support for new ROCm versions and GPU architectures, updating data type handling, and removing outdated checks.\r\n\r\n### Improvements to GPU Architecture and ROCm Version Support:\r\n* [`aten/src/ATen/Context.cpp`](diffhunk://#diff-33de472d304acbe57d693c8567370c638068bedc1aa0ce8e9dc115dad05a7810L323-R326): Added support for new GPU architectures `gfx1200`, `gfx1201`, and `gfx950` based on ROCm version checks.\r\n* [`aten/src/ATen/native/cuda/Blas.cpp`](diffhunk://#diff-e8a569efee1e650172f120a0fdcda024fe3e4703a4ee3336425c8f685af6b3abL196-R199): Updated architecture support in multiple functions to include `gfx1200`, `gfx1201`, and `gfx950` based on ROCm version checks. [[1]](diffhunk://#diff-e8a569efee1e650172f120a0fdcda024fe3e4703a4ee3336425c8f685af6b3abL196-R199) [[2]](diffhunk://#diff-e8a569efee1e650172f120a0fdcda024fe3e4703a4ee3336425c8f685af6b3abL865-R876)\r\n\r\n### Updates to Data Type Handling:\r\n* [`aten/src/ATen/cuda/CUDADataType.h`](diffhunk://#diff-9188bb13b1a49f459141f5f9b875593d1c5ce2beb5ad711fdbaf5bc7089ec015L81-L98): Enhanced data type conversion to include new float8 types for both CUDA and ROCm environments.\r\n* [`aten/src/ATen/cuda/tunable/GemmHipblaslt.h`](diffhunk://#diff-bfa1a3b5d4bef1892bf50338775f3b0fd8cd31fc1868148f3968b98aefb68e3fL29-R80): Updated `HipDataTypeFor` template to handle new float8 types and added hard-coded enum values for ROCm versions prior to 6.3.\r\n\r\n### Removal of Outdated Checks:\r\n* [`cmake/public/LoadHIP.cmake`](diffhunk://#diff-b98e27b9a5f196a6965a99ee5a7bb15b3fc633d6375b767635b1b04ccb2fd3d5L169-L197): Removed the check for `HIP_NEW_TYPE_ENUMS` as it is no longer necessary with the updated ROCm versions. [[1]](diffhunk://#diff-b98e27b9a5f196a6965a99ee5a7bb15b3fc633d6375b767635b1b04ccb2fd3d5L169-L197) [[2]](diffhunk://#diff-b98e27b9a5f196a6965a99ee5a7bb15b3fc633d6375b767635b1b04ccb2fd3d5L211-R182)\r\n\r\nThese changes ensure better compatibility and performance on newer hardware and software environments, particularly for users leveraging ROCm and CUDA for deep learning and scientific computing tasks.\r\n\n\ncc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd",
        "open",
        "2025-02-06T18:57:21Z",
        null,
        null,
        "module: rocm, open source, release notes: linalg_frontend",
        "main",
        "ocp_gfx950",
        11,
        98,
        25,
        8,
        1,
        0,
        "jeffdaily",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146632"
    ],
    [
        146631,
        "Support ignoring parameters in FSDP2",
        "Differential Revision: D69153051\n\n\n\n\ncc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o",
        "open",
        "2025-02-06T18:46:40Z",
        null,
        null,
        "oncall: distributed, fb-exported, release notes: distributed (fsdp), ciflow/inductor",
        "main",
        "export-D69153051",
        3,
        392,
        5,
        1,
        17,
        2,
        "awgu, weifengpy, weifengpy, weifengpy",
        "COMMENTED, COMMENTED, COMMENTED, COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146631"
    ],
    [
        146626,
        " [inductor] Improve type annotations in _inductor/pattern_matcher.py",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146626\n* #146248\n\n\n\ncc @ezyang @SherlockNoMad @EikanWang @jgong5 @wenzhe-nrv @voznesenskym @penguinwu @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @desertfire @chauhang @aakhundov",
        "open",
        "2025-02-06T17:59:04Z",
        null,
        null,
        "open source, release notes: fx, topic: not user facing, fx, module: inductor, ciflow/inductor, suppress-api-compatibility-check, suppress-bc-linter",
        "gh/rec/132/base",
        "gh/rec/132/head",
        2,
        41,
        33,
        6,
        2,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146626"
    ],
    [
        146625,
        "Move capture_provenance to make_node_impl",
        "Previously we were only logging `make_user_impl` implementations, which only gets triggered for operations done on python SymInts, not cpp SymInts. Instead `make_node_impl` will get triggered for both python and cpp SymInt operations.\r\n\r\n\r\ncc @ezyang @SherlockNoMad @EikanWang @jgong5 @wenzhe-nrv",
        "open",
        "2025-02-06T17:53:35Z",
        null,
        null,
        "ciflow/trunk, release notes: fx, fx, ciflow/inductor, merging",
        "main",
        "angelayi/test_expression_created",
        1,
        89,
        90,
        1,
        13,
        2,
        "bobrenjc93, bobrenjc93",
        "APPROVED, APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146625"
    ],
    [
        146623,
        "bug fix: ensure 4d input in _scaled_dot_product_attention_math_mps",
        "This pr addresses the issue in the MPS backend for `_scaled_dot_product_attention_math_mps` where a 3d input like (num_heads, seq_len, query_dim) cannot be automatically treated as (1, num_heads, seq_len, query_dim), which can be inferred on cpu or cuda, which can be circumvented by adding a util function to ensure a 4d shape.\r\n\r\nThe issue was found in https://github.com/hiyouga/LLaMA-Factory/issues/6835, in [transformers qwen2_vl](https://github.com/huggingface/transformers/blob/1590c664306766f32ba68c50e67f14d61b16925d/src/transformers/models/qwen2_vl/modeling_qwen2_vl.py#L373C14-L373C93), 3d q/k/v were passed into sdpa function, which lead to an error.\r\n\r\nConsidering consistency, since this pattern might pop up elsewhere in the transformers codebase, I think it makes more sense to maintain the same intuition across all platforms.\r\n\r\n---\r\nreproduce code:\r\n```\r\nimport torch\r\nimport torch.nn.functional as F\r\n\r\nhead_num, seq_len, embed_dim = 16, 16, 80\r\nbsz = 1\r\n\r\nq = torch.randn(head_num, seq_len, embed_dim)\r\nk = torch.randn(head_num, seq_len, embed_dim)\r\nv = torch.randn(head_num, seq_len, embed_dim)\r\nattention_mask = torch.ones(1, seq_len, seq_len)\r\n\r\noo_cpu = F.scaled_dot_product_attention(\r\n    q.to(\"cpu\"),\r\n    k.to(\"cpu\"),\r\n    v.to(\"cpu\"),\r\n    attention_mask.to(\"cpu\"),\r\n    dropout_p=0.0\r\n)\r\n\r\nif torch.backends.mps.is_available():\r\n    oo_mps = F.scaled_dot_product_attention(\r\n        q.to(\"mps\"),\r\n        k.to(\"mps\"),\r\n        v.to(\"mps\"),\r\n        attention_mask.to(\"mps\"),\r\n        dropout_p=0.0\r\n    )\r\n    assert torch.allclose(oo_cpu, oo_mps.to(\"cpu\"), atol=1e-5)\r\n```\r\n\r\nerror outputs:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/opt/homebrew/Caskroom/miniconda/base/envs/torch-dev/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-2-5169b8d2c5dd>\", line 21, in <module>\r\n    oo_mps = F.scaled_dot_product_attention(\r\nIndexError: Dimension out of range (expected to be in range of [-3, 2], but got 3)\r\n```\r\n\r\nhardware and envs:\r\n```\r\ntorch               2.6.0\r\napple m3 max\r\n```\r\n\r\n---\r\n\r\n",
        "open",
        "2025-02-06T17:15:59Z",
        null,
        null,
        "triaged, open source, topic: bug fixes, release notes: mps, ciflow/mps",
        "main",
        "main",
        2,
        66,
        14,
        6,
        5,
        0,
        "Skylion007, Skylion007, malfet, malfet",
        "COMMENTED, COMMENTED, COMMENTED, COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146623"
    ],
    [
        146622,
        "Fix inductor non-stable argsort/sort test",
        "- Prevent the inductor test for argsort/sort from wrongly failing when the argsort/sort output with stable=False differs from pytorch but is still a valid argsort output.\r\n- Add functionality to allow alternative assert_equal functions in inductor tests for future cases.\r\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @desertfire @chauhang @aakhundov",
        "open",
        "2025-02-06T17:10:12Z",
        null,
        null,
        "triaged, open source, topic: not user facing, module: inductor",
        "main",
        "PYT-466",
        2,
        169,
        19,
        1,
        1,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146622"
    ],
    [
        146620,
        "Enable qint8 and quint8 add for AArch64 using ACL directly",
        "This enables qint8 and quint8 add for AArch64 through Arm Compute Library (ACL) directly.\r\nIt\u2019s based on changes in PR #145942 which enables the use of ACL directly in ATen.\r\nRelative performance improvement using OMP_NUM_THREADS=1 is ~15x, using OMP_NUM_THREADS=32 it\u2019s ~5.4x.\r\n\r\nScript to benchmark quantised add performance:\r\n```\r\nimport torch\r\nimport torch.profiler as profiler\r\n\r\na_f32 = torch.rand((400, 3456),dtype=torch.float)\r\nb_f32 = torch.rand((400, 3456),dtype=torch.float)\r\na_q = torch.quantize_per_tensor(a_f32, 1.2, 0, torch.qint8)\r\nb_q = torch.quantize_per_tensor(b_f32, 1.7, 5, torch.qint8)\r\n\r\nwith profiler.profile(with_stack=True, profile_memory=False, record_shapes=True) as prof:\r\n    for i in range(1000):     \r\n        _ = torch.ops.quantized.add(a_q, b_q, 1.3, 2)\r\nprint(prof.key_averages(group_by_input_shape=True).table(sort_by='self_cpu_time_total', row_limit=50))\r\n```\n\ncc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10",
        "open",
        "2025-02-06T16:54:08Z",
        null,
        null,
        "module: cpu, triaged, open source, release notes: quantization, release notes: releng",
        "main",
        "acl_qadd",
        10,
        577,
        15,
        3,
        2,
        1,
        "malfet, malfet",
        "COMMENTED, COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146620"
    ],
    [
        146617,
        "Generate test reports for pytest when option is given",
        "The argument needs to be appended when test reports should be generated. `IS_CI` is not necessarily set, so rather check `TEST_SAVE_XML` instead as in other places where test reports are conditionally enabled.\r\n\r\nSee also https://github.com/pytorch/pytorch/issues/126523",
        "open",
        "2025-02-06T16:12:06Z",
        null,
        null,
        "triaged, open source, topic: not user facing",
        "main",
        "Flamefire-patch-1",
        1,
        2,
        1,
        1,
        1,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146617"
    ],
    [
        146616,
        "[don't merge] test baseline",
        "Fixes #ISSUE_NUMBER\r\n",
        "open",
        "2025-02-06T16:04:22Z",
        null,
        null,
        "open source, topic: not user facing, ciflow/binaries_wheel, ciflow/xpu",
        "main",
        "test_main",
        1,
        1,
        0,
        1,
        7,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146616"
    ],
    [
        146615,
        "[BE][Ez]: Enable specific ruff rules to prevent antipatterns and bugs",
        "Enables a few ruff rules\r\n* Ban print statements within asserts (likely bugs)\r\n* ~Use string for Decimal literal to prevent loss of precision~ \r\n* ~Do not use default args for __post__init__ in dataclasses, they likely were meant to go into the factory method, the __init__, or somewhere else. The default values are useless here.~\r\n\r\nWait until ruff upgrade for the last 2\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-02-06T16:04:20Z",
        null,
        null,
        "triaged, open source, better-engineering, topic: not user facing, module: dynamo, ciflow/inductor",
        "main",
        "skylion007/enable-RUF-2025-02-06",
        2,
        4,
        3,
        3,
        1,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146615"
    ],
    [
        146614,
        "[CD] Add python 3.13t build for xpu",
        "Fixes #146451\r\n",
        "open",
        "2025-02-06T15:55:52Z",
        null,
        null,
        "open source, ciflow/trunk, topic: not user facing, ciflow/binaries_wheel",
        "main",
        "xpu_py_3_13t",
        3,
        339,
        2,
        1,
        5,
        1,
        "atalman",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146614"
    ],
    [
        146612,
        "[WIP] BaseSubclass",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146612\n\n",
        "open",
        "2025-02-06T15:33:23Z",
        null,
        null,
        "",
        "gh/IvanKobzarev/100/base",
        "gh/IvanKobzarev/100/head",
        3,
        98,
        17,
        1,
        2,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146612"
    ],
    [
        146604,
        "[Profiler] Enable CUPTI teardown to reduce profiler overhead",
        "The problem is that the profiler slowed down\r\ntraining by roughly 10-20% even after completion\r\nbecause cuptiFinalize was not called in Kineto due to TEARDOWN_CUPTI=0. Disabling CUPTI teardown was a workaround for crashes which occured when CUDA graphs were used. This issue was fixed in CUDA 12.6. Also there is no point in disabling CUPTI teardown if CUDA Graphs are not used.\r\n\r\nFixes #144455 \r\n\n\ncc @robieta @chaekit @guotuofeng @guyang3532 @dzhulgakov @davidberard98 @briancoutinho @sraikund16 @sanrise",
        "open",
        "2025-02-06T13:43:37Z",
        null,
        null,
        "triaged, open source, oncall: profiler, topic: not user facing",
        "main",
        "fix/144455_teardown_cupti",
        2,
        22,
        4,
        4,
        8,
        1,
        "sraikund16, sraikund16, davidberard98, mgmtea, mgmtea, mgmtea, mgmtea",
        "APPROVED, APPROVED, CHANGES_REQUESTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146604"
    ],
    [
        146597,
        "Fix bug in _ref_test_helper in test_ops and fix failing test on Aarch64",
        "We have a failing unit test on Aarch64\r\n\r\n```\r\nException: Caused by reference input at index 34: SampleInput(input=Tensor[size=(5, 5, 4), device=\"cpu\", dtype=torch.complex64, contiguous=False], args=(), kwargs={}, broadcasts_input=False, name='')\r\n\r\nTo execute this test, run the following from the base repo dir:\r\n    PYTORCH_OPINFO_SAMPLE_INPUT_INDEX=34 python test/test_ops.py TestCommonCPU.test_python_ref__refs_square_cpu_complex64\r\n\r\nThis message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0\r\n```\r\n\r\nAfter debugging it I found that `ex` variable is not being reset to None on each loop inside _ref_test_helper. Which after fixing, highlighted another expectedFailure to reenable - `nn.functional.hinge_embedding_loss` which was incorrectly being skipped due to the same problem.\r\n\r\nhttps://github.com/pytorch/pytorch/blob/4a545eb85d6ba06079787a83f8ab1a8c8f67c76f/test/test_ops.py#L546\r\nex variable is not reset after this for next loop iteration",
        "open",
        "2025-02-06T11:07:07Z",
        null,
        null,
        "triaged, open source, topic: not user facing",
        "main",
        "test_fix",
        3,
        3,
        11,
        1,
        3,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146597"
    ],
    [
        146596,
        "separate f16 vectorized class from bf16",
        "This refactoring is required as part of https://github.com/pytorch/pytorch/pull/143666\n\ncc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @malfet @snadampal @milpuz01",
        "open",
        "2025-02-06T10:58:15Z",
        null,
        null,
        "module: cpu, open source, module: arm, topic: not user facing",
        "main",
        "refactor",
        4,
        1031,
        404,
        1,
        1,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146596"
    ],
    [
        146595,
        "skip test_torch_dynamo_codegen_pow if CPU backend is not cpp",
        "The test asserts that `aten.pow` is not present in the generated kernel code. When using a CPU backend other than cpp, the kernel contains comments referencing the aten ops that produced the kernel in this case `aten.pow`. \r\n\r\nThis PR skips that test case if the CPU backend is not cpp.\r\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-02-06T10:46:32Z",
        null,
        null,
        "triaged, open source, topic: not user facing, module: dynamo",
        "main",
        "skip-if-cpu-backend-not-cpp",
        1,
        4,
        0,
        1,
        1,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146595"
    ],
    [
        146593,
        "[NOT FOR LANDING] experimental NVSHMEM integration",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146593\n* #146592\n\n\n\ncc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o",
        "open",
        "2025-02-06T10:26:18Z",
        null,
        null,
        "oncall: distributed, release notes: distributed (c10d)",
        "gh/yifuwang/195/base",
        "gh/yifuwang/195/head",
        7,
        471,
        4,
        2,
        1,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146593"
    ],
    [
        146592,
        "clang-format CUDASymmetricMemory.cu",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #146593\n* __->__ #146592\n\n\n\ncc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o",
        "open",
        "2025-02-06T10:26:14Z",
        null,
        null,
        "oncall: distributed, release notes: distributed (c10d)",
        "gh/yifuwang/194/base",
        "gh/yifuwang/194/head",
        1,
        20,
        10,
        1,
        2,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146592"
    ],
    [
        146589,
        "[DDP] Use NCCL allocated memory for gradient bucket",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146589\n\r\nSo that NVLink SHARP comes with zero-copy on H100+ platforms, for DDP applications.\r\nLess SM usage, less memory contention between NCCL kernel and compute kernels.\r\n\r\nAdded env `DDP_DISABLE_COMM_MEM` as a back-out option:\r\n```\r\nAn environment variable to disable comm-optimized memory pool.\r\nDefault is 0, which means comm-optimized memory pool is enabled.\r\nUsers can set it to 1 in case of seeing regression or OOM (because this\r\ncomm MemPool may not share space with regular compute MemPool).\r\n```\r\n\r\ncc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o\n\nDifferential Revision: [D69297766](https://our.internmc.facebook.com/intern/diff/D69297766)",
        "open",
        "2025-02-06T09:01:24Z",
        null,
        null,
        "oncall: distributed, ciflow/trunk, release notes: distributed (c10d), release notes: distributed (ddp), merging",
        "gh/kwen2501/123/base",
        "gh/kwen2501/123/head",
        7,
        127,
        13,
        6,
        14,
        2,
        "Skylion007, Skylion007, Skylion007, Skylion007, kwen2501, Skylion007, kwen2501, syed-ahmed, syed-ahmed, syed-ahmed, syed-ahmed, syed-ahmed, syed-ahmed, yifuwang, kwen2501, yifuwang, fegin, syed-ahmed, kwen2501, c-p-i-o, c-p-i-o, c-p-i-o, c-p-i-o, fduwjj, fduwjj",
        "COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, CHANGES_REQUESTED, APPROVED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, APPROVED, COMMENTED, APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146589"
    ],
    [
        146587,
        "[Dynamo] Allow dynamo to handle `str.xxx()`",
        "\r\nFixes #146350\r\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-02-06T08:47:14Z",
        null,
        null,
        "open source, topic: not user facing, module: dynamo",
        "main",
        "fix/dynamo/str",
        2,
        13,
        0,
        2,
        10,
        1,
        "zou3519, shink, shink, zou3519",
        "COMMENTED, COMMENTED, COMMENTED, APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146587"
    ],
    [
        146583,
        "[symbolic shapes] Log symnode id",
        "We want to log the symnode id which will help us with provenance tracking between expressions created.\r\n\r\n\r\ncc @ezyang @SherlockNoMad @EikanWang @jgong5 @wenzhe-nrv",
        "open",
        "2025-02-06T07:45:13Z",
        null,
        null,
        "ciflow/trunk, release notes: fx, fx, ciflow/inductor, merging",
        "main",
        "angelayi/test",
        1,
        26,
        7,
        1,
        7,
        1,
        "bobrenjc93",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146583"
    ],
    [
        146582,
        "[Partitioner] Reduce time consuming of partitions merger",
        "This patch optimize maybe_merge_partition func through 3-ways:\r\n\r\nRemove unnecessary copy https://github.com/pytorch/pytorch/blob/main/torch/fx/passes/infra/partitioner.py#L99. The number of copied nodes is large if we can merge all of the nodes of graph into one partition.\r\nRecord users of each partition to avoid duplicate iteration over nodes https://github.com/pytorch/pytorch/blob/main/torch/fx/passes/infra/partitioner.py#L133. The trip count of this loop maybe very large.\r\nThe nodes number of each partitions maybe not balance https://github.com/pytorch/pytorch/blob/main/torch/fx/passes/infra/partitioner.py#L145. We always encounter one issue: one partition has n nodes, but the other has one node. Merge the smaller partition into the larger can help to reduce time consuming.\n\ncc @ezyang @SherlockNoMad @EikanWang @jgong5 @wenzhe-nrv",
        "open",
        "2025-02-06T07:35:55Z",
        null,
        null,
        "triaged, open source, release notes: fx, topic: not user facing, fx",
        "main",
        "lingzhiz/optimize_partition_merger",
        1,
        39,
        26,
        3,
        2,
        0,
        "Skylion007",
        "COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146582"
    ],
    [
        146581,
        "Clarify that compile(module) only affects the forward method",
        "Fixes #141616\r\n\r\n## Changes\r\n\r\n- Add `Note` to Clarify how compile works with `nn.Module`\r\n- Optimize plain url address with clickable description\r\n\r\n## Test Result\r\n\r\n### Before\r\n\r\n![image](https://github.com/user-attachments/assets/15ff9985-7e91-4d71-be7d-cdd38eacd3f9)\r\n![image](https://github.com/user-attachments/assets/26e27ba4-52da-4336-b72d-a0f9d0ebe839)\r\n\r\n\r\n### After\r\n\r\n![image](https://github.com/user-attachments/assets/5eaa8421-19e8-4186-af3d-dab4323d2c95)\r\n![image](https://github.com/user-attachments/assets/a96a8a79-6320-4748-931f-33f4dbc640eb)\r\n\r\n",
        "open",
        "2025-02-06T07:34:49Z",
        null,
        null,
        "triaged, open source, topic: not user facing",
        "main",
        "opt/docs/compile",
        1,
        8,
        2,
        1,
        2,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146581"
    ],
    [
        146580,
        "[Partitioner] Remove unnecessary upstream nodes in dependency viewer",
        "We iterate upstream nodes to update partition map. But actually did nothing due to we iterate nodes with reversed topological order https://github.com/pytorch/pytorch/pull/136608/files#diff-f2f9dd3903fd99955732eb694941fea0cb7301a58d59554787f3311d417e5615L193 so that there exists no upstream nodes in assignment. Remove it to reduce for-loop overhead which up to O(N * N) complexity.\r\n\n\ncc @ezyang @SherlockNoMad @EikanWang @jgong5 @wenzhe-nrv",
        "open",
        "2025-02-06T07:32:01Z",
        null,
        null,
        "triaged, open source, release notes: fx, topic: not user facing, fx",
        "main",
        "lingzhiz/remove_upstream_nodes",
        1,
        0,
        19,
        1,
        2,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146580"
    ],
    [
        146578,
        "add `torch.float4_e2m1fn_x2` to PyTorch",
        "Summary:\r\n\r\nAdds the `torch.float4_e2m1fn_x2` dtype to PyTorch, as detailed in\r\nhttps://github.com/pytorch/pytorch/issues/146414 . Please see the issue for a detailed definition of the format.  \r\n\r\nNote that I decided to keep the casts out of this to significantly simplify the code, as defining casting between packed and unpacked formats will be tricky using the existing casting machinery.  \r\n\r\nExample of basic functionality:\r\n\r\n```python\r\nimport torch\r\n\r\n# creation with empty\r\nx0 = torch.empty(4, 4, dtype=torch.float4_e2m1fn_x2)\r\n\r\n# printing, prints the uint8 representation of the stored values\r\nprint(x0)\r\n\r\n# view as other dtype\r\nx0.view(torch.uint8).view(torch.float4_e2m1fn_x2)\r\n```\r\n\r\nDone in this PR:\r\n* tensor creation and tensor printing works (no other ops defined)\r\n\r\nFor future PRs:\r\n* torch._scaled_mm\r\n* PT2\r\n* various cleanups (detailed in comments with issue numbers)\r\n\r\nTest Plan:\r\n\r\n```\r\npytest test/quantization/core/experimental/test_floatx.py -s\r\n```\r\n\r\nReviewers:\r\n\r\nSubscribers:\r\n\r\nTasks:\r\n\r\nTags:\r\n\r\ncc @yanbing-j @albanD @kadeng @penguinwu @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10",
        "open",
        "2025-02-06T07:08:42Z",
        null,
        null,
        "release notes: quantization",
        "gh/vkuzo/1/head",
        "gh/vkuzo/2/head",
        7,
        70,
        5,
        3,
        2,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146578"
    ],
    [
        146574,
        "[ROCm][TunableOp] Offline results are saved to file when offline tuning is disabled.",
        "This PR is to fix UT breakage that has been reported internally and is considered high priority. When `tunable.record_untuned_enable(False)` is invoked, we flush the results of the untuned gemm file.\r\n\r\nOffline tuning I/O currently doesn't have a set untuned results filename member function or untuned results write to file member function. When performing back-to-back unit tests, the same ofstream ends up getting reused between UTs. Due to the way the UT are executed, this can lead to unexpected failures.\r\n\r\ncc: @jfactory07 \r\n\r\n\r\n\r\ncc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang",
        "open",
        "2025-02-06T06:04:28Z",
        null,
        null,
        "module: rocm, open source, topic: not user facing, ciflow/rocm",
        "main",
        "fix_tunableop_untuned_fileio",
        1,
        2,
        0,
        1,
        4,
        0,
        "jeffdaily",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146574"
    ],
    [
        146573,
        "add python root bin to windows load path.",
        "This PR is extend python root bin path to dll load list. \r\nIt makes PyTorch robust and compatible to more dependency libraries, such as `intel-pti`.\r\n\r\ncc @peterjc123 @mszhanyi @skyline75489 @nbcsm @iremyux @Blackhex @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10",
        "open",
        "2025-02-06T05:58:55Z",
        null,
        null,
        "module: windows, open source, ciflow/trunk, topic: not user facing, intel",
        "main",
        "xu_add_init_path",
        1,
        8,
        1,
        1,
        1,
        0,
        "EikanWang",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146573"
    ],
    [
        146571,
        "[Dynamo][autograd.Function] Relax backward speculation strict mode a bit",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #146742\n* #146741\n* __->__ #146571\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-02-06T05:05:51Z",
        null,
        null,
        "topic: not user facing, module: dynamo, ciflow/inductor",
        "gh/yanboliang/63/base",
        "gh/yanboliang/63/head",
        7,
        228,
        44,
        5,
        1,
        0,
        "yanboliang, zou3519, zou3519, zou3519, yanboliang, zou3519, zou3519",
        "COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146571"
    ],
    [
        146562,
        "WIP hacky reordering pass",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146562\n* #146561\n* #146560\n* #146559\n* #146558\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @desertfire @chauhang @aakhundov",
        "open",
        "2025-02-06T01:41:51Z",
        null,
        null,
        "module: inductor, ciflow/inductor",
        "gh/wconstab/396/base",
        "gh/wconstab/396/head",
        3,
        325,
        24,
        1,
        2,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146562"
    ],
    [
        146561,
        "Improve comms debug visualization",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #146562\n* __->__ #146561\n* #146560\n* #146559\n* #146558\n\n",
        "open",
        "2025-02-06T01:41:46Z",
        null,
        null,
        "module: inductor, ciflow/inductor",
        "gh/wconstab/395/base",
        "gh/wconstab/395/head",
        1,
        2,
        3,
        1,
        2,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146561"
    ],
    [
        146560,
        "enable reorder",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #146562\n* #146561\n* __->__ #146560\n* #146559\n* #146558\n\n",
        "open",
        "2025-02-06T01:41:41Z",
        null,
        null,
        "module: inductor, ciflow/inductor",
        "gh/wconstab/394/base",
        "gh/wconstab/394/head",
        1,
        1,
        1,
        1,
        2,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146560"
    ],
    [
        146559,
        "Apply changes from https://github.com/pytorch/pytorch/commit/211847de3c1c3d6cbd299e14a001b794eabf2a2d",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #146562\n* #146561\n* #146560\n* __->__ #146559\n* #146558\n\n",
        "open",
        "2025-02-06T01:41:36Z",
        null,
        null,
        "oncall: distributed, ciflow/inductor",
        "gh/wconstab/393/base",
        "gh/wconstab/393/head",
        1,
        65,
        15,
        1,
        2,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146559"
    ],
    [
        146558,
        "[dtensor] support mixed precision for redistribute (#20)",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #146562\n* #146561\n* #146560\n* #146559\n* __->__ #146558\n\n",
        "open",
        "2025-02-06T01:41:31Z",
        null,
        null,
        "oncall: distributed, ciflow/inductor",
        "gh/wconstab/392/base",
        "gh/wconstab/392/head",
        2,
        52,
        15,
        1,
        3,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146558"
    ],
    [
        146557,
        "Add fqn_modifier at loading_state_dict and unit test",
        "In Fusion model, users might change the state_dict keys by state_dict_hook\r\nThe load_state_dict APIs here won't call model.state_dict() so that the hooks won't be called to change the keys, causing the mismatch between fqn and state_dict keys.\r\n\r\nThe PR here suggests users to add how they would change the state_dict key prefix (they can name it, here we call \"fqn_modifiers\") by default\r\nDuring loading state_dict, we have the prefix change during getting fqn so that they can be processed same as through state_dict hook.\r\n\r\nFor example:\r\nThere's a state_dict_hook:\r\n\r\n```\r\ndef _state_dict_hook(self, destination, prefix, keep_vars):\r\n    \"\"\"Remove \"embedding\" from the original embedding in the state_dict\r\n    name. This keeps the orginal state dict name for the embedding\r\n    from before fusing with the FusionEmbedding.\r\n\r\n    [!Note] This update changes the order of the OrderedDict\r\n    \"\"\"\r\n    key = prefix + \"embedding.weight\"\r\n    new_key = prefix + \"weight\"\r\n    destination[new_key] = destination[key]\r\n    del destination[key]\r\n```\r\n\r\nIn the dsd after this PR, we would skip \"embedding.\" before \"weight\" if find the \"fqn_modifiers\" attribute at that module\r\n```\r\ndef fqn_modifiers(self) -> Dict[str, str]:\r\n    return {\r\n        \"weight\": \"embedding\",\r\n    }\r\n```\n\ncc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o @LucasLLC @MeetVadakkanchery @mhorowitz @pradeepfn @ekr0",
        "open",
        "2025-02-06T01:16:54Z",
        null,
        null,
        "oncall: distributed, topic: not user facing, module: distributed_checkpoint",
        "main",
        "dsd_fqn_modifiers",
        3,
        97,
        7,
        4,
        1,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146557"
    ],
    [
        146545,
        "Update test.sh to run a greater set of unit tests on aarch64",
        "expanded set of unit tests that run  on aarch64 to be the entire set of tests that can be run by run_test.py\r\n\r\n\n\ncc @seemethere @malfet @pytorch/pytorch-dev-infra",
        "open",
        "2025-02-05T23:41:27Z",
        null,
        null,
        "module: ci, triaged, open source, topic: not user facing",
        "main",
        "expanded_unit_tests",
        1,
        1,
        22,
        2,
        3,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146545"
    ],
    [
        146543,
        "Update local_timer.py to improve queue handling",
        "- Switched from `multiprocessing.Queue` to `torch.multiprocessing.Queue`\r\n- Wrapped `qsize()` in `try-except` to prevent `NotImplementedError`\n\ncc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o",
        "open",
        "2025-02-05T23:33:01Z",
        null,
        null,
        "oncall: distributed, triaged, open source, release notes: distributed (torchelastic)",
        "main",
        "update-local-timer",
        1,
        12,
        9,
        1,
        2,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146543"
    ],
    [
        146540,
        "[export] Draft export custom streamer",
        "* Instead of using tlparse's StreamHandler, draft-export will use its own, which will capture the logs, filter them, and only output the relevant ones to the log file. \r\n* To do this, the CaptureStructuredTrace logger will use a `LogRecord` which is basically a dictionary with a custom hash function based on what is being logged. This allows us to deduplicate logs which represent the same thing, such as:\r\n  * \"missing_fake_kernel\" logs with the same operator\r\n  * \"mismatched_fake_kernel\" logs with the same operator and reasoning\r\n  * \"propagate_real_tensor\", \"create_unbacked_symbol\", and \"guard_added\" logs occurring on lines with the same stacktrace",
        "open",
        "2025-02-05T23:24:25Z",
        null,
        null,
        "release notes: export",
        "main",
        "angelayi/draft_logger",
        2,
        89,
        48,
        1,
        1,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146540"
    ],
    [
        146537,
        "[WIP] Log graph breaks",
        "Graph breaks currently aren't logged. We want to log them. Need to test before merging.\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @desertfire @chauhang @aakhundov",
        "open",
        "2025-02-05T23:08:22Z",
        null,
        null,
        "topic: not user facing, module: inductor, module: dynamo, ciflow/inductor, dynamo-logging",
        "main",
        "gh/raymo/log-graph-breaks",
        4,
        64,
        7,
        1,
        1,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146537"
    ],
    [
        146535,
        "[wip] disable decorator for ca",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146535\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @yf225",
        "open",
        "2025-02-05T22:52:10Z",
        null,
        null,
        "module: dynamo, ciflow/inductor, module: compiled autograd",
        "gh/xmfan/174/base",
        "gh/xmfan/174/head",
        9,
        48,
        0,
        2,
        2,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146535"
    ],
    [
        146532,
        "[symbolic shapes] Log SymNode id for provenance",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* (to be filled)\n\n\n\ncc @ezyang @SherlockNoMad @EikanWang @jgong5 @wenzhe-nrv",
        "open",
        "2025-02-05T22:47:15Z",
        null,
        null,
        "release notes: fx, fx, ciflow/inductor",
        "gh/angelayi/66/base",
        "gh/angelayi/66/head",
        3,
        148,
        92,
        1,
        1,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146532"
    ],
    [
        146530,
        "[aoti] Fix FakeTensorMode not detected in aot_compile when there's no input",
        "Summary:\r\nFixes https://github.com/pytorch/pytorch/issues/118304\r\n\r\nWhen we don't have inputs, we should still try to get a FakeTensorMode because there can be unbacked symints in the graph.\r\n\r\nSo we get the FakeTensorMode once when entering compile_fx, and then using the that FakeTensorMode for the rest of the lowering.\r\n\r\nTest Plan:\r\n```\r\nbuck run fbcode//mode/dev-nosan //caffe2/test/inductor:test_aot_inductor -- -r unbacked_arg\r\n```\r\n\r\nDifferential Revision: D69158049\r\n\r\n\r\n\r\n\r\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @desertfire @chauhang @aakhundov",
        "open",
        "2025-02-05T22:25:56Z",
        null,
        null,
        "fb-exported, ciflow/trunk, module: inductor, ciflow/inductor, release notes: AO frontend",
        "main",
        "export-D69158049",
        3,
        60,
        12,
        1,
        8,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146530"
    ],
    [
        146526,
        "[inductor] add units to estimated runtime log",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146526\n* #146513\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @desertfire @chauhang @aakhundov",
        "open",
        "2025-02-05T21:57:04Z",
        null,
        null,
        "module: inductor, ciflow/inductor",
        "gh/xmfan/173/base",
        "gh/xmfan/173/head",
        1,
        2,
        2,
        1,
        2,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146526"
    ],
    [
        146525,
        "[dynamo] improved graph break messages for some common graph break sites",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146525\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-02-05T21:47:17Z",
        null,
        null,
        "module: dynamo, ciflow/inductor, release notes: dynamo, module: compile ux",
        "gh/williamwen42/205/base",
        "gh/williamwen42/205/head",
        14,
        517,
        76,
        2,
        1,
        0,
        "zou3519, zou3519, zou3519, zou3519, zou3519, zou3519, jansel, bobrenjc93",
        "COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146525"
    ],
    [
        146520,
        "CUDA CachingHostAllocator tracks registrations to call correct free",
        "Users may change the allocator config at will. torch unit tests do this. However, allocations using cudaHostRegister should use corresonding cudaHostUnregister and similarly for cudaHostAlloc / cudaFreeHost.",
        "open",
        "2025-02-05T21:19:31Z",
        null,
        null,
        "triaged, open source, topic: not user facing",
        "main",
        "cuda_host_allocator_free",
        1,
        15,
        6,
        1,
        2,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146520"
    ],
    [
        146514,
        "[DTensor][Test] Create a simple unit test for tensordot",
        "Fixes #ISSUE_NUMBER\r\n\r\nThe dims and shape of the tensors are from a specific Shampoo use case. We want to create a unit test for it to make sure there are no regressions for this.\n\ncc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wconstab @d4l3k @c-p-i-o",
        "open",
        "2025-02-05T19:35:29Z",
        null,
        null,
        "oncall: distributed, Merged, Reverted, ciflow/trunk, topic: not user facing, merging, ci-no-td",
        "main",
        "tensordot",
        1,
        23,
        0,
        2,
        13,
        1,
        "tianyu-l",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146514"
    ],
    [
        146513,
        "[dynamo] check for incompatible configs",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146513\n\r\ninternal: https://fb.workplace.com/groups/1075192433118967/permalink/1599802033991335/\r\n\r\nAssuming flags don't change during compilation, we shouldn't allow incompatible configs to be set at torch.compile wrap time.\r\n\r\nNot in this PR: For flags that need to change during compilation, we'd have to be strict about where they can be used in the compile lifecycle\r\n\r\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @desertfire @chauhang @aakhundov",
        "open",
        "2025-02-05T19:09:40Z",
        null,
        null,
        "Merged, Reverted, ciflow/trunk, module: inductor, module: dynamo, ciflow/inductor, release notes: dynamo, merging, ci-no-td",
        "gh/xmfan/172/base",
        "gh/xmfan/172/head",
        3,
        31,
        0,
        4,
        13,
        0,
        "williamwen42",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146513"
    ],
    [
        146512,
        "Fix `DispatchStub.cpp` compilation for gcc 14",
        "Otherwise I get the following error:\r\n\r\n```bash\r\n\r\n.../intel-xpu-backend-for-triton/pytorch/aten/src/ATen/native/DispatchStub.cpp:152:18: error: no matching function for call to \u2018find(std::array<c10::DeviceType, 7>::const_iterator, std::array<c10::DeviceType, 7>::const_iterator, const c10::DeviceType&)\u2019\r\n  152 |     if (std::find(supported_devices.begin(), supported_devices.end(), device_type) == supported_devices.end()) {\r\n      |         ~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nIn file included from /usr/include/c++/14/bits/locale_facets.h:48,\r\n                 from /usr/include/c++/14/bits/basic_ios.h:37,\r\n                 from /usr/include/c++/14/ios:46,\r\n                 from /usr/include/c++/14/ostream:40,\r\n                 from .../intel-xpu-backend-for-triton/pytorch/c10/core/DeviceType.h:13,\r\n                 from .../intel-xpu-backend-for-triton/pytorch/aten/src/ATen/native/DispatchStub.h:3,\r\n                 from .../intel-xpu-backend-for-triton/pytorch/aten/src/ATen/native/DispatchStub.cpp:2:\r\n/usr/include/c++/14/bits/streambuf_iterator.h:435:5: note: candidate: \u2018template<class _CharT2> typename __gnu_cxx::__enable_if<std::__is_char<_CharT2>::__value, std::istreambuf_iterator<_CharT, std::char_traits<_CharT> > >::__type std::find(istreambuf_iterator<_CharT, char_traits<_CharT> >, istreambuf_iterator<_CharT, char_traits<_CharT> >, const _CharT2&)\u2019\r\n  435 |     find(istreambuf_iterator<_CharT> __first,\r\n      |     ^~~~\r\n/usr/include/c++/14/bits/streambuf_iterator.h:435:5: note:   template argument deduction/substitution failed:\r\n.../intel-xpu-backend-for-triton/pytorch/aten/src/ATen/native/DispatchStub.cpp:152:18: note:   mismatched types \u2018std::istreambuf_iterator<_CharT, std::char_traits<_CharT> >\u2019 and \u2018const std::array<c10::DeviceType, 7>::value_type*\u2019 {aka \u2018const c10::DeviceType*\u2019}\r\n  152 |     if (std::find(supported_devices.begin(), supported_devices.end(), device_type) == supported_devices.end()) {\r\n      |         ~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n```\r\n",
        "open",
        "2025-02-05T18:51:17Z",
        null,
        null,
        "triaged, open source, module: dispatch, topic: not user facing",
        "main",
        "patch-2",
        1,
        1,
        0,
        1,
        3,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146512"
    ],
    [
        146510,
        "[ONNX] Bump torchlib opset to 22",
        "Fixes #ISSUE_NUMBER\r\n",
        "open",
        "2025-02-05T18:47:25Z",
        null,
        null,
        "open source, release notes: onnx",
        "main",
        "justinchu/torchlib-opset22",
        30,
        20652,
        436,
        30,
        1,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146510"
    ],
    [
        146509,
        "[BE][CI][Easy] bump `ruff` to 0.9.0: long statements in docstrings",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #145606\n* #144546\n* #144569\n* #145148\n* __->__ #146509\n\n\n\ncc @ezyang @SherlockNoMad @EikanWang @jgong5 @wenzhe-nrv",
        "open",
        "2025-02-05T18:43:13Z",
        null,
        null,
        "open source, ciflow/trunk, release notes: onnx, topic: not user facing, fx",
        "gh/XuehaiPan/240/base",
        "gh/XuehaiPan/240/head",
        4,
        26,
        6,
        4,
        4,
        0,
        "justinchuby",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146509"
    ],
    [
        146506,
        "Support contextlib.ExitStack",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146506\n* #146505\n* #146503\n* #146502\n* #146501\n* #146500\n* #146504\n* #146499\n* #146498\n* #146497\n* #146496\n* #146493\n* #146492\n* #146491\n* #146495\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-02-05T17:22:12Z",
        null,
        null,
        "open source, module: dynamo, ciflow/inductor",
        "gh/guilhermeleobas/107/base",
        "gh/guilhermeleobas/107/head",
        2,
        621,
        9,
        5,
        2,
        0,
        "guilhermeleobas",
        "COMMENTED",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146506"
    ],
    [
        146505,
        "Allow setting attribute to NestedUserFunctionVariable",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #146506\n* __->__ #146505\n* #146503\n* #146502\n* #146501\n* #146500\n* #146504\n* #146499\n* #146498\n* #146497\n* #146496\n* #146493\n* #146492\n* #146491\n* #146495\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-02-05T17:22:05Z",
        null,
        null,
        "open source, module: dynamo, ciflow/inductor",
        "gh/guilhermeleobas/106/base",
        "gh/guilhermeleobas/106/head",
        2,
        21,
        1,
        5,
        2,
        0,
        "anijain2305",
        "COMMENTED",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146505"
    ],
    [
        146504,
        "Introduce `UserDefinedExceptionClassVariable`",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #146506\n* #146505\n* #146503\n* #146502\n* #146501\n* #146500\n* __->__ #146504\n* #146499\n* #146498\n* #146497\n* #146496\n* #146493\n* #146492\n* #146491\n* #146495\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-02-05T17:21:58Z",
        null,
        null,
        "open source, module: dynamo, ciflow/inductor",
        "gh/guilhermeleobas/105/base",
        "gh/guilhermeleobas/105/head",
        5,
        45,
        5,
        5,
        2,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146504"
    ],
    [
        146503,
        "Create new dynamo ObservedExceptions at runtime",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #146506\n* #146505\n* __->__ #146503\n* #146502\n* #146501\n* #146500\n* #146504\n* #146499\n* #146498\n* #146497\n* #146496\n* #146493\n* #146492\n* #146491\n* #146495\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-02-05T17:21:52Z",
        null,
        null,
        "open source, module: dynamo, ciflow/inductor",
        "gh/guilhermeleobas/104/base",
        "gh/guilhermeleobas/104/head",
        3,
        18,
        1,
        5,
        2,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146503"
    ],
    [
        146502,
        "Correctly propagate exception to parent tx",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #146506\n* #146505\n* #146503\n* __->__ #146502\n* #146501\n* #146500\n* #146504\n* #146499\n* #146498\n* #146497\n* #146496\n* #146493\n* #146492\n* #146491\n* #146495\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-02-05T17:21:46Z",
        null,
        null,
        "open source, module: dynamo, ciflow/inductor",
        "gh/guilhermeleobas/103/base",
        "gh/guilhermeleobas/103/head",
        2,
        92,
        3,
        5,
        2,
        0,
        "anijain2305",
        "APPROVED",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146502"
    ],
    [
        146501,
        "Update CPython tests for ctx manager to use unittest",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #146506\n* #146505\n* #146503\n* #146502\n* __->__ #146501\n* #146500\n* #146504\n* #146499\n* #146498\n* #146497\n* #146496\n* #146493\n* #146492\n* #146491\n* #146495\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-02-05T17:21:40Z",
        null,
        null,
        "open source, topic: not user facing, module: dynamo, ciflow/inductor",
        "gh/guilhermeleobas/102/base",
        "gh/guilhermeleobas/102/head",
        1,
        206,
        211,
        5,
        1,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146501"
    ],
    [
        146500,
        "Allow trace through unittest",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #146506\n* #146505\n* #146503\n* #146502\n* #146501\n* __->__ #146500\n* #146504\n* #146499\n* #146498\n* #146497\n* #146496\n* #146493\n* #146492\n* #146491\n* #146495\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-02-05T17:21:34Z",
        null,
        null,
        "open source, module: dynamo, ciflow/inductor, release notes: dynamo",
        "gh/guilhermeleobas/101/base",
        "gh/guilhermeleobas/101/head",
        6,
        623,
        14,
        5,
        1,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146500"
    ],
    [
        146499,
        "Add `__context/cause/suppress_context/traceback__` to Exception",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #146506\n* #146505\n* #146503\n* #146502\n* #146501\n* #146500\n* #146504\n* __->__ #146499\n* #146498\n* #146497\n* #146496\n* #146493\n* #146492\n* #146491\n* #146495\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-02-05T17:21:27Z",
        null,
        null,
        "open source, module: dynamo, ciflow/inductor, release notes: dynamo",
        "gh/guilhermeleobas/100/base",
        "gh/guilhermeleobas/100/head",
        5,
        361,
        13,
        5,
        1,
        0,
        "guilhermeleobas, guilhermeleobas",
        "COMMENTED, COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146499"
    ],
    [
        146498,
        "Add `sys.exc_info` and `sys.exception`",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #146506\n* #146505\n* #146503\n* #146502\n* #146501\n* #146500\n* #146504\n* #146499\n* __->__ #146498\n* #146497\n* #146496\n* #146493\n* #146492\n* #146491\n* #146495\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-02-05T17:21:20Z",
        null,
        null,
        "open source, module: dynamo, ciflow/inductor, release notes: dynamo",
        "gh/guilhermeleobas/99/base",
        "gh/guilhermeleobas/99/head",
        3,
        162,
        0,
        5,
        1,
        1,
        "guilhermeleobas, anijain2305",
        "COMMENTED, APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146498"
    ],
    [
        146497,
        "Propagate `AttributeError` to user code in user_defined.py",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #146506\n* #146505\n* #146503\n* #146502\n* #146501\n* #146500\n* #146504\n* #146499\n* #146498\n* __->__ #146497\n* #146496\n* #146493\n* #146492\n* #146491\n* #146495\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-02-05T17:21:13Z",
        null,
        null,
        "open source, topic: not user facing, module: dynamo, ciflow/inductor",
        "gh/guilhermeleobas/98/base",
        "gh/guilhermeleobas/98/head",
        3,
        38,
        1,
        5,
        1,
        1,
        "anijain2305, anijain2305",
        "COMMENTED, CHANGES_REQUESTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146497"
    ],
    [
        146496,
        "Handle `is`/`is not`",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #146506\n* #146505\n* #146503\n* #146502\n* #146501\n* #146500\n* #146504\n* #146499\n* #146498\n* #146497\n* __->__ #146496\n* #146493\n* #146492\n* #146491\n* #146495\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-02-05T17:21:06Z",
        null,
        null,
        "open source, module: dynamo, ciflow/inductor, release notes: dynamo",
        "gh/guilhermeleobas/97/base",
        "gh/guilhermeleobas/97/head",
        2,
        26,
        0,
        4,
        1,
        1,
        "anijain2305, anijain2305",
        "COMMENTED, CHANGES_REQUESTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146496"
    ],
    [
        146495,
        "Fix round(...) with constants",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #146506\n* #146505\n* #146503\n* #146502\n* #146501\n* #146500\n* #146504\n* #146499\n* #146498\n* #146497\n* #146496\n* #146493\n* #146492\n* #146491\n* __->__ #146495\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-02-05T17:20:59Z",
        null,
        null,
        "open source, ciflow/trunk, module: dynamo, ciflow/inductor, release notes: dynamo, merging",
        "gh/guilhermeleobas/96/base",
        "gh/guilhermeleobas/96/head",
        2,
        10,
        1,
        4,
        6,
        0,
        "anijain2305",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146495"
    ],
    [
        146494,
        "Fix STOPITERATION_ERROR opcode",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #146506\n* #146505\n* #146504\n* #146503\n* #146502\n* #146501\n* #146500\n* #146499\n* #146498\n* #146497\n* #146496\n* #146495\n* __->__ #146494\n* #146493\n* #146492\n* #146491\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-02-05T17:20:54Z",
        null,
        null,
        "open source, module: dynamo, ciflow/inductor, release notes: dynamo",
        "gh/guilhermeleobas/95/base",
        "gh/guilhermeleobas/95/head",
        1,
        5,
        4,
        2,
        1,
        1,
        "anijain2305, anijain2305",
        "COMMENTED, CHANGES_REQUESTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146494"
    ],
    [
        146493,
        "Add `RAISE_VARARGS 0`",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #146506\n* #146505\n* #146503\n* #146502\n* #146501\n* #146500\n* #146504\n* #146499\n* #146498\n* #146497\n* #146496\n* __->__ #146493\n* #146492\n* #146491\n* #146495\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-02-05T17:20:47Z",
        null,
        null,
        "open source, module: dynamo, ciflow/inductor, release notes: dynamo",
        "gh/guilhermeleobas/94/base",
        "gh/guilhermeleobas/94/head",
        2,
        27,
        1,
        4,
        1,
        0,
        "zou3519, guilhermeleobas",
        "COMMENTED, COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146493"
    ],
    [
        146492,
        "Add `WITH_EXCEPT_START` opcode",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #146506\n* #146505\n* #146503\n* #146502\n* #146501\n* #146500\n* #146504\n* #146499\n* #146498\n* #146497\n* #146496\n* #146493\n* __->__ #146492\n* #146491\n* #146495\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-02-05T17:20:40Z",
        null,
        null,
        "open source, module: dynamo, ciflow/inductor, release notes: dynamo",
        "gh/guilhermeleobas/93/base",
        "gh/guilhermeleobas/93/head",
        2,
        46,
        0,
        4,
        1,
        1,
        "zou3519, anijain2305",
        "COMMENTED, APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146492"
    ],
    [
        146491,
        "Add `make_dynamo_test`",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #146506\n* #146505\n* #146503\n* #146502\n* #146501\n* #146500\n* #146504\n* #146499\n* #146498\n* #146497\n* #146496\n* #146493\n* #146492\n* __->__ #146491\n* #146495\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-02-05T17:20:34Z",
        null,
        null,
        "open source, topic: not user facing, module: dynamo, ciflow/inductor",
        "gh/guilhermeleobas/92/base",
        "gh/guilhermeleobas/92/head",
        1,
        45,
        0,
        4,
        1,
        0,
        "zou3519, anijain2305, Skylion007, zou3519",
        "APPROVED, APPROVED, COMMENTED, COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146491"
    ],
    [
        146490,
        "[export] Serialize special values of float into strings for json.",
        "Summary: Currently inf is serialized as Infinity in JSON which is not standard compliant. Instead we will tweak all special floating points into strings and handle them at json layer.\n\nTest Plan:\nsee D69060784\nCI\n\nDifferential Revision: D69186425\n\n\n",
        "open",
        "2025-02-05T16:36:50Z",
        null,
        null,
        "fb-exported, ciflow/trunk, ciflow/inductor, release notes: export",
        "main",
        "export-D69186425",
        6,
        133,
        53,
        3,
        2,
        0,
        "yiming0416",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146490"
    ],
    [
        146489,
        "Update code_template.py re.compile() is directly applied to the regex\u2026",
        "\u2026 string inside the class variable\r\n\r\nre.compile() is directly applied to the regex string inside the class variable\r\n\r\nRegular expressions are very expensive computationally. So, this avoids any redundant compilation.\r\nFixes #ISSUE_NUMBER\r\n",
        "open",
        "2025-02-05T16:23:56Z",
        null,
        null,
        "open source, topic: not user facing",
        "main",
        "patch-1",
        1,
        3,
        2,
        1,
        2,
        0,
        "mikaylagawarecki",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146489"
    ],
    [
        146485,
        "Update quantile doc",
        "Fixes #146156\r\n",
        "open",
        "2025-02-05T15:33:15Z",
        null,
        null,
        "triaged, open source, release notes: python_frontend",
        "main",
        "patch-1",
        1,
        3,
        3,
        1,
        2,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146485"
    ],
    [
        146482,
        "Update addr doc",
        "Fixes https://github.com/pytorch/pytorch/issues/146399\r\n",
        "open",
        "2025-02-05T13:29:04Z",
        null,
        null,
        "topic: not user facing",
        "main",
        "albanD-patch-1",
        1,
        1,
        1,
        1,
        1,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146482"
    ],
    [
        146481,
        "[WIP][Windows][Inductor] Enable Inductor UT on XPU Windows.",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146481\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @desertfire @chauhang @aakhundov",
        "open",
        "2025-02-05T13:22:21Z",
        null,
        null,
        "open source, topic: not user facing, module: inductor, module: dynamo, ciflow/inductor, keep-going, ciflow/xpu",
        "gh/etaf/96/base",
        "gh/etaf/96/head",
        9,
        21,
        17,
        1,
        1,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146481"
    ],
    [
        146480,
        "[Submodule]: Update KleidiAI submodule to v1.3.0",
        "Change-Id: I687255982c72ee7daca438a15b718f07298963cc\r\n\n\ncc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @malfet @snadampal @milpuz01",
        "open",
        "2025-02-05T12:54:37Z",
        null,
        null,
        "module: cpu, open source, module: arm, ciflow/trunk, topic: not user facing, merging",
        "main",
        "kleidiai_submodule_update",
        1,
        1,
        1,
        1,
        9,
        1,
        "digantdesai, malfet",
        "APPROVED, APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146480"
    ],
    [
        146478,
        "[c10d] Add hccl distributed backend to c10d data structures",
        " # MOTIVATION\r\nIntel Gaudi is an out-of-tree PyTorch accelerator having its own device /dispatch key ```hpu``` .\r\nWith this change we add entries for Gaudi's distributed backend ```hccl``` to the c10d Backend data structures.\r\nThis is to ensure that there is no naming conflict in case a new in-tree accelerator is introduced with the same backend name.\r\n\r\n\r\nThe Out-of-tree backends are registered calling https://github.com/pytorch/pytorch/blob/fd0cd6a08f706b7bb1dedb296217b6441e4fb9ff/torch/distributed/distributed_c10d.py#L302\r\n\r\nSuccessful registration adds the backend name to the list : \r\nhttps://github.com/pytorch/pytorch/blob/fd0cd6a08f706b7bb1dedb296217b6441e4fb9ff/torch/distributed/distributed_c10d.py#L265\r\n\r\nWe are binding the process group creator constructs at run-time so if there are other distributed backend with the same device name they can safely add the device type to the dictionary \r\n\r\nhttps://github.com/pytorch/pytorch/blob/fd0cd6a08f706b7bb1dedb296217b6441e4fb9ff/torch/distributed/distributed_c10d.py#L274\r\n\r\nAnd add another entry to the dictionary with the same backend name ( but different device name )\r\nhttps://github.com/pytorch/pytorch/blob/fd0cd6a08f706b7bb1dedb296217b6441e4fb9ff/torch/distributed/distributed_c10d.py#L268\r\n\r\nIn addition the out-of-tree devices can utilize the ```backend_list``` to check for successful backend registration  eg: APIs like ```is_hccl_available```\r\n \r\n\n\ncc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o",
        "open",
        "2025-02-05T12:10:24Z",
        null,
        null,
        "oncall: distributed, triaged, open source, release notes: distributed (c10d)",
        "main",
        "c10d_add_hccl_to_backends",
        1,
        19,
        8,
        1,
        3,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146478"
    ],
    [
        146477,
        "Improve error handling when checking CUDA version in case nvcc is not found",
        "Fixes:\r\n- https://github.com/pytorch/pytorch/issues/101138\r\n\r\n**Description**\r\nThe PR enhances error handling in `_check_cuda_version` by verifying the existence of the `nvcc` executable before invoking `subprocess.check_output`. If `nvcc` is missing, a `FileNotFoundError` is raised with a clear message, guiding users to check their CUDA installation and path configuration.\r\n\r\n**Testing**\r\nManually tested with and without `nvcc` present in the expected path.\r\n\n\ncc @peterjc123 @mszhanyi @skyline75489 @nbcsm @iremyux @Blackhex",
        "open",
        "2025-02-05T12:09:22Z",
        null,
        null,
        "module: windows, triaged, open source, release notes: fx",
        "main",
        "pytorch-101138",
        1,
        4,
        1,
        1,
        1,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146477"
    ],
    [
        146476,
        "[Feat]: Improve KleidiAI 4 bit kernel performance",
        "Description:\r\n1. New thread blocking accelerates GEMVs\r\n\r\nPerf improvements:\r\n12% speedup in LLM prefill phase and upto 16% speedup in autoregressive phase\r\n\r\n\r\nChange-Id: Ie574ff8459fdb75701ae366158b4e118c70694e4\r\n\r\n\r\ncc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @malfet @snadampal @milpuz01",
        "open",
        "2025-02-05T11:50:00Z",
        null,
        null,
        "module: cpu, triaged, open source, module: arm, ciflow/trunk, topic: performance, release notes: intel, merging",
        "main",
        "kleidiai_threading_improvement",
        1,
        171,
        312,
        1,
        10,
        0,
        "malfet",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146476"
    ],
    [
        146475,
        "fix: replace stderr with stdout for download messages in hub.py",
        "This PR addresses an issue where download logs in `hub.py` are sent to `stderr` instead of `stdout`. Hence, when running models with workers, these messages are incorrectly categorized as errors, leading to confusion. ",
        "open",
        "2025-02-05T10:29:29Z",
        null,
        null,
        "triaged, open source, ciflow/trunk, topic: not user facing, merging",
        "main",
        "main",
        1,
        2,
        2,
        1,
        12,
        0,
        "mikaylagawarecki",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146475"
    ],
    [
        146474,
        "Fix torch.take_along_dim param type and default description",
        "## Changes\r\n\r\n- Change type description to `LongTensor`, consistent with [`torch.take`](https://pytorch.org/docs/stable/generated/torch.take.html)\r\n- Add `dim` param default value description\r\n\r\n## Test Result\r\n\r\n**Before**\r\n![image](https://github.com/user-attachments/assets/720ce158-2bc1-48b5-a188-56fcc7188d96)\r\n\r\n**After**\r\n![image](https://github.com/user-attachments/assets/05fe20bd-9476-4b97-ac2b-9b161d6532a1)\r\n\r\n",
        "open",
        "2025-02-05T10:01:46Z",
        null,
        null,
        "triaged, open source, ciflow/trunk, release notes: python_frontend, merging",
        "main",
        "opt/docs/take_along_dim",
        1,
        2,
        2,
        1,
        7,
        0,
        "mikaylagawarecki",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146474"
    ],
    [
        146473,
        "[export] Fix logger handler",
        "Differential Revision: D69169179\n\n\n",
        "open",
        "2025-02-05T08:28:27Z",
        null,
        null,
        "fb-exported, release notes: export",
        "main",
        "export-D69169179",
        1,
        2,
        0,
        1,
        2,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146473"
    ],
    [
        146472,
        "Refactoring pipeline parallelism test cases to be device agnostic [1/n]",
        "In this series of PR we intend to refactor pipeline parallelism test cases to enable to be completely device agnostic.\r\n\r\nThese changes will include the following approaches to do the same :\r\n\r\n\r\n- Allowing for multiple device types using instantiate_device_type_test\r\n- Replacing calls to cuda stream with torch.get_device_module(device) wherever it applies\r\n\r\nThis should result in improvement in usability for all devices\r\n\r\n\r\nFor this PR we have shown support for the following devices:\r\n\r\n- CPU (wherever applicable)\r\n- CUDA\r\n- HPU\r\n- XPU\r\n\r\nTo add other device new users can simply append their device to the device list \r\n\r\ncc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o",
        "open",
        "2025-02-05T07:53:31Z",
        null,
        null,
        "oncall: distributed, triaged, open source, ciflow/trunk, topic: not user facing, merging, module: pipelining",
        "main",
        "AnantGulati_pipeline_refactoring",
        4,
        57,
        41,
        5,
        5,
        1,
        "H-Huang",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146472"
    ],
    [
        146467,
        "Fix an issue where functional collectives don't force fx stride on inputs when compiled",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146467\n\r\nFixes https://github.com/pytorch/pytorch/issues/146416\r\n\r\nAlso added contiguity checks in the C++ functional collective ops to prevent striding issues introduced during compilation manifest as silent correctness issues.\r\n\r\ncc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @desertfire @chauhang @aakhundov",
        "open",
        "2025-02-05T03:54:36Z",
        null,
        null,
        "oncall: distributed, ciflow/trunk, release notes: distributed (c10d), module: inductor, ciflow/inductor, merging",
        "gh/yifuwang/193/base",
        "gh/yifuwang/193/head",
        4,
        105,
        27,
        5,
        8,
        1,
        "Chillee, lw, shunting314",
        "APPROVED, APPROVED, APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146467"
    ],
    [
        146466,
        "Fix one_hot inconsistent errors after compile",
        "Fixes #146274\r\n\r\n**Test Result**\r\n\r\n```python\r\n>>> import torch\r\n>>> f = torch.nn.functional.one_hot\r\n>>> a = torch.arange(0, 5) % 3  # [0,1,2,0,1]\r\n>>> num_classes = 0\r\n>>> torch.nn.functional.one_hot(a,num_classes)\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nRuntimeError: Class values must be smaller than num_classes.\r\n\r\n>>> torch.compile(torch.nn.functional.one_hot)(a,num_classes)\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/zong/code/pytorch/torch/_dynamo/eval_frame.py\", line 570, in _fn\r\n    return fn(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/zong/code/pytorch/torch/_dynamo/external_utils.py\", line 48, in inner\r\n    return fn(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^\r\nRuntimeError: Class values must be smaller than num_classes.\r\n\r\n```\r\n\r\ncc @bdhirsh",
        "open",
        "2025-02-05T02:47:27Z",
        null,
        null,
        "triaged, open source, topic: not user facing",
        "main",
        "fix/aten/one_hot",
        1,
        18,
        9,
        1,
        2,
        1,
        "zou3519",
        "COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146466"
    ],
    [
        146464,
        "[symbolic shapes] Log id for each SymNode",
        "Fixes #ISSUE_NUMBER\r\n\n\ncc @ezyang @SherlockNoMad @EikanWang @jgong5 @wenzhe-nrv",
        "open",
        "2025-02-05T01:32:37Z",
        null,
        null,
        "release notes: fx, fx, ciflow/inductor",
        "main",
        "angelayi/provenance_id",
        3,
        148,
        92,
        1,
        1,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146464"
    ],
    [
        146456,
        "Fix workarea compute in lapackSyevd",
        "work-query APIs return floating point values, that could loose precision when converted back to int. Solve this by using `nextafter` and `ceil`\r\nAdd regression test \r\n\r\nFixes #145801\r\n",
        "open",
        "2025-02-05T00:24:37Z",
        null,
        null,
        "ciflow/trunk, release notes: linalg_frontend, merging",
        "main",
        "wdvr/iss_145801",
        2,
        14,
        2,
        5,
        4,
        0,
        "malfet",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146456"
    ],
    [
        146455,
        "[logging] Save compile state in CompiledFxGraph and make it available at runtime",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146455\n\r\nSummary: To support logging the correct compile_id for runtime timings (like Triton autotuning), save the compile_id in CompiledFxGraph make it available to logging utilities, i.e., dynamo_timed.\r\n\r\nThe previous attempt put the compile_id in the inductor_metadata with the Triton output code, but that broke Triton caching and we reverted. This version does the following:\r\n* When creating or deserializing a CompiledFxGraph, save the compile-time compile_id.\r\n* Implement a class `RuntimeCompileContext` that's analogous to `CompileContext` where we can look up the compile_id at runtime.\r\n* Set this runtime compile context during `CompiledFxGraph.__call__`.\r\n* Removes the compile_id as a param to dynamo_timed; dynamo_timed can figure it out instead.\r\n* Removes separate dynamo_timed params for compile-time and runtime dynamo_compile column names. We can use one param have dynamo_timed figure out whether to treat as a runtime or compile-time event.\r\n\r\nTest Plan:\r\n* tlparse (`python benchmarks/dynamo/torchbench.py --performance --training --amp --backend inductor --device cuda --print-compilation-time --repeat 5 --cold-start-latency --only nanogpt`): https://fburl.com/bu5i8efk\r\n* dynamo_compile: https://fburl.com/scuba/dynamo_compile/sandbox/3d74ps92\r\n* pt2_compile_events: https://fburl.com/scuba/pt2_compile_events/ooqoe5tu\r\n\r\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @desertfire @chauhang @aakhundov",
        "open",
        "2025-02-05T00:23:56Z",
        null,
        null,
        "topic: not user facing, module: inductor, module: dynamo, ciflow/inductor",
        "gh/masnesral/176/base",
        "gh/masnesral/176/head",
        7,
        113,
        74,
        5,
        2,
        0,
        "masnesral, masnesral, masnesral, masnesral, masnesral, masnesral, masnesral, masnesral, masnesral, masnesral, xmfan, masnesral, masnesral, xmfan, xmfan, masnesral, xmfan, xmfan, masnesral",
        "COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146455"
    ],
    [
        146454,
        "[dynamo][fullgraph] Raise NoGraphError if no graph with fullgraph=True",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146454\n* #146507\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-02-04T23:50:47Z",
        null,
        null,
        "ciflow/trunk, topic: not user facing, module: dynamo, ciflow/inductor",
        "gh/anijain2305/669/base",
        "gh/anijain2305/669/head",
        3,
        77,
        0,
        4,
        1,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146454"
    ],
    [
        146452,
        "cpp_wrapper: enable all CI inductor tests",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146452\n* #146706\n* #146424\n* #146109\n* #146449\n* #144349\n* #144293\n* #144002\n\nWith the speedups from precompiled headers, we can now enable all currently enabled CI tests for inductor in cpp_wrapper mode.",
        "open",
        "2025-02-04T22:58:10Z",
        null,
        null,
        "open source, topic: not user facing, ciflow/inductor, keep-going, ci-no-test-timeout",
        "gh/benjaminglass1/66/base",
        "gh/benjaminglass1/66/head",
        1,
        25,
        45,
        6,
        1,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146452"
    ],
    [
        146449,
        "cpp_wrapper: handle mixed-device C-shim fallbacks",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #146452\n* #146706\n* #146424\n* #146109\n* __->__ #146449\n* #144349\n* #144293\n* #144002\n\nFixes an error from test_torch, where a CUDA cpp_wrapper run called a CUDA native C-shim kernel with two CPU tensors.\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @desertfire @chauhang @aakhundov",
        "open",
        "2025-02-04T22:07:25Z",
        null,
        null,
        "open source, topic: not user facing, module: inductor, ciflow/inductor",
        "gh/benjaminglass1/65/base",
        "gh/benjaminglass1/65/head",
        5,
        90,
        42,
        4,
        1,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146449"
    ],
    [
        146448,
        "[ROCm] Indexing perf optimization via Unroll/WideFetch/IdxReuse/OneDupOpt",
        "Fixes #ISSUE_NUMBER\r\n\n\ncc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd",
        "open",
        "2025-02-04T21:56:59Z",
        null,
        null,
        "module: rocm, triaged, open source, release notes: cuda, ciflow/rocm",
        "main",
        "main",
        1,
        49,
        0,
        1,
        2,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146448"
    ],
    [
        146443,
        "Stop poisoning fork on Dataloader creation when pin_memory is enabled",
        "Fixes https://github.com/pytorch/pytorch/issues/144687\r\nNeeds https://github.com/pytorch/pytorch/pull/146098 that already landed to fix the issue above\r\n\r\nA longer-term fix would be to move cuda's non-poisoning is_available() check to c++. But that would be quite a bit of work.\r\n\r\nThis PR also updates the behavior of current_accelerator() in python to match getAccelerator() in C++ and update all docs to reflect that.",
        "open",
        "2025-02-04T20:54:13Z",
        null,
        null,
        "release notes: dataloader, topic: bug fixes",
        "main",
        "fix_dataloader",
        6,
        66,
        26,
        5,
        1,
        1,
        "ngimel, guangyey, guangyey, guangyey, guangyey, andrewkho",
        "APPROVED, COMMENTED, APPROVED, COMMENTED, COMMENTED, APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146443"
    ],
    [
        146440,
        "[sigmoid] Implement a OSS only model runner.",
        "Summary: Implement an oss version of modelrunner with clean dependencies. The new oss model runner only removes thrift and only use json header to load the model.\n\nTest Plan: Test will be added in the next diff separately. (D69060784)\n\nDifferential Revision: D68846877\n\n\n",
        "open",
        "2025-02-04T19:38:20Z",
        null,
        null,
        "fb-exported, ciflow/trunk, ciflow/inductor, release notes: export",
        "main",
        "export-D68846877",
        1,
        5,
        4,
        1,
        5,
        0,
        "SherlockNoMad",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146440"
    ],
    [
        146436,
        "[Testing] Reduce `test_exp` flakiness",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146436\n\r\nBy setting `reference_in_float` to false,  as `exp(a + b)` could yield significantly different results than `exp(a.half()+b.half())` as one can see in the following example (which is accidentally the random values generated by MacOS RNG for this test)\r\n\r\n```\r\n>>> import torch\r\n>>> x=torch.tensor(2.5599, dtype=torch.half)\r\n>>> y=torch.tensor(0.6970, dtype=torch.half)\r\n>>> (x + y).exp()\r\ntensor(26., dtype=torch.float16)\r\n>>> (x.float() + y.float()).exp()\r\ntensor(25.9799)\r\n```\r\n\r\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @desertfire @chauhang @aakhundov",
        "open",
        "2025-02-04T18:54:11Z",
        null,
        null,
        "Merged, Reverted, topic: not user facing, ciflow/mps, module: inductor, ciflow/inductor, ci-no-td",
        "gh/malfet/169/base",
        "gh/malfet/169/head",
        2,
        9,
        2,
        2,
        6,
        0,
        "dcci, malfet, dcci, jansel",
        "COMMENTED, COMMENTED, APPROVED, APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146436"
    ],
    [
        146427,
        "add the `torch.float8_e8m0fnu` dtype to PyTorch",
        "Summary:\r\n\r\nAdds the `torch.float8_e8m0fnu` dtype to PyTorch, as detailed in\r\nhttps://github.com/pytorch/pytorch/issues/146414 . Please see the issue for a detailed definition of the format.  Example of basic functionality:\r\n\r\n```python\r\nimport torch\r\n\r\n# round trip\r\nx0 = torch.randn(4, 4, dtype=torch.float32)\r\nx1 = x0.to(torch.float8_e8m0fnu)  # RNE rounding\r\nx2 = x1.to(torch.float32)  # 2 ** exponent\r\n\r\n# creation with empty\r\nx0 = torch.empty(4, 4, dtype=torch.float8_e8m0fnu)\r\n\r\n# printing\r\nprint(x0)\r\n```\r\n\r\nDone in this PR:\r\n* numerical correctness\r\n* op coverage (except for `torch._scaled_mm`): create tensor, cast to/from float32\r\n* printing a tensor works\r\n\r\nFor future PRs:\r\n* performance optimizations for casting\r\n* torch._scaled_mm\r\n* PT2\r\n* various cleanups (detailed in comments with issue numbers)\r\n\r\nTest Plan:\r\n\r\n```\r\npytest test/quantization/core/experimental/test_float8.py -s\r\n```\r\n\r\nReviewers:\r\n\r\nSubscribers:\r\n\r\nTasks:\r\n\r\nTags:\r\n\r\ncc @yanbing-j @albanD @kadeng @penguinwu @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10",
        "open",
        "2025-02-04T18:07:11Z",
        null,
        null,
        "module: cpu, release notes: quantization, module: float8",
        "main",
        "gh/vkuzo/1/head",
        23,
        508,
        43,
        8,
        2,
        0,
        "vkuzo, vkuzo, vkuzo, vkuzo, vkuzo, vkuzo, vkuzo, vkuzo, vkuzo, vkuzo, vkuzo, drisspg, drisspg, drisspg, drisspg, vkuzo, vkuzo, drisspg, drisspg, vkuzo, vkuzo, eqy, vkuzo, drisspg",
        "COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146427"
    ],
    [
        146426,
        "Test typing of arithmetic operators on Tensor (see #145838)",
        "See #145838\r\n\r\nStack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146426\n\r\n",
        "open",
        "2025-02-04T18:06:50Z",
        null,
        null,
        "open source, ciflow/trunk, topic: not user facing, merging",
        "gh/rec/131/base",
        "gh/rec/131/head",
        2,
        462,
        0,
        5,
        10,
        0,
        "Skylion007, Skylion007, rec, rec",
        "COMMENTED, APPROVED, COMMENTED, COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146426"
    ],
    [
        146424,
        "cpp_wrapper: fix test_torchinductor* tests",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #146452\n* #146706\n* __->__ #146424\n* #146109\n* #146449\n* #144349\n* #144293\n* #144002\n\n\n\ncc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @voznesenskym @penguinwu @EikanWang @Guobing-Chen @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @desertfire @chauhang @aakhundov",
        "open",
        "2025-02-04T18:04:13Z",
        null,
        null,
        "module: cpu, open source, topic: not user facing, module: inductor, ciflow/inductor",
        "gh/benjaminglass1/64/base",
        "gh/benjaminglass1/64/head",
        3,
        22,
        7,
        5,
        1,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146424"
    ],
    [
        146421,
        "experimental specialization logging",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146421\n\n\n\ncc @ezyang @SherlockNoMad @EikanWang @jgong5 @wenzhe-nrv\n\nDifferential Revision: [D69163120](https://our.internmc.facebook.com/intern/diff/D69163120)",
        "open",
        "2025-02-04T17:34:46Z",
        null,
        null,
        "ciflow/trunk, release notes: fx, fx, ciflow/inductor",
        "gh/bobrenjc93/270/base",
        "gh/bobrenjc93/270/head",
        1,
        37,
        1,
        6,
        2,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146421"
    ],
    [
        146420,
        "[ROCm] Enable tunable warp size for stride one indexing backwards kernel",
        "Enable tunable warp size for stride one indexing backwards kernel. This will allow for the indexing backward kernel with stride one to work on smaller warp sizes.\n\ncc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd",
        "open",
        "2025-02-04T17:24:38Z",
        null,
        null,
        "module: rocm, triaged, open source, topic: not user facing, ciflow/periodic, rocm, ciflow/rocm",
        "main",
        "improve-backwards-indexing-with-stride-1",
        1,
        74,
        0,
        1,
        1,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146420"
    ],
    [
        146418,
        "[BE]: Add TypeVarTuple to RNN Args for better type inference",
        "Fixes #ISSUE_NUMBER\r\n",
        "open",
        "2025-02-04T16:46:07Z",
        null,
        null,
        "open source",
        "main",
        "skylion007/typevartuple-nn-rnn-2025-02-04",
        1,
        5,
        4,
        1,
        2,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146418"
    ],
    [
        146417,
        "Only call triton in worker process, kick off worker processes earlier, during inductor codegen",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\r\n* __->__ #146417\r\n\r\n### Big idea\r\nThis PR extends https://github.com/pytorch/pytorch/pull/144288 by combining calling triton in worker processes with the future cache: we kick off triton compilation in the worker processes earlier, during inductor codegen. Basically instead of calling async_compile.triton for the first time only after the entire code has been generated, we start compiling as soon as we know we'll need to compile the kernel. Then, when loading the generated inductor code, we can simply read from our in memory future cache, considerably increasing the parallelism.\r\n### Implementation Overview\r\nIn total, the diff does the following:\r\n- Converts TritonFuture to LambdaFuture, only calling triton.compile on worker processes\r\n- Now that triton.compile() isn't called on the main process, we call TritonBundler on all compiled kernels when we get them back from workers\r\n- Extend @eellison's future cache to a class, mostly as a refactor\r\n- Finally, call async_compile.triton ahead of time in Scheduler.codegen if workers are warmed up. This causes the subsequent\r\nasync_compile.triton call that occurs after codegen to cache hit on cold start.\r\nIn the diffs after this, I will add more to CompiledTritonKernels so that TritonBundler, on a warm start, automatically populates the in memory cache on warm start with the existing triton kernels, avoiding calling triton altogether on warm starts.\r\nBecause LambdaFutures are much faster to kick off than TritonFutures, due to not needing to load from TritonCodeCache at all, the time spent kicking off these worker jobs is pretty minimal for inductor codegen.\r\n\r\nDifferential Revision: [D69123174](https://our.internmc.facebook.com/intern/diff/D69123174/)\r\n\r\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @desertfire @chauhang @aakhundov",
        "open",
        "2025-02-04T16:20:19Z",
        null,
        null,
        "fb-exported, ciflow/trunk, topic: not user facing, module: inductor, ciflow/inductor",
        "gh/jamesjwu/106/base",
        "gh/jamesjwu/106/head",
        9,
        250,
        77,
        16,
        17,
        1,
        "jamesjwu, jamesjwu, masnesral, jamesjwu, jansel",
        "COMMENTED, COMMENTED, COMMENTED, COMMENTED, CHANGES_REQUESTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146417"
    ],
    [
        146415,
        "Only call triton in worker process, ahead of time compile",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146415\n\n# Big idea\nThis PR extends https://github.com/pytorch/pytorch/pull/144288 by combining calling triton in worker processes with the future cache: we kick off triton compilation in the worker processes earlier, during inductor codegen. Basically instead of calling async_compile.triton for the first time only after the entire code has been generated, we start compiling as soon as we know we'll need to compile the kernel. Then, when loading the generated inductor code, we can simply read from our in memory future cache, considerably increasing the parallelism.\n# Implementation Overview\nIn total, the diff does the following:\n- Converts TritonFuture to LambdaFuture, only calling triton.compile on worker processes\n- Now that triton.compile() isn't called on the main process, we call TritonBundler on all compiled kernels when we get them back from workers\n- Extend @eellison's future cache to a class, mostly as a refactor\n- Finally, call async_compile.triton ahead of time in Scheduler.codegen if workers are warmed up. This causes the subsequent\nasync_compile.triton call that occurs after codegen to cache hit on cold start.\nIn the diffs after this, I will add more to CompiledTritonKernels so that TritonBundler, on a warm start, automatically populates the in memory cache on warm start with the existing triton kernels, avoiding calling triton altogether on warm starts.\nBecause LambdaFutures are much faster to kick off than TritonFutures, due to not needing to load from TritonCodeCache at all, the time spent kicking off these worker jobs is pretty minimal for inductor codegen.\n\n### Can we split the diff for easier review?\nIt's best if this diff lands atomically with all of these changes, as doing the ahead of time codegen compile is only performant if we replace TritonFuture with LambdaFuture(as we don't need to load the triton kernel on the main process). However, I've made a diff stack for easier reviewing here:\n- D69070048 - Run async_compile.triton ahead of time in Scheduler.codegen\n- D68633454 - Only call triton in worker process\n\nDifferential Revision: [D69070616](https://our.internmc.facebook.com/intern/diff/D69070616/)\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @desertfire @chauhang @aakhundov",
        "open",
        "2025-02-04T16:14:20Z",
        null,
        null,
        "fb-exported, module: inductor, ciflow/inductor",
        "gh/jamesjwu/105/base",
        "gh/jamesjwu/105/head",
        6,
        111,
        63,
        1,
        3,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146415"
    ],
    [
        146407,
        "[ROCm] Unskip std:bad_alloc failures",
        "Flakey MI300 issue related to memory usage should now be resolved after https://github.com/pytorch/pytorch/actions/runs/13007160888?pr=145829.\n\ncc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @hongxiayang @naromero77amd @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @desertfire @chauhang @aakhundov",
        "open",
        "2025-02-04T14:14:17Z",
        null,
        null,
        "module: rocm, triaged, open source, ciflow/trunk, topic: not user facing, module: inductor, ciflow/inductor, merging, ciflow/rocm",
        "main",
        "unskip-bad-alloc",
        1,
        0,
        7,
        1,
        4,
        0,
        "jeffdaily",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146407"
    ],
    [
        146406,
        "Only enable aotriton on x86_64 and aarch64",
        "Make `USE_FLASH_ATTENTION` and `USE_MEM_EFF_ATTENTION` depend on `CPU_INTEL OR CPU_AARCH64`.\r\n\r\n[aotriton pre-built](https://github.com/ROCm/aotriton/releases) is only available on x86_64.\r\n\r\nAlthough `AOTRITON_INSTALL_FROM_SOURCE` can be specified to build from source, building aotriton requires CUDA, so on architectures without CUDA support (like riscv64), it still needs to be disabled.",
        "open",
        "2025-02-04T13:33:47Z",
        null,
        null,
        "triaged, open source, topic: not user facing",
        "main",
        "patch-1",
        1,
        3,
        2,
        1,
        4,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146406"
    ],
    [
        146403,
        "Use std::string_view",
        "Fixes #ISSUE_NUMBER\r\n",
        "open",
        "2025-02-04T12:59:16Z",
        null,
        null,
        "open source, topic: not user facing",
        "main",
        "string_view_gen2",
        2,
        2,
        1,
        1,
        1,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146403"
    ],
    [
        146395,
        "[dynamo][builtin-skipfile-cleanup] Remove random",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146395\n* #146339\n* #146116\n* #146322\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-02-04T05:35:35Z",
        null,
        null,
        "module: dynamo, ciflow/inductor, keep-going",
        "gh/anijain2305/668/base",
        "gh/anijain2305/668/head",
        1,
        0,
        2,
        1,
        2,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146395"
    ],
    [
        146393,
        "PEP585: More fixes 2",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146393\n* #146392\n* #146391\n\n\n\ncc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o @EikanWang @jgong5 @wenzhe-nrv @sanchitintel @ezyang @SherlockNoMad",
        "open",
        "2025-02-04T04:20:02Z",
        null,
        null,
        "oncall: distributed, oncall: jit, release notes: quantization, fx, ciflow/inductor, release notes: AO frontend",
        "gh/aorenste/217/base",
        "gh/aorenste/217/head",
        30,
        62,
        76,
        1,
        1,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146393"
    ],
    [
        146392,
        "PEP585: More fixes",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #146393\n* __->__ #146392\n* #146391\n\n",
        "open",
        "2025-02-04T04:19:56Z",
        null,
        null,
        "release notes: onnx, module: inductor, module: dynamo, ciflow/inductor",
        "gh/aorenste/216/base",
        "gh/aorenste/216/head",
        30,
        108,
        147,
        1,
        1,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146392"
    ],
    [
        146391,
        "PEP585: Add noqa to necessary tests",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146391\n\n",
        "open",
        "2025-02-04T04:19:51Z",
        null,
        null,
        "topic: not user facing",
        "gh/aorenste/215/base",
        "gh/aorenste/215/head",
        7,
        63,
        31,
        7,
        1,
        1,
        "justinchuby, justinchuby, albanD, aorenste",
        "COMMENTED, APPROVED, COMMENTED, COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146391"
    ],
    [
        146388,
        "[WIP][CUDA][cuDNN] Experimental `cudnn_rms_norm`",
        "opt-in for now behind two new native functions---the plan would be to eventually add it as the `CUDA:` backend to `rms_norm`\r\n\r\nInitial experiments show forward ~4-5x speed, up fwd+bwd ~3x speedup\n\ncc @csarofeen @ptrblck @xwang233 @msaroufim",
        "open",
        "2025-02-04T02:55:56Z",
        null,
        null,
        "module: cudnn, module: cuda, open source, module: norms and normalization, topic: not user facing",
        "main",
        "cudnnrmsforward",
        5,
        422,
        0,
        4,
        5,
        0,
        "albanD, eqy",
        "COMMENTED, COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146388"
    ],
    [
        146387,
        "AMD: reverting to default config for better performance.",
        "TopK performance on ROCm performs better on the test suite with the default config.",
        "open",
        "2025-02-04T02:49:19Z",
        null,
        null,
        "open source, release notes: cuda",
        "main",
        "topk_rocm_tune",
        1,
        0,
        9,
        1,
        2,
        1,
        "malfet",
        "APPROVED",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146387"
    ],
    [
        146385,
        "[WIP] Confirm XPU Regression",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146385\n\n",
        "open",
        "2025-02-04T02:32:40Z",
        null,
        null,
        "triaged, open source, topic: not user facing, ciflow/xpu",
        "gh/EikanWang/74/base",
        "gh/EikanWang/74/head",
        1,
        1,
        1,
        1,
        1,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146385"
    ],
    [
        146372,
        "[Submodule] Turning flash-attention integration into 3rd party submod (#144120)",
        "Summary:\n\n# Summary\n\n### Sticky points\n\nCuda-graph rng handling has changed / deviated from original implementation. We will be left with a dangling 'offset' val and confusing naming due to BC\n\n## Dependencies\n- Flash PR: https://github.com/Dao-AILab/flash-attention/pull/1419\n\n### Other Points\n- The BC linter is complaining about losing generate.py and its functions which is not real BC surface\ncc albanD\n\nimported-using-ghimport\n\nTest Plan:\nImported from OSS\n\nBuilding in dev\n`buck build @//mode/dev-nosan -c fbcode.nvcc_arch=h100a  //caffe2:ATen-cu --show-full-output    `\n\nI and Nming the .so I do see that the flash symbols are correctly named:\n```\n0000000001c3dfb0 t pytorch_flash::run_mha_bwd(pytorch_flash::Flash_bwd_params&, CUstream_st*)::$_0::operator()() const::{lambda()#1}::operator()() const::{lambda()#1}::operator()() const::{lambda()#7}::operator()() const\n0000000001c36080 t pytorch_flash::run_mha_fwd(pytorch_flash::Flash_fwd_params&, CUstream_st*, bool)::$_0::operator()() const::{lambda()#2}::operator()() const::{lambda()#1}::operator()() const::{lambda()#6}::operator()() const\n0000000001c360e0 t pytorch_flash::run_mha_fwd(pytorch_flash::Flash_fwd_params&, CUstream_st*, bool)::$_0::operator()() const::{lambda()#2}::operator()() const::{lambda()#1}::operator()() const::{lambda()#7}::operator()() const\n0000000001c35fc0 t pytorch_flash::run_mha_fwd(pytorch_flash::Flash_fwd_params&, CUstream_st*, bool)::$_0::operator()() const::{lambda()#1}::operator()() const::{lambda()#1}::operator()() const::{lambda()#6}::operator()() const\n0000000001c36020 t pytorch_flash::run_mha_fwd(pytorch_flash::Flash_fwd_params&, CUstream_st*, bool)::$_0::operator()() const::{lambda()#1}::operator()() const::{lambda()#1}::operator()() const::{lambda()#7}::operator()() const\n```\n\nReviewed By: vkuzo\n\nDifferential Revision: D68502879\n\nPulled By: drisspg\n",
        "open",
        "2025-02-04T00:34:54Z",
        null,
        null,
        "fb-exported, ciflow/trunk, topic: not user facing, ciflow/inductor, module: sdpa",
        "main",
        "export-D68502879",
        30,
        121,
        4214,
        1,
        9,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146372"
    ],
    [
        146367,
        "[dynamo][EXPERIMENT] Prototype for `mark_traceable`",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146367\n* #146714\n* #146713\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-02-04T00:05:53Z",
        null,
        null,
        "module: dynamo, ciflow/inductor",
        "gh/StrongerXi/81/base",
        "gh/StrongerXi/81/head",
        8,
        366,
        30,
        2,
        2,
        1,
        "StrongerXi, zou3519, StrongerXi, StrongerXi, StrongerXi, StrongerXi, zou3519",
        "COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146367"
    ],
    [
        146364,
        "[DeviceMesh] Add some documentation for `from_group` API and add a 2D test",
        "Fixes #ISSUE_NUMBER\r\n\n\ncc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wconstab @d4l3k @c-p-i-o @tianyu-l @XilunWu",
        "open",
        "2025-02-03T22:54:40Z",
        null,
        null,
        "oncall: distributed, module: dtensor, release notes: distributed (dtensor)",
        "main",
        "add_from_group_doc_and_test",
        2,
        102,
        11,
        1,
        1,
        2,
        "fduwjj, fduwjj, wz337, fegin, wz337",
        "COMMENTED, APPROVED, COMMENTED, COMMENTED, COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146364"
    ],
    [
        146356,
        "[cutlass backend] fix bug for accuminator dtype",
        "Will add unit tests for accuracy. \r\n\r\nStack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #146743\n* __->__ #146356\n\r\n\r\n\r\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @desertfire @chauhang @aakhundov",
        "open",
        "2025-02-03T22:03:13Z",
        null,
        null,
        "Merged, Reverted, ciflow/trunk, topic: not user facing, module: inductor, ciflow/inductor, ci-no-td",
        "gh/henrylhtsang/2/base",
        "gh/henrylhtsang/2/head",
        2,
        7,
        74,
        5,
        10,
        0,
        "Chillee",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146356"
    ],
    [
        146355,
        "[dynamo] replace hardcoded eval frame control flags skip_code_recursive_flag/cache_limit_hit_flag",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146355\n* #145603\n\r\nThis PR and the previous:\r\n- Moves parts of `eval_frame.c` to C++.\r\n- Reduces code duplication in `dynamo__custom_eval_frame` and makes the control flow more clear.\r\n- Enables `convert_frame` to signal to `eval_frame.cpp` in a general manner how to evaluate this frame, recursive frames, and future frames with the same code object (default/compile, skip, run-only). e.g. this will allow us to change skipping/cache limit hit eval_frame behavior directly from convert_frame without requiring changes to C/C++.\r\n\r\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-02-03T22:01:46Z",
        null,
        null,
        "topic: not user facing, module: dynamo, ciflow/inductor",
        "gh/williamwen42/204/base",
        "gh/williamwen42/204/head",
        9,
        215,
        172,
        5,
        1,
        0,
        "williamwen42, jansel, anijain2305, williamwen42, jansel",
        "COMMENTED, CHANGES_REQUESTED, COMMENTED, COMMENTED, COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146355"
    ],
    [
        146352,
        "Build a storage reader/writer to write checkpoints in HF format",
        "Summary: Title - we want to write checkpoints in HF format with DCP, this diff allows this for the non-distributed use case.\r\n\r\nTest Plan:\r\nbuck2 test 'fbcode//mode/dev-nosan' fbcode//caffe2/test/distributed/checkpoint:test_hf_torchtune_storage\r\n\r\nN6476188 --> able to save and load tensor in hf format\r\n\r\nDifferential Revision: D68444967\r\n\r\n\r\n\r\n\r\ncc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o @LucasLLC @MeetVadakkanchery @mhorowitz @pradeepfn @ekr0",
        "open",
        "2025-02-03T21:20:40Z",
        null,
        null,
        "oncall: distributed, fb-exported, ciflow/trunk, module: distributed_checkpoint",
        "main",
        "export-D68444967",
        4,
        316,
        5,
        1,
        6,
        0,
        "saumishr, ankitageorge, ankitageorge, ankitageorge, ankitageorge, ankitageorge, fegin, ankitageorge, ankitageorge",
        "COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146352"
    ],
    [
        146341,
        "Only call triton in worker process; run async_compile.triton ahead of time in Scheduler.codegen",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* (to be filled)\n\n### Big idea\nThis PR extends https://github.com/pytorch/pytorch/pull/144288 by combining calling triton in worker processes with the future cache: we kick off triton compilation in the worker processes earlier, during inductor codegen. Basically instead of calling async_compile.triton for the first time only after the entire code has been generated, we start compiling as soon as we know we'll need to compile the kernel. Then, when loading the generated inductor code, we can simply read from our in memory future cache, considerably increasing the parallelism.\n\n### Implementation Overview\nIn total, the diff does the following:\n- Converts TritonFuture to LambdaFuture, only calling triton.compile on worker processes\n- Now that triton.compile() isn't called on the main process, we call TritonBundler on all compiled kernels when we get them back from workers\n- Extend @eellison's future cache to a class, mostly as a refactor\n- Finally, call async_compile.triton ahead of time in Scheduler.codegen if workers are warmed up. This causes the subsequent async_compile.triton call that occurs after codegen to cache hit on cold start.\n\nIn the diffs after this, I will add more to CompiledTritonKernels so that TritonBundler, on a warm start, automatically populates the in memory cache on warm start with the existing triton kernels, avoiding calling triton altogether on warm starts.\n\nBecause LambdaFutures are much faster to kick off than TritonFutures, due to not needing to load from TritonCodeCache at all, the time spent kicking off these worker jobs is pretty minimal for inductor codegen.\n\n### Can we split the diff for easier review?\nIt's best if this diff lands atomically with all of these changes, as doing the ahead of time codegen compile is only performant if we replace TritonFuture with LambdaFuture(as we don't need to load the triton kernel on the main process). However, I've made a diff stack for easier reviewing here:\nD69070048 - Run async_compile.triton ahead of time in Scheduler.codegen\nD68633454 - Only call triton in worker process\n\nDifferential Revision: [D69013710](https://our.internmc.facebook.com/intern/diff/D69013710/)\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @desertfire @chauhang @aakhundov",
        "open",
        "2025-02-03T20:46:41Z",
        null,
        null,
        "module: inductor, ciflow/inductor",
        "gh/jamesjwu/102/base",
        "gh/jamesjwu/102/head",
        6,
        108,
        71,
        1,
        2,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146341"
    ],
    [
        146335,
        "[dynamic shapes][WIP] mark backed size symbols as size-like",
        "experimental, to apply upper-bound / maxsize size-oblivious semantics to backed symbols\r\n\r\n\r\ncc @ezyang @SherlockNoMad @EikanWang @jgong5 @wenzhe-nrv @voznesenskym @penguinwu @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-02-03T19:56:25Z",
        null,
        null,
        "ciflow/trunk, release notes: fx, fx, module: dynamo, ciflow/inductor",
        "main",
        "pianpwk/treat_sizes_as_size_like",
        5,
        31,
        8,
        7,
        2,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146335"
    ],
    [
        146333,
        "Add optional generator to distribution sampler/rsample methods.",
        "Fixes part of #45115 and #11340\r\nAdds a generator parameter to all the sample/rsample methods of torch distribution classes\n\ncc @fritzo @neerajprad @alicanb @nikitaved",
        "open",
        "2025-02-03T19:47:59Z",
        null,
        null,
        "module: distributions, triaged, open source, topic: not user facing",
        "main",
        "features/distribution_generator",
        30,
        160,
        138,
        1,
        4,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146333"
    ],
    [
        146324,
        "[torch][amdsmi] Avoid ODR violation when loading amdsmi",
        "Summary:\namdsmi bundles its own copy of `libamd_smi.so`. When you're interacting with `amdsmi` from *only* python that's fine, but when you try to interact with `libamd_smi.so` from native code too this poses a problem, because from native code you'll be linking against the copy of `libamd_smi.so` from the SDK.\n\nThis means you'll end up with 2 copies of `libamd_smi.so` in your process, and potentially (Murphey's law says you will, as does our CI) violate ODR.\n\nIn order to avoid this issue from the PT side of the world we can hook the `dlopen(\"path/to/bundled/libamd_smi.so\")` and try to use the already loaded/SDK version of `libamd_smi.so` first, before proceeding to use the `path/to/bundled/libamd_smi.so`.\n\nTest Plan: CI, inspect process using libamd_smi.so from native + python and observe only a single copy loaded\n\nDifferential Revision: D69064038\n\n\n",
        "open",
        "2025-02-03T18:59:18Z",
        null,
        null,
        "fb-exported, ciflow/trunk, topic: not user facing",
        "main",
        "export-D69064038",
        1,
        41,
        1,
        1,
        14,
        1,
        "malfet, danzimm, malfet, malfet",
        "COMMENTED, COMMENTED, APPROVED, COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146324"
    ],
    [
        146321,
        "[ONNX] Support custom axis name through dynamic_shapes",
        "Fixes #143443\r\n\r\nThis PR aims to support custom dynamic axis naming through dynamic_shapes. Currently, _Dim and _DimHint do not support dynamic axis naming (#144273).\r\n\r\n1. **the original dynamic shapes guarantee**\r\nThe axis renaming is only applied when dynamic shapes include string instead of all _Dim and _DimHint. Thus, there will not be any inconsistent behavior to dynamic_shapes with torch.export.export if the given dynamic shapes follow torch.export.export format.\r\n2. _DimHint.AUTO is applied to the axes that are specified with custom names to avoid exporter crash. (_DimHint.DYNAMIC crashes when the export fails.)\r\n3.  There's no need to handle cases where kwargs are out of order with the model signature,\r\n    as torch.export.export supports dynamism only when kwargs and dynamic_shapes are provided in order.\r\n    https://github.com/pytorch/pytorch/blob/49082f9dba3b79a344cb03652972ddbe7c3729cc/torch/export/_trace.py#L2034\r\n4. If `torch.onnx.ExportedProgram` finds the axes share the same constraints, they will have the same name (e.g. s0, s1, ...). Therefore, even if the ONNX users specify them with different custom names, they won't be respected.\r\n\r\nExample model:\r\n```python\r\n        class NestedModel(torch.nn.Module):\r\n            def forward(\r\n                self,\r\n                x: torch.Tensor,\r\n                ys: list[torch.Tensor],\r\n                zs: dict[str, torch.Tensor],\r\n                c: torch.Tensor,\r\n            ):\r\n                y = ys[0] + ys[1] + zs[\"a\"] + zs[\"b\"]\r\n                w = 5\r\n                if x.shape[0] < 3 and c.shape[0] != 4:\r\n                    return x + w, x + y, c\r\n                else:\r\n                    return x - w, x - y, c\r\n\r\n        input = (\r\n            torch.ones(5),\r\n            [torch.zeros(5), torch.ones(5)],\r\n            {\"a\": torch.zeros(5), \"b\": torch.ones(5)},\r\n            torch.ones(6),\r\n        )\r\n\r\n        dynamic_shapes = (\r\n            {0: torch.export.Dim(\"dim_x\", min=3)},  # _Dim\r\n            [(\"custom_name_axis_ys_0\",), (torch.export.Dim.AUTO,)],  # custom name\r\n            {\r\n                \"a\": {0: torch.export.Dim.AUTO},\r\n                \"b\": (\"custom_name_axis_zs_b_0\",),\r\n            },  # _DimHint\r\n            {0: \"custom_name_axis_c_0\"},  # custom name\r\n        )\r\n\r\n```",
        "open",
        "2025-02-03T18:00:09Z",
        null,
        null,
        "open source, release notes: onnx, topic: new features",
        "main",
        "titaiwang/support_axis_name",
        6,
        722,
        247,
        9,
        1,
        0,
        "justinchuby, justinchuby, justinchuby, justinchuby, justinchuby, justinchuby, justinchuby, xadupre, xadupre, xadupre, xadupre, xadupre, justinchuby, titaiwangms, titaiwangms, titaiwangms, titaiwangms, titaiwangms, titaiwangms, titaiwangms, titaiwangms, titaiwangms, justinchuby, justinchuby",
        "COMMENTED, APPROVED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146321"
    ],
    [
        146319,
        "[export][dynamic shapes] use size-oblivious upper bound reasoning for backed symbols",
        "cc @ezyang @SherlockNoMad @EikanWang @jgong5 @wenzhe-nrv",
        "open",
        "2025-02-03T17:31:50Z",
        null,
        null,
        "release notes: fx, fx, ciflow/inductor",
        "main",
        "pianpwk/backed_symint_endofbounds",
        2,
        28,
        11,
        1,
        1,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146319"
    ],
    [
        146318,
        "Hack AC to not clear recomputed activations",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #146633\n* __->__ #146318\n* #145399\n* #145533\n* #145531\n* #145520\n\n",
        "open",
        "2025-02-03T17:29:36Z",
        null,
        null,
        "",
        "gh/soulitzer/351/base",
        "gh/soulitzer/351/head",
        1,
        2,
        2,
        1,
        2,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146318"
    ],
    [
        146313,
        "[Dynamo] fix torch._dynamo.assume_constant_result when used on class method",
        "This PR fixes `torch._dynamo.assume_constant_result` when it is used on a class method, and the class was instantiated inside of the dynamo traced code.\r\n\r\nThe issue: Currently when an object is modified by storing attributes on it in dynamo, the side effects of those attribute stores are tracked using the `SideEffect` system, and the modifications to the class are not performed in the first place.  For example, in\r\n```python\r\nclass A:\r\n    def __init__(self):\r\n        self.value = 123\r\n```\r\nThe `self.value` field will not be set on the class `A` but rather it will only be tracked within the `SideEffect` system.\r\n\r\nThis causes a problem with `torch._dynamo.assume_constant_result` as it converts the value in dynamo back into a normal python value and invokes the function as normal python.  However, it currently does not find the `self.value` field (as it was never set on the underlying object).  This PR checks if the object passed to the `torch._dynamo.assume_constant_result` has any pending mutations from the `SideEffect` system and applies them before calling the `torch._dynamo.assume_constant_result` function.\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-02-03T16:00:09Z",
        null,
        null,
        "triaged, open source, module: dynamo",
        "main",
        "dynamo-fixes-6",
        2,
        37,
        8,
        2,
        8,
        1,
        "anijain2305, StrongerXi, jansel, anijain2305",
        "APPROVED, COMMENTED, CHANGES_REQUESTED, COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146313"
    ],
    [
        146310,
        "Fix type stubs for SymmetricMemory",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146310\n* #146308\n\n",
        "open",
        "2025-02-03T14:11:18Z",
        null,
        null,
        "release notes: distributed (c10d)",
        "gh/lw/7/base",
        "gh/lw/7/head",
        1,
        35,
        4,
        1,
        1,
        1,
        "yifuwang",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146310"
    ],
    [
        146308,
        "Support SymmetricMemory's signaling kernels on sm60 and sm70",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #146310\n* __->__ #146308\n\nBy leveraging libcudacxx's utilities: https://nvidia.github.io/cccl/libcudacxx/extended_api/synchronization_primitives/atomic_ref.html\n\ncc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o",
        "open",
        "2025-02-03T13:35:20Z",
        null,
        null,
        "oncall: distributed, release notes: distributed (c10d)",
        "gh/lw/6/base",
        "gh/lw/6/head",
        3,
        36,
        58,
        1,
        1,
        0,
        "yifuwang",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146308"
    ],
    [
        146290,
        "[ c10d ] modify API to get device string from device with torch.device",
        "Modify the ```get_default_backend_for_device()``` API to extract the device string using ```torch.device()```\r\n\n\ncc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o",
        "open",
        "2025-02-03T03:24:49Z",
        null,
        null,
        "oncall: distributed, triaged, open source, release notes: distributed (c10d)",
        "main",
        "c10d_api_modification",
        1,
        1,
        1,
        1,
        2,
        0,
        "guangyey",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146290"
    ],
    [
        146289,
        "Use device agnostic APIs for device_count and backend in common_fsdp",
        "Replace device specific APIs with device abstracted API\r\n",
        "open",
        "2025-02-03T03:10:30Z",
        null,
        null,
        "triaged, open source, topic: not user facing",
        "main",
        "fsdp_common_cleanup",
        1,
        8,
        8,
        1,
        6,
        0,
        "guangyey",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146289"
    ],
    [
        146288,
        "[Trace PyDispatcher] Capture Vmapped autograd function as graph",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146288\n* #146272\n* #146271\n* #146270\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-02-03T02:14:22Z",
        null,
        null,
        "topic: not user facing, module: dynamo, ciflow/inductor",
        "gh/yanboliang/62/base",
        "gh/yanboliang/62/head",
        5,
        275,
        3,
        3,
        1,
        2,
        "yanboliang, zou3519, zou3519, zou3519, zou3519, zou3519, yanboliang, yanboliang, yanboliang, yanboliang, yanboliang, yanboliang, yanboliang, zou3519, zou3519, zou3519",
        "COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146288"
    ],
    [
        146285,
        "[scan] Autograd with partial gradient support",
        "This PR introduces the Autograd feature for scan with partial gradient support. It is a combination of the already opened PRs: https://github.com/pytorch/pytorch/pull/135631 and https://github.com/bohnstingl/pytorch/pull/4\r\n\r\ncc @ydwu4 ",
        "open",
        "2025-02-03T00:36:42Z",
        null,
        null,
        "triaged, open source, topic: not user facing",
        "main",
        "scan_autograd22",
        2,
        1324,
        212,
        4,
        2,
        1,
        "ydwu4",
        "COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146285"
    ],
    [
        146280,
        "[Inductor-CPU] Avoid redundant compute of index in AVX512 FP32 acc GEMM micro-kernel",
        "`constexpr int idx` doesn't seem to help here since `idx` is equal to `i`:\r\n\r\n```cpp\r\n        constexpr int row = i / COLS;\r\n        constexpr int col = i % COLS;\r\n\r\n       // some other code\r\n\r\n        constexpr int idx = row * COLS + col;\r\n        vc[idx] = at::vec::fmadd(va, vb[col], vc[idx]);\r\n```\r\n\r\nTODO\r\n- [ ] Although it's known at the time of compilation as to what various values of `i` would be due to forced-unrolling of `compute` lambda calls, check if the compiler really computes values of `row`, `col` and `idx` corresponding to each value of `i` at compile-time.\r\n \r\n\r\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @desertfire @chauhang @aakhundov",
        "open",
        "2025-02-02T20:19:37Z",
        null,
        null,
        "open source, topic: not user facing, module: inductor, ciflow/inductor",
        "main",
        "sanchitintel/modify_fp32_micro_gemm",
        1,
        1,
        2,
        2,
        1,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146280"
    ],
    [
        146275,
        "Correctly handle duplicated arguments when merging input views.",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146275\n\nFix: #135099\n\nThis PR changes how we map the original inputs into the new set of\ninputs that take in the tensor input's base instead of their aliases.\n\n**Problem:** in order to create this mapping, we had a dictionary that\nmapped the hashed arguments into their respective indices. However, if\nthere's a group of equal arguments, we will have only one mapping for\nsuch an argument. This breaks the assumption that there will be one\nmapping for each argument.\n\n**Solution:** map the hashed arguments into a list of indices. Then, we\nwill be able to correctly reconstruct the parameters for the new calling\nconvention.",
        "open",
        "2025-02-02T17:20:19Z",
        null,
        null,
        "open source, topic: not user facing, ciflow/inductor",
        "gh/ysiraichi/82/base",
        "gh/ysiraichi/82/head",
        2,
        28,
        4,
        2,
        2,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146275"
    ],
    [
        146273,
        "[dcp] Minor improvements to filesystem writer",
        "- Apply same check to `_SerialCpuLoader ` from `_OverlappingCpuLoader`  for determining when to clone non-contiguous cpu tensors\r\n- Add minor helper function to avoid iterating over `WriteItem`s twice to collect bytes and tensor write items\r\n- Use the metadata filename constant instead of harcoding \r\n\r\ncc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o @LucasLLC @MeetVadakkanchery @mhorowitz @pradeepfn @ekr0",
        "open",
        "2025-02-02T07:47:24Z",
        null,
        null,
        "oncall: distributed, triaged, open source, topic: not user facing, module: distributed_checkpoint",
        "main",
        "dist-ckpt-clone-patch",
        1,
        16,
        6,
        4,
        2,
        0,
        "Skylion007, ananthsub, ananthsub",
        "COMMENTED, COMMENTED, COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146273"
    ],
    [
        146267,
        "Format tests by PYFMT",
        "Fixes #ISSUE_NUMBER\r\n\n\ncc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o @LucasLLC @MeetVadakkanchery @mhorowitz @pradeepfn @ekr0",
        "open",
        "2025-02-02T04:02:51Z",
        null,
        null,
        "oncall: distributed, triaged, open source, topic: not user facing, ciflow/inductor, module: distributed_checkpoint",
        "main",
        "ruff_import",
        8,
        104,
        78,
        2,
        1,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146267"
    ],
    [
        146265,
        "Add libtorch CUDA 12.8 ",
        "Trying removing sm50 and sm60 to get around the ld --relink error\r\nTest will fail but testing the build.\r\n\r\nhttps://github.com/pytorch/pytorch/issues/145570\r\n\n\ncc @ptrblck @msaroufim @eqy",
        "open",
        "2025-02-02T02:42:46Z",
        null,
        null,
        "module: cuda, triaged, open source, ciflow/binaries, topic: not user facing",
        "main",
        "cu128-libtorch-build",
        3,
        66,
        4,
        1,
        4,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146265"
    ],
    [
        146264,
        "[ROCm] opportunistic fastatomics for ReduceAdd operations for MI300 GPUs",
        "In this approach, we are catching any lane within a wave that is doing fastatomics to the same destination address and computing the sum on the CU. This is leading to 3x improvement in scatter_add performance and 2x improvement in index_select.\r\n\r\nco-authored by: @amd-hhashemi\r\n\r\nFixes #ISSUE_NUMBER\r\n\r\n\r\ncc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd",
        "open",
        "2025-02-02T00:23:33Z",
        null,
        null,
        "module: rocm, open source, topic: not user facing, ciflow/periodic, ciflow/unstable, ciflow/rocm",
        "main",
        "pg-scatter-add-dup-fix",
        5,
        63,
        1,
        6,
        3,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146264"
    ],
    [
        146248,
        "Make fx.node.map_arg() and .map_aggregate() generic",
        "## What's the problem?\r\n\r\nThe popular `fx.node.map_arg()` and `fx.node.map_aggregate()` apply operations recursively on `dict`s, `tuples`, `list`s, etc, and return a new collection of the same type.\r\n\r\nUnfortunately, their base input type is `Argument`, which is [very unspecific indeed](https://github.com/pytorch/pytorch/blob/5d55a6585d5806c2743e92118e663f5abb261895/torch/fx/node.py#L48-L58): most type information is just thrown away at the call site of either of these functions, as far as the type checker goes.\r\n\r\nAs `torch` moves to a more typed code base, this would force innocent, unsuspecting developers to add logically unnecessary casts or `# type: ignore` statements.\r\n\r\n## What's the solution?\r\n\r\nMaking these two `node.map_*` functions generic on the first argument and return type means that type information is preserved for the type checker. (The signature of the other parameter, the function that visits the nodes and subnodes, has not changed, nor should it.)\r\n\r\n## Won't it break everything?\r\n\r\nIt doesn't break the type checker - one place needed an extra hint.\r\n\r\nThere have been code breakages, resolved one, at least one new one... we'll see!\r\n\r\nStack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #146626\n* __->__ #146248\n\r\n\r\n\r\ncc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o @ezyang @malfet @xuzhao9 @gramster @SherlockNoMad @EikanWang @jgong5 @wenzhe-nrv @voznesenskym @penguinwu @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-02-01T18:09:17Z",
        null,
        null,
        "oncall: distributed, module: typing, open source, better-engineering, ciflow/trunk, release notes: fx, topic: not user facing, fx, module: dynamo, ciflow/inductor, suppress-api-compatibility-check, merging, suppress-bc-linter",
        "gh/rec/129/base",
        "gh/rec/129/head",
        4,
        53,
        56,
        14,
        7,
        0,
        "Skylion007, Skylion007, rec, Skylion007, XuehaiPan, XuehaiPan, Skylion007, rec, Skylion007, rec, rec, Skylion007",
        "COMMENTED, COMMENTED, COMMENTED, COMMENTED, CHANGES_REQUESTED, APPROVED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146248"
    ],
    [
        146247,
        "torch/nn/modules/conv.py: docs: improvements",
        "Fix highlighting in generated documentation (`torch/nn/modules/conv.py`):\r\n\r\n* attrs should be `:attrs:`,\r\n* constants should be constants,\r\n* text in math should be '\\text{}`.\r\n\r\nReborn of #136218.\r\n\r\n/cc @albanD, @jbschlosser, @mikaylagawarecki",
        "open",
        "2025-02-01T18:06:45Z",
        null,
        null,
        "triaged, open source, topic: not user facing",
        "main",
        "patch-2",
        1,
        106,
        106,
        1,
        2,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146247"
    ],
    [
        146244,
        "Simplify CUDA version checking on tests",
        "Since we require CUDA >=11.0\r\n\r\n\r\ncc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o",
        "open",
        "2025-02-01T14:37:48Z",
        null,
        null,
        "oncall: distributed, triaged, open source, release notes: distributed (c10d)",
        "main",
        "windows_tests2",
        5,
        7,
        17,
        1,
        1,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146244"
    ],
    [
        146237,
        "[2/N] Fix cppcoreguidelines-init-variables suppression",
        "Fixes #ISSUE_NUMBER\r\n",
        "open",
        "2025-02-01T09:07:08Z",
        null,
        null,
        "triaged, open source, topic: not user facing",
        "main",
        "fix_init_variable",
        9,
        10,
        19,
        2,
        2,
        0,
        "Skylion007, cyyever, cyyever",
        "COMMENTED, COMMENTED, COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146237"
    ],
    [
        146234,
        "[1/N] Fix F401 errors in tests",
        "Fixes #ISSUE_NUMBER\r\n",
        "open",
        "2025-02-01T05:05:54Z",
        null,
        null,
        "triaged, open source, topic: not user facing",
        "main",
        "win_tests",
        5,
        29,
        48,
        2,
        2,
        0,
        "Skylion007, cyyever, soulitzer, cyyever, albanD, cyyever",
        "COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146234"
    ],
    [
        146228,
        "[PT2][Inductor][reland] Add runtime numeric check for the post grad pass",
        "Summary: We observed compilation time regression with previous diff implementation D63438718. Here we fix the issue and reland the diff\n\nTest Plan:\n### numeric check enablement test\n\n```\nbuck2 run mode/opt //scripts/jackiexu0313/pt2:local_model_with_pt2 -- --test_mode batch  --use_synthetic_data --flow_id 685229965 -n\n```\n\n\n### compilation time check\n\n```\nbuck2 run mode/opt //caffe2/benchmarks/dynamo/fb:torchbench_run_nanogpt_training -- -m nanogpt -t training\n```\n\n```\ntorchbench_run\n    duration_ms: 219528\n    defaults-batch_size: 1\n    defaults-speedup-x1000: 1408\n    defaults-abs_latency-x1000: 29068\n    defaults-compilation_latency-x1000: 93996\n    defaults-compression_ratio-x1000: 924\n    defaults-eager_peak_mem-x1000: 2473\n    defaults-dynamo_peak_mem-x1000: 2675\n    defaults-calls_captured: 1156\n    defaults-unique_graphs: 3\n    defaults-graph_breaks: 8\n    defaults-unique_graph_breaks: 6\n    defaults-autograd_captures: 0\n    defaults-autograd_compiles: 0\n    defaults-cudagraph_skips: 0\n    cudagraphs-batch_size: 1\n    cudagraphs-speedup-x1000: 5065\n    cudagraphs-abs_latency-x1000: 7983\n    cudagraphs-compilation_latency-x1000: 76961\n    cudagraphs-compression_ratio-x1000: 1485\n    cudagraphs-eager_peak_mem-x1000: 4473\n    cudagraphs-dynamo_peak_mem-x1000: 3012\n    cudagraphs-calls_captured: 1154\n    cudagraphs-unique_graphs: 2\n    cudagraphs-graph_breaks: 4\n    cudagraphs-unique_graph_breaks: 4\n    cudagraphs-autograd_captures: 0\n    cudagraphs-autograd_compiles: 0\n    cudagraphs-cudagraph_skips: 0\n    cudagraphs_dynamic-batch_size: 1\n    cudagraphs_dynamic-speedup-x1000: 5038\n    cudagraphs_dynamic-abs_latency-x1000: 8334\n    cudagraphs_dynamic-compilation_latency-x1000: 22521\n    cudagraphs_dynamic-compression_ratio-x1000: 893\n    cudagraphs_dynamic-eager_peak_mem-x1000: 4017\n    cudagraphs_dynamic-dynamo_peak_mem-x1000: 4493\n    cudagraphs_dynamic-calls_captured: 1154\n    cudagraphs_dynamic-unique_graphs: 2\n    cudagraphs_dynamic-graph_breaks: 4\n    cudagraphs_dynamic-unique_graph_breaks: 4\n    cudagraphs_dynamic-autograd_captures: 0\n    cudagraphs_dynamic-autograd_compiles: 0\n    cudagraphs_dynamic-cudagraph_skips: 0\n```\n\n\n```\nservicelab create benchmark_torchbench_run_nanogpt_training -d D68979204\n```\n\nSuccessfully submitted experiment: https://www.internalfb.com/servicelab/experiment/4800587892/\n\nDifferential Revision: D68979204\n\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @desertfire @chauhang @aakhundov",
        "open",
        "2025-02-01T01:13:15Z",
        null,
        null,
        "fb-exported, module: inductor, ciflow/inductor, release notes: inductor, ci-no-td",
        "main",
        "export-D68979204",
        5,
        92,
        25,
        1,
        5,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146228"
    ],
    [
        146227,
        "[ROCm][TunableOp] Add bias data type to TunableOp signature.",
        "Fixes #ISSUE_NUMBER\r\n\n\ncc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang",
        "open",
        "2025-02-01T01:09:32Z",
        null,
        null,
        "module: rocm, open source, topic: not user facing, ciflow/rocm",
        "main",
        "tunableop_fp8_bias",
        1,
        1,
        1,
        1,
        1,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146227"
    ],
    [
        146224,
        "[ONNX] Migrate torchlib into PyTorch",
        "Migrate torchlib and fix relevant implementation.\r\n\r\n1. Use the current ONNX IR based graph builder to run unit tests. Removed the eager evaluator mode tests because they are not relevant to pytorch\r\n2. Updated the builder to handle `Split` nodes correctly by supporting the num_outputs argument\r\n3. Simplified the torchlib registry to directly produce a list of OnnxDecompMeta, which can be consumed directly by the dispatcher\r\n4. Remove handling of traceable functions because all traceable functions are now trace only\r\n5. `torchvision` and `quantized_decomposed` ops are not included.\r\n\r\nFixes https://github.com/pytorch/pytorch/issues/139301\r\n\r\n## Next steps\r\nThe follow up PRs will decouple the implementation from ONNX Script type system\r\n",
        "open",
        "2025-02-01T00:54:16Z",
        null,
        null,
        "module: onnx, triaged, open source, ciflow/trunk, release notes: onnx, topic: new features, merging",
        "main",
        "justinchu/ghstack/torchlib",
        30,
        20652,
        435,
        30,
        6,
        0,
        "justinchuby, justinchuby, justinchuby, justinchuby, justinchuby, justinchuby, justinchuby, xadupre, xadupre, xadupre, xadupre, xadupre, xadupre, justinchuby, justinchuby, justinchuby, justinchuby, justinchuby, xadupre, justinchuby, justinchuby, justinchuby, justinchuby, justinchuby, justinchuby, titaiwangms, justinchuby, justinchuby, justinchuby, justinchuby",
        "COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146224"
    ],
    [
        146223,
        "What will happen?",
        null,
        "open",
        "2025-02-01T00:25:10Z",
        null,
        null,
        "topic: not user facing",
        "main",
        "malfet-patch-10",
        1,
        4,
        14,
        2,
        1,
        0,
        "malfet, Skylion007",
        "COMMENTED, COMMENTED",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146223"
    ],
    [
        146222,
        "[while_loop][inductor] support sym expression as cond_fn output",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146222\n\r\n\r\nAs titled. Previously, we only support tensor output of cond_fn, this PR changes to also allow a shape expr to be returned in cond_fn.\r\n\r\naoti generated output code looks like:\r\n```\r\nV0203 11:28:05.750000 2611693 torch/_inductor/compile_fx.py:1091] [1/0] [__output_code]     bool buf7_cond_result;\r\n....\r\n(while_loop_cond_graph_0_arg2_1_handle);\r\nV0203 11:27:59.336000 2611693 torch/_inductor/compile_fx.py:1091] [1/0] [__output_code]         buf7_cond_result = u0 + u1 < 10L;\r\nV0203 11:27:59.336000 2611693 torch/_inductor/compile_fx.py:1091] [1/0] [__output_code]         if (!buf7_cond_result) break;\r\n```\r\n\r\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @desertfire @chauhang @aakhundov",
        "open",
        "2025-02-01T00:23:46Z",
        null,
        null,
        "Merged, Reverted, ciflow/trunk, topic: not user facing, module: inductor, ciflow/inductor, ci-no-td",
        "gh/ydwu4/205/base",
        "gh/ydwu4/205/head",
        6,
        81,
        13,
        2,
        9,
        0,
        "desertfire",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146222"
    ],
    [
        146218,
        "[FSDP2][DEBUG] enforcing ReduceOp.SUM to avoid bug in ReduceOp.AVG",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\r\n* __->__ #146218\r\n\r\nworkaround for https://github.com/pytorch/pytorch/issues/144045 , but not sure if we should land\r\n\r\n\r\ncc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o",
        "open",
        "2025-01-31T23:34:19Z",
        null,
        null,
        "oncall: distributed, release notes: distributed (fsdp), ciflow/inductor",
        "gh/weifengpy/21/base",
        "gh/weifengpy/21/head",
        1,
        4,
        4,
        1,
        3,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146218"
    ],
    [
        146215,
        "Update Dependencies.cmake",
        "fix cmake if check error:\r\n\u201cUnknown arguments specified\u201d\r\n\r\nFixes #ISSUE_NUMBER\r\n",
        "open",
        "2025-01-31T23:14:29Z",
        null,
        null,
        "triaged, open source",
        "main",
        "patch-1",
        1,
        1,
        1,
        1,
        3,
        0,
        "Skylion007, longlene",
        "COMMENTED, COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146215"
    ],
    [
        146202,
        "[ROCm] follow up to #138964, remove work-around",
        "PR #138964 used #ifdef to skip non-contig tensor copies on ROCm due to failing tests.\r\n\n\ncc @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd",
        "open",
        "2025-01-31T21:42:15Z",
        null,
        null,
        "module: rocm, open source, release notes: cuda, ciflow/rocm",
        "main",
        "rocm_followup_128964",
        1,
        0,
        4,
        1,
        2,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146202"
    ],
    [
        146199,
        "docs: change log to ln in Softplus function and class",
        "Updated the math formula in the softplus function in torch.nn.functional.py and the Softplus class in torch.nn.modules.activation.py from log to ln for correctness and accuracy.\r\n\r\n",
        "open",
        "2025-01-31T20:47:48Z",
        null,
        null,
        "triaged, open source",
        "main",
        "docs",
        2,
        2,
        2,
        1,
        3,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146199"
    ],
    [
        146192,
        "torch.check distributions",
        "Fixes #ISSUE_NUMBER\r\n",
        "open",
        "2025-01-31T19:46:52Z",
        null,
        null,
        "",
        "main",
        "angelayi/distribution",
        1,
        6,
        3,
        1,
        2,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146192"
    ],
    [
        146191,
        "[cuda] Speed up layernorm backward by ~13% by using warp shuffles for the 16x32 kernel invocation",
        "Before this PR we had 2 kernels:\r\n1. For blocksize=32x32, this kernel *only* used warp shuffles to do the reduction\r\n2. For blocksize=16x32, this kernel *only* used shared memory to do the reduction\r\n\r\nThis PR replaces those two kernels with a single generic kernel with template parameters for the block size.\r\n\r\n1. Uses template parameters for blockDim.x and blockDim.y.\r\n1. Uses those template parameters to do a partial final reduction in shared memory if needed (i.e. if blockDim.y > 32).\r\n1. Then, for the final 32 rows, it uses warp shuffles when we need to reduce 32 rows down to a single row.\r\n1. Uses slightly more shared memory to reduce bank conflicts when reading the transposed data in both cases\r\n\r\nWhen compared to the baseline 16x32 kernel, ncu shows lower latency:\r\n\r\n![image](https://github.com/user-attachments/assets/df4fe13a-31b8-42ef-bc5d-348b39ec21e5)\r\n\r\nncu shows much lower shared memory loads and stores:\r\n\r\n![image](https://github.com/user-attachments/assets/c233c712-e2b2-4038-a5e9-9acc45c6e5b9)\r\n\r\nncu shows lower cycle count:\r\n\r\n![image](https://github.com/user-attachments/assets/83e7e236-66f1-4d8f-a1e4-11368fb69d09)\r\n\r\nncu shows lower sync instructions:\r\n\r\n![image](https://github.com/user-attachments/assets/f5caba87-417b-43a0-a304-0abaecb093dc)\r\n\r\nFor the 32x32 kernel, nvcc in theory should optimize away the shared memory reduction loop completely and performance should be identical to the previous specialized kernel.",
        "open",
        "2025-01-31T19:45:47Z",
        null,
        null,
        "release notes: cuda",
        "main",
        "ln1",
        3,
        62,
        129,
        8,
        1,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146191"
    ],
    [
        146189,
        "[test]",
        "Fixes #ISSUE_NUMBER\r\n",
        "open",
        "2025-01-31T19:04:04Z",
        null,
        null,
        "release notes: releng",
        "main",
        "csl/build_test_more_procs",
        7,
        276,
        362,
        30,
        1,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146189"
    ],
    [
        146182,
        "[export] Allow bypassing version check with unsafe API.",
        "Summary:\nas title.\nhttps://fb.workplace.com/groups/1028545332188949/permalink/10024343514259357/\n\nTest Plan:\n```\nwith torch.export._unsafe_skip_version_check():\n    ep = torch.export.load(...)\n```\nCI\n\nDifferential Revision: D68791202\n\n\n",
        "open",
        "2025-01-31T18:22:22Z",
        null,
        null,
        "fb-exported, ciflow/trunk, release notes: export",
        "main",
        "export-D68791202",
        1,
        23,
        6,
        1,
        2,
        0,
        "angelayi",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146182"
    ],
    [
        146180,
        "[AOTI] Improve readability of package_cpp_only",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146180\n\nSummary: Made two improvements here: 1) Emit interface.cpp into a separate file instead of embedding it to the model code; 2) Add prefix to mark the generated files as model code or weights(constants).\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov",
        "open",
        "2025-01-31T17:53:29Z",
        null,
        null,
        "topic: improvements, module: inductor, ciflow/inductor, release notes: inductor",
        "gh/desertfire/535/base",
        "gh/desertfire/535/head",
        4,
        28,
        23,
        1,
        1,
        0,
        "angelayi",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146180"
    ],
    [
        146176,
        "[executorch hash update] update the pinned executorch hash",
        "Based on latest green in HUD https://hud.pytorch.org/hud/pytorch/executorch/main/1?per_page=50\n",
        "open",
        "2025-01-31T17:35:35Z",
        null,
        null,
        "ciflow/trunk, topic: not user facing, ciflow/inductor",
        "main",
        "et_pin_bump",
        2,
        2,
        2,
        1,
        6,
        0,
        "huydhn",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146176"
    ],
    [
        146173,
        "[CI] Get rid of UCC builds",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146173\n\nThere hasn't been any active development/testing of those in last 2 years",
        "open",
        "2025-01-31T17:09:55Z",
        null,
        null,
        "topic: not user facing",
        "gh/malfet/159/base",
        "gh/malfet/159/head",
        5,
        0,
        99,
        1,
        4,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146173"
    ],
    [
        146172,
        "Factory function support for NestedTensor",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146172\n* #146101\n* #145922\n* #141842\n* #141841\n* #146052\n\r\nRebase of https://github.com/pytorch/pytorch/pull/117904 removing unnecessary bits now that python nested int already holds the necessary metadata.\r\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-01-31T17:03:29Z",
        null,
        null,
        "release notes: nested tensor, module: dynamo, ciflow/inductor",
        "gh/soulitzer/350/base",
        "gh/soulitzer/350/head",
        16,
        229,
        7,
        6,
        1,
        0,
        "ezyang, Skylion007",
        "APPROVED, COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146172"
    ],
    [
        146171,
        "Noob attempt at tensor_pointer_to_tensor_handle accepting const",
        "Fairly certain this will fail lint but is there a reason creating an AtenTensorHandle is not const preserving?\r\n\r\nStack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\r\n* __->__ #146171\r\n\r\n",
        "open",
        "2025-01-31T16:35:50Z",
        null,
        null,
        "ciflow/inductor, release notes: inductor",
        "gh/janeyx99/221/base",
        "gh/janeyx99/221/head",
        1,
        5,
        0,
        1,
        1,
        0,
        "desertfire",
        "APPROVED",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146171"
    ],
    [
        146145,
        "[CUDAEvent.h] support cuda events in cudagraphs",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146145\n\n",
        "open",
        "2025-01-31T09:36:23Z",
        null,
        null,
        "release notes: cuda",
        "gh/nmacchioni/39/base",
        "gh/nmacchioni/39/head",
        5,
        104,
        5,
        9,
        7,
        0,
        "ngimel, nmacchioni, galv",
        "COMMENTED, COMMENTED, COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146145"
    ],
    [
        146143,
        "Fix C++20 build errors",
        "Without breaking C++17.\r\n\r\n\r\ncc @EikanWang @jgong5 @wenzhe-nrv @sanchitintel",
        "open",
        "2025-01-31T08:21:24Z",
        null,
        null,
        "oncall: jit, triaged, open source, NNC, release notes: jit",
        "main",
        "cxx20_error",
        5,
        30,
        2,
        4,
        3,
        1,
        "albanD, cyyever, albanD, swolchok, cyyever, cyyever, malfet, malfet, malfet, cyyever",
        "COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, CHANGES_REQUESTED, COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146143"
    ],
    [
        146142,
        "Fix condition number invertible input(s) documented results",
        "`torch.linalg.cond` documentation states a singular input raises a RuntimeError, though unit tests show it in fact returns `inf` (https://github.com/pytorch/pytorch/blob/main/test/test_linalg.py#L1576).\r\n\r\n Fixes the documentation and adds an example.\r\n\r\nIt appears earlier documentation reflected this behavior (https://github.com/pytorch/pytorch/pull/45832/files/9008c10d63e7f5ddd0f06bbd5c7f1548c945d917#diff-316ce439a56491298e2d98deeca82606c52e5bde2f1ceb16c534ec03386c817eR358) \r\n\r\nand then got updated here: https://github.com/pytorch/pytorch/commit/d578e8cfa2db71e45c3565b42ff2b10d13643402.\r\n",
        "open",
        "2025-01-31T07:29:14Z",
        null,
        null,
        "triaged, open source, release notes: linalg_frontend",
        "main",
        "redwrasse/linalg-cond-non-invertible-err",
        1,
        1,
        2,
        2,
        8,
        2,
        "lezcano, redwrasse, lezcano",
        "CHANGES_REQUESTED, COMMENTED, COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146142"
    ],
    [
        146135,
        "WIP: async compile",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146135\n* #146134\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @desertfire @chauhang @aakhundov",
        "open",
        "2025-01-31T05:36:47Z",
        null,
        null,
        "module: inductor, ciflow/inductor",
        "gh/aorenste/214/base",
        "gh/aorenste/214/head",
        2,
        340,
        0,
        5,
        2,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146135"
    ],
    [
        146134,
        "Subprocess compile",
        "Add a mode to `fx_codegen_and_compile()` to compile in a separate process. This is to prepare for async compile where we'll compile and run eager in parallel (and also be able to move the compile phase to a remote computer).\r\n\r\nAdded a test based which runs the test_torchinductor tests with subprocess compiling turned on.\r\n\r\nStack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\r\n* __->__ #146134\r\n\r\n\r\n\r\ncc @ezyang @SherlockNoMad @EikanWang @jgong5 @wenzhe-nrv @voznesenskym @penguinwu @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @desertfire @chauhang @aakhundov",
        "open",
        "2025-01-31T05:36:42Z",
        null,
        null,
        "release notes: fx, fx, module: inductor, ciflow/inductor",
        "gh/aorenste/213/base",
        "gh/aorenste/213/head",
        7,
        615,
        58,
        6,
        1,
        1,
        "jamesjwu, jamesjwu",
        "COMMENTED, APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146134"
    ],
    [
        146133,
        "Apply ruff fixes to torch/**/*py",
        "Fixes #ISSUE_NUMBER\r\n\n\ncc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o @EikanWang @jgong5 @wenzhe-nrv @sanchitintel @ezyang @SherlockNoMad @voznesenskym @penguinwu @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @desertfire @chauhang @aakhundov",
        "open",
        "2025-01-31T05:10:03Z",
        null,
        null,
        "oncall: distributed, oncall: jit, triaged, open source, release notes: quantization, fx, module: inductor, module: dynamo, ciflow/inductor, release notes: export",
        "main",
        "ruff_fix",
        30,
        92,
        123,
        1,
        1,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146133"
    ],
    [
        146113,
        "Fix logging and test files which misspell \"precision\"",
        "Noticed this while working on something, decided to submit a quick fix.",
        "open",
        "2025-01-31T00:51:57Z",
        null,
        null,
        "ciflow/trunk, release notes: linalg_frontend, merging",
        "main",
        "spell",
        2,
        2,
        2,
        1,
        7,
        0,
        "drisspg",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146113"
    ],
    [
        146110,
        "[scan] Corrections for scan",
        "This PR resolves some minor issues with the scan HOP and unifies the handling of the additional_inputs in the same way as for associative_scan.\r\n\r\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @ydwu4 ",
        "open",
        "2025-01-31T00:26:28Z",
        null,
        null,
        "triaged, open source, topic: not user facing, module: dynamo",
        "main",
        "scan_hop_fixes",
        3,
        25,
        39,
        3,
        4,
        0,
        "ydwu4, bohnstingl, bohnstingl, Skylion007",
        "COMMENTED, COMMENTED, COMMENTED, COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146110"
    ],
    [
        146109,
        "cpp_wrapper: fix inductor triton tests",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #146452\n* #146706\n* #146424\n* __->__ #146109\n* #146449\n* #144349\n* #144293\n* #144002\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @desertfire @chauhang @aakhundov",
        "open",
        "2025-01-31T00:12:22Z",
        null,
        null,
        "open source, topic: not user facing, module: inductor, ciflow/inductor",
        "gh/benjaminglass1/63/base",
        "gh/benjaminglass1/63/head",
        5,
        75,
        44,
        7,
        1,
        0,
        "benjaminglass1",
        "COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146109"
    ],
    [
        146104,
        "[DO NOT MERGE] Testing C2 MI300 cluster.",
        "This PR is to test the stability of the C2 MI300x cluster.\n\ncc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd",
        "open",
        "2025-01-30T23:49:22Z",
        null,
        null,
        "module: rocm, open source, topic: not user facing, ciflow/unstable",
        "main",
        "test-c2",
        3,
        11,
        8,
        1,
        1,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146104"
    ],
    [
        146101,
        "(WIP) Update NJT ops to check data for raggedness check",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #146172\n* __->__ #146101\n* #145922\n* #141842\n* #141841\n* #146052\n\r\n\r\nNext:\r\n   - make sure guards are okay\r\n\r\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-01-30T23:22:38Z",
        null,
        null,
        "release notes: nested tensor, module: dynamo, ciflow/inductor",
        "gh/soulitzer/349/base",
        "gh/soulitzer/349/head",
        11,
        430,
        74,
        7,
        2,
        0,
        "albanD, soulitzer, soulitzer, soulitzer, soulitzer",
        "COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146101"
    ],
    [
        146098,
        "Move get accelerator to use build time flags when possible",
        "This PR does two main things (they are in a single PR to show how the newly added APIs are used).\r\n\r\n- Add isBuilt and isAvailable APIs to the AcceleratorHook interface. See inline doc for their exact semantic\r\n- Use the newly added isBuilt for accelerator check to ensure it does not poison fork\r\n\r\n\r\ncc @egienvalue we should do an MTIA patch for this and move to compile-time check once we figure out the CUDA+MTIA binary situation\r\ncc @guangyey we would need to add these APIs to the HPU backend (which I don't have access to) and we can move it to be compile time as well to avoid initialization.",
        "open",
        "2025-01-30T23:13:45Z",
        null,
        null,
        "Merged, Reverted, ciflow/trunk, release notes: python_frontend, topic: bug fixes, ciflow/mps, ciflow/xpu, ci-no-td",
        "main",
        "acc_clean",
        7,
        78,
        28,
        11,
        12,
        0,
        "ngimel, janeyx99, janeyx99, janeyx99, janeyx99, malfet, egienvalue, albanD, EikanWang",
        "APPROVED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, APPROVED, COMMENTED, COMMENTED, APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146098"
    ],
    [
        146093,
        "TEST3",
        "Fixes #ISSUE_NUMBER\r\n\n\ncc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o",
        "open",
        "2025-01-30T22:36:55Z",
        null,
        null,
        "oncall: distributed, release notes: releng, ciflow/inductor",
        "main",
        "zainr/mypy-break-test3",
        8,
        23,
        18,
        4,
        1,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146093"
    ],
    [
        146092,
        "TEST2",
        "Fixes #ISSUE_NUMBER\r\n\n\ncc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o",
        "open",
        "2025-01-30T22:36:11Z",
        null,
        null,
        "oncall: distributed, release notes: releng, ciflow/inductor",
        "main",
        "zainr/mypy-break-test2",
        8,
        25,
        18,
        2,
        1,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146092"
    ],
    [
        146091,
        "[WIP] TEST 1",
        "Fixes #ISSUE_NUMBER\r\n\n\ncc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o",
        "open",
        "2025-01-30T22:35:18Z",
        null,
        null,
        "oncall: distributed, release notes: releng, ciflow/inductor",
        "main",
        "zainr/mypy-break-test",
        8,
        26,
        18,
        1,
        1,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146091"
    ],
    [
        146090,
        "Fix TestDataLoader.test_segfault unexpected success on Aarch64",
        "TestDataLoader.test_segfault gives unexpected success on linux Aarch64",
        "open",
        "2025-01-30T22:35:15Z",
        null,
        null,
        "triaged, open source, release notes: dataloader",
        "main",
        "dataloader",
        3,
        18,
        8,
        2,
        2,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146090"
    ],
    [
        146073,
        "Nccl update to 2.25.1 for cuda 11.8-12.6",
        "Should resolve: https://github.com/pytorch/pytorch/issues/144768\r\nWe build one common nccl version for all cuda builds: ``NCCL_VERSION=v2.25.1-1``\r\nFor CUDA 11.8 we package this wheel instead of installing it from pypi.\r\n\r\n",
        "open",
        "2025-01-30T21:09:07Z",
        null,
        null,
        "topic: not user facing, ciflow/binaries_wheel",
        "main",
        "nccl_module_test",
        15,
        174,
        177,
        14,
        4,
        1,
        "Skylion007, atalman, atalman, malfet",
        "APPROVED, COMMENTED, COMMENTED, APPROVED",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146073"
    ],
    [
        146069,
        "Set /NODEFAULTLIB:vcomp for MSVC when linking caffe2::mkl with libiomp5md.lib ",
        "Fixes:\r\n- https://github.com/pytorch/pytorch/issues/113490\r\n\r\nThe PR sets `/NODEFAULTLIB:vcomp` link flag when linking caffe2::mkl with libiomp5md.lib.\r\n\r\nThe changes have been verified by checking build output with `VERBOSE=1`, for example:\r\n```\r\nC:\\PROGRA~1\\MICROS~1\\2022\\COMMUN~1\\VC\\Tools\\MSVC\\1442~1.344\\bin\\Hostx64\\x64\\link.exe /nologo caffe2\\CMakeFiles\\torch_global_deps.dir\\__\\torch\\csrc\\empty.c.obj /out:bin\\torch_global_deps.dll /implib:lib\\torch_global_deps.lib /pdb:bin\\torch_global_deps.pdb /dll /version:0.0 /machine:x64 /ignore:4049 /ignore:4217 /ignore:4099 /debug /INCREMENTAL:NO /NODEFAULTLIB:vcomp -LIBPATH:\\lib -LIBPATH:\\lib\\intel64 -LIBPATH:\\lib\\intel64_win -LIBPATH:\\lib\\win-x64 C:\\lib\\mkl_intel_lp64.lib C:\\lib\\mkl_intel_thread.lib C:\\lib\\mkl_core.lib C:\\lib\\libiomp5md.lib kernel32.lib user32.lib gdi32.lib winspool.lib shell32.lib ole32.lib oleaut32.lib uuid.lib comdlg32.lib advapi32.lib /MANIFEST:EMBED,ID=2\r\n```\r\n\r\n\r\ncc @malfet @seemethere @peterjc123 @mszhanyi @skyline75489 @nbcsm @iremyux @Blackhex",
        "open",
        "2025-01-30T20:33:33Z",
        null,
        null,
        "module: build, module: windows, triaged, open source, ciflow/trunk, release notes: build, topic: bug fixes",
        "main",
        "pytorch-113490",
        1,
        3,
        0,
        1,
        4,
        1,
        "malfet",
        "CHANGES_REQUESTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146069"
    ],
    [
        146068,
        "Error handling for launcher method in CachingAutotuner",
        "Fixes #146018\r\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang @aakhundov",
        "open",
        "2025-01-30T20:25:52Z",
        null,
        null,
        "triaged, open source, function request, topic: not user facing, module: inductor",
        "main",
        "error_handling_caching_autotuner",
        2,
        178,
        1,
        2,
        10,
        1,
        "eellison",
        "CHANGES_REQUESTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146068"
    ],
    [
        146064,
        "[PT2] Support add/remove passes in pre_grad",
        "Summary:\nsupport the same functionality with acc_tracer disabled, add a new config for pre_grad add/remove_passes, at the front end it still uses the same interface\n\nsome minor updates in pre_grad passes to make sure the passes are run in desired order, after added passes, still run pass like remove_noops at the end\n\nTest Plan: add new UT, please see stacked diff for add pass tests (TODO: update diff link)\n\nDifferential Revision: D68909278\n\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang @aakhundov",
        "open",
        "2025-01-30T19:17:34Z",
        null,
        null,
        "fb-exported, ciflow/trunk, topic: not user facing, module: inductor, ciflow/inductor",
        "main",
        "export-D68909278",
        3,
        77,
        94,
        1,
        3,
        0,
        "frank-wei",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146064"
    ],
    [
        146063,
        "[wip] torch._dynamo.disable on the CA graph",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #146063\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @yf225",
        "open",
        "2025-01-30T19:14:34Z",
        null,
        null,
        "module: dynamo, ciflow/inductor, module: compiled autograd",
        "gh/xmfan/166/base",
        "gh/xmfan/166/head",
        2,
        122,
        0,
        1,
        2,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/146063"
    ],
    [
        146061,
        "[inductor][triton] Fix average pool nd for int64 dtype",
        "The eager mode implementation of average pool nd returns an integer tensor if the input is also an integer tensor. This should also be preserved in inductor.\r\n\r\nFixes pytest -k test_comprehensive_nn_functional_avg_pool2d_cpu_int64 error: Triton compilation failed: triton_poi_fused_avg_pool2d_0\r\n\r\nSee WIP https://github.com/pytorch/pytorch/pull/145865#issuecomment-26200289890 to potentially enable such tests as they aren't enabled yet.\r\n\r\nFixes #ISSUE_NUMBER\r\n\r\n\r\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang @aakhundov",
        "open",
        "2025-01-30T18:54:43Z",
        null,
        null,
        "triaged, open source, topic: not user facing, module: inductor, ciflow/inductor",
        "main",
        "mwizak/fix-avg-pool-int64-dtype",
        3,
        29,
        5,
        1,
        3,
        1,
        "eellison",
        "COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146061"
    ],
    [
        146055,
        "[Win][CD] Install cmake and setuptools from PyPI",
        "And also avoid repeating the same command over and over\r\n",
        "open",
        "2025-01-30T18:14:34Z",
        null,
        null,
        "topic: not user facing, ciflow/binaries_wheel",
        "main",
        "malfet-patch-8",
        1,
        10,
        9,
        3,
        1,
        2,
        "Skylion007, atalman, seemethere",
        "APPROVED, APPROVED, APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146055"
    ],
    [
        146051,
        "inductor.config.descriptive_names = False is not actually supported (#145523) (#145523)",
        "Summary:\nThis config is not supported (it throws an error when set), and doesn't really make sense imo.\n\nApproved by: https://github.com/eellison\n\nTest Plan: contbuild & OSS CI, see https://hud.pytorch.org/commit/pytorch/pytorch/edf266e9bbbf6063f7c4a336ffb50234e11a0a82\n\nDifferential Revision: D68846308\n\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang @aakhundov",
        "open",
        "2025-01-30T17:52:47Z",
        null,
        null,
        "fb-exported, ciflow/trunk, topic: docs, module: inductor, ciflow/inductor, release notes: inductor",
        "main",
        "export-D68846308",
        1,
        2,
        3,
        1,
        4,
        0,
        "Skylion007",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/146051"
    ],
    [
        145999,
        "[CUDA] Optimize CUDA occupancy for indexing operators like index_select, index_add and index_reduce",
        "### Background\r\nFor indexing operations such as **torch.index_select**, **torch.index_add**, and **torch.index_reduce**, GPU performance is relatively low when handling large input sizes. On an A100 GPU, `torch.index_select` achieves only about 30% of the theoretical memory bandwidth for large inputs. Notably, `torch.index_select` and `torch.index_add` are among the most time-consuming operations in the forward and backward passes of `torch.nn.Embedding`, which is widely used in NLP and deep learning ranking models (DLRM).\r\n\r\nUpon analysis, I identified the following line of code`dim3 largeIndexGrid(std::min(ceil_div(sourceTotalSize, (uint64_t)128), (uint64_t)(mpc * 8)))`. Here, the blockSize is fixed at 128, and the number of blocks is set to `mpc * 8`, where mpc represents the number of streaming multiprocessors (SMs). This configuration results in only 1024 threads per SM (128 \u00d7 8). However, on an A100 GPU, each SM supports up to 2048 concurrent threads, meaning that the theoretical occupancy is limited to 50%.\r\n\r\nThis commit improves grid dimension calculation by leveraging maxThreadsPerMultiProcessor and blockSize, aligning with best practices commonly used in other GPU kernels in PyTorch.\r\n\r\n### Tests Performed\r\n1. Correctness testing: All tests in `test/test_torch.py` passed, including numerous tests covering `torch.index_select`, `torch.index_add` and `torch.index_reduce`. \r\n2. Performance testing: I run performance testing with the  following [script ](https://github.com/YyWangCS/FairySpeed/blob/main/embedding/bench_index_ops.py)on A100, the performance number is as follows. \r\n\r\n#### torch.index_select\r\n\r\n| num_embedding | embedding_dim | input_size | kernel latency before optimization (\u00b5s) | kernel latency after optimization (\u00b5s) |\r\n| ------------- | ------------- | ---------- | --------------------------------------- | -------------------------------------- |\r\n| 1000000       | 128           | 307200     | 518.3                                   | 359.4                                  |\r\n| 1000000       | 32            | 307200     | 141.4                                   | 97.3                                   |\r\n| 1000000       | 128           | 204800     | 347.5                                   | 242.4                                  |\r\n| 1000000       | 32            | 204800     | 96.2                                    | 66.2                                   |\r\n| 128000        | 4096          | 4096       | 219.2                                   | 158.6                                  |\r\n\r\n#### torch.index_add\r\n\r\n| num_embedding | embedding_dim | input_size | kernel latency before optimization (\u00b5s) | kernel latency after optimization (\u00b5s) |\r\n| ------------- | ------------- | ---------- | --------------------------------------- | -------------------------------------- |\r\n| 1000000       | 128           | 307200     | 526.8                                   | 379.4                                  |\r\n| 1000000       | 32            | 307200     | 143.4                                   | 103.3                                  |\r\n| 1000000       | 128           | 204800     | 352.2                                   | 256.1                                  |\r\n| 1000000       | 32            | 204800     | 98.9                                    | 69.9                                   |\r\n| 128000        | 4096          | 4096       | 222.8                                   | 165.0                                  |\r\n\r\n#### torch.index_reduce\r\n\r\n| num_embedding | embedding_dim | input_size | kernel latency before optimization (\u00b5s) | kernel latency after optimization (\u00b5s) |\r\n| ------------- | ------------- | ---------- | --------------------------------------- | -------------------------------------- |\r\n| 1000000       | 128           | 307200     | 732.5                                   | 470.7                                  |\r\n| 1000000       | 32            | 307200     | 197.5                                   | 126.6                                  |\r\n| 1000000       | 128           | 204800     | 490.8                                   | 316.4                                  |\r\n| 1000000       | 32            | 204800     | 133.7                                   | 86.1                                   |\r\n### Reference\r\n[Performance Optimization of Embedding Computation on GPU Part 1: GPU Occupancy Optimization](https://yywangcs.notion.site/Performance-Optimization-of-Embedding-Computation-on-GPU-Part-1-GPU-Occupancy-Optimization-178fc9f5d805800e91b6d4490afcc665)",
        "open",
        "2025-01-30T01:14:03Z",
        null,
        null,
        "triaged, open source, release notes: cuda",
        "main",
        "YyWangCS/opt_embedding",
        1,
        25,
        7,
        4,
        2,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145999"
    ],
    [
        145992,
        "fix indirect broadcast",
        "Fixes #142250\r\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang @aakhundov",
        "open",
        "2025-01-30T00:27:28Z",
        null,
        null,
        "topic: not user facing, module: inductor, ciflow/inductor",
        "main",
        "findhao/fix-indirect-access",
        2,
        37,
        1,
        2,
        1,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/145992"
    ],
    [
        145990,
        "[Profiler] Add Full PG ranks to Metadata",
        "Summary: We only add a shortened list of PG ranks if there is a job distributed across multiple nodes. Let's add the PG ranks to the JSON metadata\n\nTest Plan:\nhttps://www.internalfb.com/intern/perfdoctor/trace_view?filepath=tree/traces/dynocli/devvm2185.cco0.facebook.com/rank-0.Jan_29_16_19_52.2285734.pt.trace.json.gz&bucket=gpu_traces\n{F1974810517}\n\nDifferential Revision: D68867518\n\n\n",
        "open",
        "2025-01-30T00:25:11Z",
        null,
        null,
        "enhancement, fb-exported, release notes: profiler",
        "main",
        "export-D68867518",
        1,
        6,
        0,
        1,
        7,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145990"
    ],
    [
        145979,
        "[draft_export] better stack logging for strict mode",
        "Strict-mode draft export tends to log unhelpful stack traces for guards/data-dependent errors, relying on `CapturedTraceback.extract()`, which is only accurate for non-strict. For dynamo, it's better to use `TracingContext.extract_stack()` and fallback to the former when this is empty, avoiding traces pointing to the top-level export call, or lambdas (in the case of `torch._check` calls).\r\n\r\ne.g. before, for `test_draft_export.py -k test_offsets`:\r\n```\r\n    This occurred at the following stacktrace: \r\n        File /data/users/pianpwk/pytorch/test/export/test_draft_export.py, lineno 259, in test_offsets:\r\n        `ep, report = draft_export(M(), inp, strict=True)`\r\n```\r\nafter:\r\n```\r\n    This occurred at the following stacktrace: \r\n        File /data/users/pianpwk/pytorch/test/export/test_draft_export.py, lineno 254, in forward:\r\n            `if a == 0:`\r\n```\n\ncc @ezyang @SherlockNoMad @EikanWang @jgong5 @wenzhe-nrv",
        "open",
        "2025-01-29T23:04:26Z",
        null,
        null,
        "ciflow/trunk, fx, ciflow/inductor, release notes: export",
        "main",
        "pianpwk/draft_strict_stack",
        2,
        44,
        30,
        1,
        2,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145979"
    ],
    [
        145969,
        "Test of triton.compile in worker processes",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #145969\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang @aakhundov",
        "open",
        "2025-01-29T20:57:45Z",
        null,
        null,
        "module: inductor, ciflow/inductor",
        "gh/jamesjwu/100/base",
        "gh/jamesjwu/100/head",
        5,
        66,
        58,
        3,
        2,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/145969"
    ],
    [
        145966,
        "[BE] Upgrade to mypy 1.14",
        "Upgrade mypy version\r\n\r\ncc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o",
        "open",
        "2025-01-29T20:48:32Z",
        null,
        null,
        "oncall: distributed, release notes: releng, ciflow/inductor",
        "main",
        "zainr/mypy-update",
        11,
        45,
        31,
        7,
        2,
        0,
        "Skylion007, Skylion007, Skylion007, ZainRizvi, Skylion007, Skylion007, Skylion007, Skylion007",
        "COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, CHANGES_REQUESTED, COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145966"
    ],
    [
        145957,
        "Fix invalid nested int guarding in broadcast_shapes()",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #145957\n\r\nFixes #145874\r\n\r\nThis PR takes the approach of updating the logic determining whether multiple shapes broadcast together to handle nested ints specially.\r\n\r\nPossible alternative approach: don't update `broadcast_shapes()` + indicate that e.g. `Ne(j0, 1)` should statically evaluate to False. I briefly tried this but it wasn't straightforward. Is it better?",
        "open",
        "2025-01-29T19:40:37Z",
        null,
        null,
        "topic: not user facing",
        "gh/jbschlosser/227/base",
        "gh/jbschlosser/227/head",
        2,
        57,
        7,
        2,
        6,
        0,
        "bobrenjc93, jbschlosser, soulitzer",
        "COMMENTED, COMMENTED, COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145957"
    ],
    [
        145955,
        "Add MPS OpInfo db, rework test_mps to use OpInfo",
        "Infrastructure changes that will help enable: https://github.com/pytorch/pytorch/pull/142202",
        "open",
        "2025-01-29T19:29:09Z",
        null,
        null,
        "triaged, open source, release notes: mps, ciflow/mps, keep-going",
        "main",
        "dev/skotapati/mps_op_db",
        5,
        877,
        1037,
        17,
        1,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145955"
    ],
    [
        145946,
        "[ROCm][TunableOp] hipblaslt tf32 support",
        "TF32 is supported by hipblaslt. Support added by #143549.  This PR expands integration to the TunableOp feature.\n\ncc @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd",
        "open",
        "2025-01-29T18:06:41Z",
        null,
        null,
        "module: rocm, open source, ciflow/trunk, topic: not user facing, ciflow/rocm",
        "main",
        "rocm_tunableop_tf32",
        2,
        15,
        2,
        3,
        2,
        0,
        "pruthvistony",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145946"
    ],
    [
        145942,
        "Enable fast qlinear_dynamic path for AArch64 through ACL directly",
        "This enables a fast path for eager mode dynamic quantization for AArch64 through Arm Compute Library (ACL) directly.\r\n\r\nContext: PR #126687 enabled an optimized implementation for `qlinear_dynamic` for AArch64 through ideep \u2192 oneDNN \u2192 ACL which improved performance by ~10x compared to the previous implementation. \r\n\r\nHowever, the current `qlinear_dynamic` path (ideep \u2192 oneDNN \u2192 ACL) suffers from high overhead due to the API friction between the stateless oneDNN API and the stateful ACL low-precision GEMM (`lowp_gemm`) API - for example, ACL's `lowp_gemm` objects cache information like weights reduction or weights in optimized memory format which oneDNN does not allow due to its stateless nature. Hence, ACL currently runs a (redundant) sum of columns and pre-transposition (to the gemm kernel's optimal format) for each GEMM operation. \r\n\r\nThis PR addresses the sub-optimalities above by integrating ACL directly with `qlinear_dynamic`. This approach yields an average speedup (averaged over context_lengths of 2^3 up to 2^9) of ~ 50% for `bert-base-uncased`, `bert-large-uncased`, `roberta-base`, `distilbert-base-uncased` with 16 threads on a Neoverse-V1 (with `transformers==4.48`) - See benchmark code below. To achieve this, we:\r\n* Use ACL which is already built with PyTorch as a shared library when `USE_MKLDNN_ACL` is set.\r\n* Add ACL to ATen's CPU include and dependency libs\r\n* Introduce `PackedLinearWeightsACL` (as a subclasses of `PackedLinearWeightsOnednn`) with an implementation of `qlinear_dynamic` that uses ACL directly, while `qlinear` still follows the oneDNN path.\r\n* A future PR will introduce a direct ACL implementation `qlinear` and will allow us to remove the dependence on `PackedLinearWeightsOnednn`\r\n\r\nNote, that the ACL `lowp_gemm` API changed slightly between v24.09 and v24.12 which will be the new version after #138889 - Hence, this PR targets the new version - v24.12.\r\n\r\nThe following code was used to benchmark `qlinear_dynamic` performance:\r\n```\r\n# SPDX-FileCopyrightText: Copyright 2025 Arm Limited and/or its affiliate <open-source-office@arm.com>\r\n# SPDX-License-Identifier: BSD-3-Clause\r\nimport torch\r\nfrom transformers import AutoModel, AutoConfig\r\nimport time\r\nimport numpy as np\r\nfrom argparse import ArgumentParser\r\n\r\nclass ModelArgumentParser(ArgumentParser):\r\n    def __init__(self) -> None:\r\n        super().__init__(description=\"huggingface model\")\r\n        self.add_argument(\"--context_length\",\r\n                            help=\"context length - number of input tokens\",\r\n                            type=int,\r\n                            default=64\r\n        )\r\n        self.add_argument(\"--model\",\r\n                            help=\"model checkpoint - i.e. 'bert-base-uncased'\",\r\n                            type=str,\r\n                            default=None)\r\n        self.add_argument(\"--iters\",\r\n                          help=\"benchmark iterations\",\r\n                          default=500)\r\n\r\nif __name__ == \"__main__\":\r\n    parser = ModelArgumentParser()\r\n    args = parser.parse_args()\r\n    model_name = args.model\r\n    config = AutoConfig.from_pretrained(model_name)\r\n    batch_size = 1\r\n    model = AutoModel.from_pretrained(model_name)\r\n    model = torch.quantization.quantize_dynamic(model, {torch.nn.Linear}, dtype=torch.qint8)\r\n    model.eval()\r\n    inputs = torch.randint(config.vocab_size, (batch_size, args.context_length), dtype=torch.long, device=\"cpu\")\r\n    times = []\r\n    with torch.no_grad():\r\n        # warmup\r\n        for _ in range(10):\r\n            model(inputs)\r\n        # benchmark\r\n        for _ in range(args.iters):\r\n            s = time.time_ns()\r\n            model(inputs)\r\n            times.append((time.time_ns() - s) / 1e6)\r\n\r\n    print(\"Model = \", model_name)         \r\n    print(\"Context Length = \", args.context_length)\r\n    print(\"Min (ms) = \", min(times))\r\n    print(\"Mean (ms) = \", np.mean(times))  \r\n```\r\n\r\n\r\n\r\nFixes #ISSUE_NUMBER\r\n\r\n\r\ncc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @malfet @snadampal @milpuz01",
        "open",
        "2025-01-29T17:19:32Z",
        null,
        null,
        "module: cpu, triaged, open source, module: arm, release notes: quantization, release notes: releng, ciflow/linux-aarch64",
        "main",
        "acl_qlinear_dynamic",
        9,
        471,
        15,
        2,
        6,
        1,
        "malfet",
        "COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145942"
    ],
    [
        145936,
        "`torch.tensordot`: performance improvements when contracting to a scalar.",
        "As per title.\r\nFixes https://github.com/pytorch/pytorch/issues/145731\r\n\r\nTouches only compute. The CPU overhead can potentially be further reduced.\r\n\r\nBefore:\r\n```python\r\nIn [3]: n = 512\r\n\r\nIn [4]: A = torch.rand(n, n)\r\n\r\nIn [5]: B = torch.rand(n, n)\r\n\r\nIn [6]: %timeit torch.tensordot(A, B, [[0, 1], [0, 1]])\r\n2.04 ms \u00b1 70 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\r\n\r\nIn [7]: %timeit torch.tensordot(A, B, [[0, 1], [1, 0]])\r\n2.85 ms \u00b1 191 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\r\n\r\nIn [8]: %timeit torch.tensordot(A, B, [[1, 0], [0, 1]])\r\n2.9 ms \u00b1 133 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\r\n\r\nIn [9]: %timeit torch.tensordot(A, B, [[1, 0], [1, 0]])\r\n4.07 ms \u00b1 262 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\r\n```\r\n\r\nAfter\r\n```python\r\nIn [2]: n = 512\r\n\r\nIn [3]: A = torch.rand(n, n)\r\n\r\nIn [4]: B = torch.rand(n, n)\r\n\r\nIn [5]: %timeit torch.tensordot(A, B, [[0, 1], [0, 1]])\r\n30.7 \u00b5s \u00b1 2.51 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10,000 loops each)\r\n\r\nIn [6]: %timeit torch.tensordot(A, B, [[0, 1], [1, 0]])\r\n141 \u00b5s \u00b1 6.52 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10,000 loops each)\r\n\r\nIn [7]: %timeit torch.tensordot(A, B, [[1, 0], [0, 1]])\r\n142 \u00b5s \u00b1 4.03 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10,000 loops each)\r\n\r\nIn [8]: %timeit torch.tensordot(A, B, [[1, 0], [1, 0]])\r\n62.8 \u00b5s \u00b1 4.31 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10,000 loops each)\r\n\r\n```\n\ncc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o",
        "open",
        "2025-01-29T16:18:48Z",
        null,
        null,
        "oncall: distributed, open source, ciflow/trunk, release notes: python_frontend, topic: performance, ciflow/inductor",
        "main",
        "nikitaved/tensordot",
        4,
        38,
        8,
        1,
        9,
        2,
        "albanD, albanD, nikitaved, nikitaved, nikitaved",
        "APPROVED, APPROVED, COMMENTED, COMMENTED, COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145936"
    ],
    [
        145935,
        "[CPU Stream] Add noop for CPU stream record_event() and wait_event()",
        "Summary: Adds wait_event and record_event endpoints to CPU stream in order to facilitate device-agnostic code. Both methods are noops.\n\nTest Plan: CI\n\nDifferential Revision: D68833927\n\n\n\n\ncc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10",
        "open",
        "2025-01-29T15:45:43Z",
        null,
        null,
        "module: cpu, fb-exported",
        "main",
        "export-D68833927",
        1,
        6,
        0,
        1,
        3,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145935"
    ],
    [
        145930,
        "[Test][Linalg][CUDA] Increase niter in test_svd_lowrank_cuda_float64",
        "A recent PR #143049 attempted to increase tolerances to make test passable. However, we are still seeing errors like:\r\n```\r\nTraceback (most recent call last):\r\n  File \"~git/pytorch/test/test_linalg.py\", line 2540, in test_svd_lowrank\r\n    run_subtest(None, size, (), device, torch.svd_lowrank, density=density)\r\n  File \"~git/pytorch/test/test_linalg.py\", line 2505, in run_subtest\r\n    self.assertEqual(A, a, rtol=1e-7, atol=2e-7)\r\n  File \"~git/pytorch/torch/testing/_internal/common_utils.py\", line 4044, in assertEqual\r\n    raise error_metas.pop()[0].to_error(  # type: ignore[index]\r\nAssertionError: Tensor-likes are not close!\r\n\r\nMismatched elements: 90 / 1000000 (0.0%)\r\nGreatest absolute difference: 7.795904016052784e-07 at index (176, 930) (up to 2e-07 allowed)\r\nGreatest relative difference: inf at index (6, 179) (up to 1e-07 allowed)\r\n```\r\nIncreasing `niter` parameter actually decreases numerical differences.\n\ncc @ptrblck @msaroufim @eqy @jianyuh @nikitaved @pearu @mruberry @walterddr @xwang233 @Lezcano",
        "open",
        "2025-01-29T14:22:18Z",
        null,
        null,
        "module: cuda, triaged, open source, module: linear algebra, topic: not user facing",
        "main",
        "increase_niter_in_test_svd_lowrank_cuda_float64",
        1,
        1,
        1,
        1,
        1,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145930"
    ],
    [
        145923,
        "Update mi300 labels to account for multiple clusters.",
        "We now have multiple Kubernetes clusters of mi300x resources, and this commit updates labels accordingly to target both clusters evenly.\n\ncc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd",
        "open",
        "2025-01-29T08:51:26Z",
        null,
        null,
        "module: rocm, triaged, open source, Merged, Reverted, topic: not user facing, ci-no-td",
        "main",
        "mi300-labels",
        2,
        20,
        20,
        1,
        11,
        0,
        "jeffdaily",
        "DISMISSED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145923"
    ],
    [
        145922,
        "Update NestedInt equality to take into account all metadata",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #146172\n* #146101\n* __->__ #145922\n* #141842\n* #141841\n* #146052\n\n\n\ncc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @ezyang @SherlockNoMad @EikanWang @wenzhe-nrv @voznesenskym @penguinwu @Guobing-Chen @zhuhaozhe @blzheng @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-01-29T08:50:27Z",
        null,
        null,
        "module: cpu, release notes: fx, fx, module: dynamo, ciflow/inductor",
        "gh/soulitzer/347/base",
        "gh/soulitzer/347/head",
        8,
        107,
        35,
        17,
        1,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145922"
    ],
    [
        145920,
        "Add basic Gaudi support to benchmarks/dynamo",
        "This PR adds basic Gaudi support to benchmarks/dynamo\r\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-01-29T08:28:00Z",
        null,
        null,
        "triaged, open source, topic: not user facing, oncall: pt2, module: dynamo",
        "main",
        "kfojcik/basic_hpu_dynamo_benchmark",
        1,
        9,
        1,
        1,
        4,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145920"
    ],
    [
        145917,
        "Draft: fix: Some smaller mingw fixes",
        "Fixes #ISSUE_NUMBER\r\n",
        "open",
        "2025-01-29T05:27:38Z",
        null,
        null,
        "open source",
        "main",
        "fix-mingw",
        2,
        4,
        4,
        1,
        3,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/145917"
    ],
    [
        145911,
        "Add future lazy clone setting and deprecate `torch.reshape` view",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #145911\n\n\n\ncc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @voznesenskym @penguinwu @EikanWang @Guobing-Chen @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @desertfire @chauhang @aakhundov @ColinPeppler",
        "open",
        "2025-01-29T03:27:56Z",
        null,
        null,
        "oncall: distributed, module: cpu, open source, release notes: python_frontend, module: inductor, module: dynamo, ciflow/inductor",
        "gh/kurtamohler/31/base",
        "gh/kurtamohler/31/head",
        30,
        470,
        48,
        6,
        2,
        0,
        "kurtamohler",
        "COMMENTED",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/145911"
    ],
    [
        145910,
        "Fix redundant move",
        "Fixes #ISSUE_NUMBER\r\n\n\ncc @EikanWang @jgong5 @wenzhe-nrv @sanchitintel",
        "open",
        "2025-01-29T03:21:22Z",
        null,
        null,
        "oncall: jit, open source, NNC, release notes: jit",
        "main",
        "move",
        1,
        2,
        2,
        1,
        1,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/145910"
    ],
    [
        145903,
        "[NJT] Add cumsum support for nested tensors",
        "Summary: - Add cumsum support for NT\n\nTest Plan: - added unit tests\n\nDifferential Revision: D68307097\n\n\n",
        "open",
        "2025-01-29T01:24:45Z",
        null,
        null,
        "fb-exported, topic: improvements, release notes: nested tensor",
        "main",
        "export-D68307097",
        2,
        62,
        0,
        1,
        8,
        0,
        "jbschlosser, jbschlosser, ketansingh, jbschlosser",
        "COMMENTED, COMMENTED, COMMENTED, COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145903"
    ],
    [
        145885,
        "Hacky solution to bad interaction between AOTAutogradcache and Triton 3.1",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #145885\n\n",
        "open",
        "2025-01-28T22:50:12Z",
        null,
        null,
        "topic: not user facing, ciflow/inductor",
        "gh/jamesjwu/97/base",
        "gh/jamesjwu/97/head",
        2,
        28,
        1,
        2,
        1,
        0,
        "bdhirsh, jamesjwu",
        "COMMENTED, COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145885"
    ],
    [
        145873,
        "[WIP] Allow generation of inductor backend specific tests using instantiate_device_type_tests ",
        "This allows the creation of inductor backend specific test classes. Since this is an extension point for out of tree backends, it also allows out of tree backends to customise test instantiation to fit their backend / device.\r\n\r\nTo maintain backwards compatibility, `only_inductor_backends` defaults to `None` so that the behaviour of test class instantiation matches the incumbent behaviour. If `only_inductor_backends` is not None, inductor backend specific test classes will be created from a test template, e.g. `TestInductorOpInfo -> TestInductorOpInfoTritonCUSTOMDEVICE, TestInductorOpInfoHalideCUSTOMDEVICE`\r\n\r\nAn illustration of the before/after changes:\r\n\r\n```python\r\n\r\n# in test_inductor.py\r\n# Inductor test template\r\nclass TestInductor:\r\n    def test_comprehensive(...)\r\n    \r\n# Original\r\ninstantiate_device_type_tests(TestSuiteTemplate)\r\n# Generates something like this:\r\n# TestInductorCPU\r\n# TestInductorCUDA\r\n\r\n# After changes\r\ninstantiate_device_type_tests(TestSuiteTemplate, enable_inductor_backend_classes=True, only_inductor_backends=[\"cpp\", \"triton\"])\r\n# TestInductorCppCPU\r\n# TestInductorCppTriton\r\n# TestInductorTritonCUDA\r\n\r\n# Additionally, the new test classes if a native inductor backend is used are guarded\r\n# e.g.  TestInductorCppCPU\r\n# is equivalent to the following class definition\r\n# @skipUnless(HAS_CPU, \"Requires C++ compiler\")\r\n# @config.patch(\"cpu_backend\", \"cpp\")\r\n# class TestInductorCppCPU(CPUTestBase)\r\n#   ...\r\n\r\n\r\n\r\n```\r\n\r\nAn illustration of the before/after changes:\r\n\r\n\r\nFixes #ISSUE_NUMBER\r\n\r\n\r\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang @aakhundov",
        "open",
        "2025-01-28T20:54:32Z",
        null,
        null,
        "open source, module: inductor",
        "main",
        "mwizak/extend-device-agnostic-testing-with-inductor",
        4,
        195,
        71,
        5,
        2,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/145873"
    ],
    [
        145866,
        "[pytorch][cuda] Improve softmax backward pass native CUDA implementation",
        "This PR is similar to https://github.com/pytorch/pytorch/pull/122970, but works on the softmax backward pass.\r\n\r\nSpecifically, it uses shared memory to cache the gradOutput when it can fit in shared memory. Before this PR we were reading gradOutput twice.\r\n\r\nOn my H100 this seems to improve the softmax backward pass performance by about 5% for problem sizes that fit within shared memory. (Note that this is not the only kernel that runs when you call softmax backward pass -- there is an elementwise kernel that runs before this; optimizing that can be a separate PR).\r\n\r\n**Important Note**: Currently the softmax backward pass consists of an [element-wise multiply operator](https://github.com/pytorch/pytorch/blob/7f65a208848205b38445423b7e2e93a2b4994e5e/aten/src/ATen/native/cuda/SoftMax.cu#L1216), followed by [this function](https://github.com/pytorch/pytorch/blob/7f65a208848205b38445423b7e2e93a2b4994e5e/aten/src/ATen/native/cuda/SoftMax.cu#L1062) which calls the `cunn_SoftMaxBackward` kernel. With my change the kernel time reduces by about 12% (see screenshot below), while the total time (including the elementwise) reduces by about 5%.\r\n\r\n```\r\nBaseline\t\t\t\t\t\tThis PR\r\nN\tsize\tFP32 bandwidth\tFP16 bandwidth\t\tN\tsize\tFP32 bandwidth\tFP16 bandwidth\t\tfp32 diff\tfp16 diff\r\n0\t256\t134.340966\t70.042039\t\t0\t256\t133.70146\t70.342753\t\t-0.48%\t0.43%\r\n1\t512\t233.501185\t129.945803\t\t1\t512\t234.057145\t132.933066\t\t0.24%\t2.30%\r\n2\t1024\t340.667966\t229.280464\t\t2\t1024\t338.833265\t226.441699\t\t-0.54%\t-1.24%\r\n3\t2048\t379.643726\t337.452058\t\t3\t2048\t399.559017\t338.432284\t\t5.25%\t0.29%\r\n4\t4096\t416.597537\t383.625364\t\t4\t4096\t428.252403\t396.137506\t\t2.80%\t3.26%\r\n5\t6000\t431.198241\t384.384384\t\t5\t6000\t457.744577\t406.06275\t\t6.16%\t5.64%\r\n6\t8192\t462.811252\t427.292573\t\t6\t8192\t474.791032\t428.281563\t\t2.59%\t0.23%\r\n7\t10000\t464.258731\t429.050294\t\t7\t10000\t483.7643\t446.849381\t\t4.20%\t4.15%\r\n8\t10013\t465.199701\t429.824179\t\t8\t10013\t464.904407\t428.72184\t\t-0.06%\t-0.26%\r\n9\t10240\t477.07359\t428.853737\t\t9\t10240\t485.317024\t444.902586\t\t1.73%\t3.74%\r\n10\t11000\t473.038785\t430.778663\t\t10\t11000\t488.161438\t453.462162\t\t3.20%\t5.27%\r\n11\t12000\t474.342475\t432.594814\t\t11\t12000\t490.532418\t458.427653\t\t3.41%\t5.97%\r\n12\t16384\t487.468854\t473.611576\t\t12\t16384\t488.154406\t476.264631\t\t0.14%\t0.56%\r\n13\t20000\t482.029793\t465.666186\t\t13\t20000\t482.147092\t483.886193\t\t0.02%\t3.91%\r\n14\t24000\t478.368093\t474.159464\t\t14\t24000\t478.364948\t491.447921\t\t0.00%\t3.65%\r\n15\t32000\t476.523796\t473.18868\t\t15\t32000\t476.523796\t474.398962\t\t0.00%\t0.26%\r\n16\t32768\t476.104723\t477.493634\t\t16\t32768\t476.704463\t477.330606\t\t0.13%\t-0.03%\r\n17\t36864\t477.900663\t475.472787\t\t17\t36864\t477.973279\t475.728454\t\t0.02%\t0.05%\r\n18\t40960\t477.707561\t475.559064\t\t18\t40960\t478.445017\t476.088067\t\t0.15%\t0.11%\r\n19\t45056\t479.169812\t475.865134\t\t19\t45056\t479.143266\t475.878202\t\t-0.01%\t0.00%\r\n20\t49152\t477.804907\t475.382982\t\t20\t49152\t477.868404\t475.976377\t\t0.01%\t0.12%\r\n21\t65536\t481.274125\t478.171806\t\t21\t65536\t481.537733\t478.703926\t\t0.05%\t0.11%\r\n22\t66000\t481.64652\t480.095457\t\t22\t66000\t481.856013\t480.466388\t\t0.04%\t0.08%\r\n23\t68608\t481.745774\t479.034704\t\t23\t68608\t481.917596\t478.856209\t\t0.04%\t-0.04%\r\n24\t80000\t483.409361\t480.356529\t\t24\t80000\t483.330481\t480.375277\t\t-0.02%\t0.00%\r\n25\t98304\t480.736301\t481.396882\t\t25\t98304\t480.789858\t481.320143\t\t0.01%\t-0.02%\r\n```\r\n\r\nNCU profiler shows lower DRAM fetches with the new kernel:\r\n\r\n![image](https://github.com/user-attachments/assets/f3606725-d8fc-4ea5-ae6d-9c188bf32d72)\r\n\r\nNCU reports about 12% elapsed time reduction in this kernel alone compared to baseline (and because of other kernels that are run, the overall backward pass time as seen by the user gets reduced by 5%).\r\n\r\nI compared the binary size increase by running `python setup.py develop` before and after and diffing the .so files:\r\n\r\n![image](https://github.com/user-attachments/assets/8e6cee2e-3c7a-4fa4-8836-954047ce8ffc)\r\n\r\nlibtorch_cuda.so goes from 274,752,224 bytes to 274,787,072 bytes. The increase in size is 34kB which is about 0.01%.\r\n\r\nI measured the compilation time for incremental development:\r\n\r\n```\r\ntouch ./aten/src/ATen/native/cuda/SoftMax.cu\r\ntime python setup.py develop\r\nreal    0m10.083s\r\nuser    0m8.197s\r\nsys     0m3.149s\r\n```\r\n\r\nNote that this uses `ccache` and does a bunch of copies and is not just measuring the `nvcc` time. I measured the `nvcc` time separately by capturing the `nvcc` command shown in [1] below and running it on the baseline and modified kernels:\r\n\r\n```\r\n# baseline nvcc time for SoftMax.cu\r\nreal    0m35.341s\r\nuser    0m33.801s\r\nsys     0m1.289s\r\n\r\n# this PR's nvcc time for SoftMax.cu\r\nreal    0m36.513s\r\nuser    0m34.722s\r\nsys     0m1.408s\r\n```\r\n\r\nSo the `nvcc` time increases by about 1 second, or ~3% of the baseline.\r\n\r\n[1] `nvcc` command is here:\r\n```\r\n# This is the nvcc command\r\n/usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DAT_PER_OPERATOR_HEADERS -DFLASHATTENTION_DISABLE_ALIBI -DFMT_HEADER_ONLY=1 -DHAVE_MALLOC_USABLE_SIZE=1 -DHAVE_MMAP=1 -DHAVE_SHM_OPEN=1 -DHAVE_SHM_UNLINK=1 -DMINIZ_DISABLE_ZIP_READER_CRC32_CHECKS -DONNXIFI_ENABLE_EXT=1 -DONNX_ML=1 -DONNX_NAMESPACE=onnx_torch -DTORCH_CUDA_BUILD_MAIN_LIB -DTORCH_CUDA_USE_NVTX3 -DUSE_C10D_GLOO -DUSE_C10D_NCCL -DUSE_CUDA -DUSE_DISTRIBUTED -DUSE_EXTERNAL_MZCRC -DUSE_FLASH_ATTENTION -DUSE_MEM_EFF_ATTENTION -DUSE_NCCL -DUSE_RPC -DUSE_TENSORPIPE -D_FILE_OFFSET_BITS=64 -Dtorch_cuda_EXPORTS -I/home/ahmads/personal/pytorch/build/aten/src -I/home/ahmads/personal/pytorch/aten/src -I/home/ahmads/personal/pytorch/build -I/home/ahmads/personal/pytorch -I/home/ahmads/personal/pytorch/cmake/../third_party/benchmark/include -I/home/ahmads/personal/pytorch/third_party/onnx -I/home/ahmads/personal/pytorch/build/third_party/onnx -I/home/ahmads/personal/pytorch/nlohmann -I/home/ahmads/personal/pytorch/aten/src/THC -I/home/ahmads/personal/pytorch/aten/src/ATen/cuda -I/home/ahmads/personal/pytorch/third_party/fmt/include -I/home/ahmads/personal/pytorch/aten/src/ATen/../../../third_party/cutlass/include -I/home/ahmads/personal/pytorch/aten/src/ATen/../../../third_party/cutlass/tools/util/include -I/home/ahmads/personal/pytorch/build/caffe2/aten/src -I/home/ahmads/personal/pytorch/aten/src/ATen/.. -I/home/ahmads/personal/pytorch/build/nccl/include -I/home/ahmads/personal/pytorch/c10/cuda/../.. -I/home/ahmads/personal/pytorch/c10/.. -I/home/ahmads/personal/pytorch/third_party/tensorpipe -I/home/ahmads/personal/pytorch/build/third_party/tensorpipe -I/home/ahmads/personal/pytorch/third_party/tensorpipe/third_party/libnop/include -I/home/ahmads/personal/pytorch/torch/csrc/api -I/home/ahmads/personal/pytorch/torch/csrc/api/include -isystem /home/ahmads/personal/pytorch/build/third_party/gloo -isystem /home/ahmads/personal/pytorch/cmake/../third_party/gloo -isystem /home/ahmads/personal/pytorch/cmake/../third_party/tensorpipe/third_party/libuv/include -isystem /home/ahmads/personal/pytorch/cmake/../third_party/googletest/googlemock/include -isystem /home/ahmads/personal/pytorch/cmake/../third_party/googletest/googletest/include -isystem /home/ahmads/personal/pytorch/third_party/protobuf/src -isystem /home/ahmads/personal/pytorch/third_party/XNNPACK/include -isystem /home/ahmads/personal/pytorch/third_party/ittapi/include -isystem /home/ahmads/personal/pytorch/cmake/../third_party/eigen -isystem /usr/local/cuda/include -isystem /home/ahmads/personal/pytorch/torch/include -isystem /home/ahmads/personal/pytorch/third_party/ideep/include -isystem /home/ahmads/personal/pytorch/torch/include/oneapi/dnnl -isystem /home/ahmads/personal/pytorch/INTERFACE -isystem /home/ahmads/personal/pytorch/third_party/nlohmann/include -isystem /home/ahmads/personal/pytorch/third_party/NVTX/c/include -isystem /home/ahmads/personal/pytorch/cmake/../third_party/cudnn_frontend/include -DLIBCUDACXX_ENABLE_SIMPLIFIED_COMPLEX_OPERATIONS -D_GLIBCXX_USE_CXX11_ABI=1 -Xfatbin -compress-all -DONNX_NAMESPACE=onnx_torch -gencode arch=compute_90,code=sm_90 -Xcudafe --diag_suppress=cc_clobber_ignored,--diag_suppress=field_without_dll_interface,--diag_suppress=base_class_has_different_dll_interface,--diag_suppress=dll_interface_conflict_none_assumed,--diag_suppress=dll_interface_conflict_dllexport_assumed,--diag_suppress=bad_friend_decl --expt-relaxed-constexpr --expt-extended-lambda  -Wno-deprecated-gpu-targets --expt-extended-lambda -DCUB_WRAPPED_NAMESPACE=at_cuda_detail -DCUDA_HAS_FP16=1 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -O3 -DNDEBUG -std=c++17 -Xcompiler=-fPIC -DTORCH_USE_LIBUV -DCAFFE2_USE_GLOO -Xcompiler -Wall -Wextra -Wdeprecated -Wno-unused-parameter -Wno-missing-field-initializers -Wno-array-bounds -Wno-unknown-pragmas -Wno-strict-overflow -Wno-strict-aliasing -Wunused-function -Wunused-variable -Wunused-but-set-variable -Wno-maybe-uninitialized -MD -MT caffe2/CMakeFiles/torch_cuda.dir/__/aten/src/ATen/native/cuda/SoftMax.cu.o -MF caffe2/CMakeFiles/torch_cuda.dir/__/aten/src/ATen/native/cuda/SoftMax.cu.o.d -x cu -c /home/ahmads/personal/pytorch/aten/src/ATen/native/cuda/SoftMax.cu -o caffe2/CMakeFiles/torch_cuda.dir/__/aten/src/ATen/native/cuda/SoftMax.cu.o\r\n```\r\n\r\n\r\n\r\n\r\n\r\n",
        "open",
        "2025-01-28T20:26:34Z",
        null,
        null,
        "release notes: cuda",
        "main",
        "softmax1",
        4,
        157,
        12,
        13,
        2,
        1,
        "ngimel, ahmadsharif1",
        "COMMENTED, COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145866"
    ],
    [
        145865,
        "[WIP] Add test_torchinductor_opinfo.py to triton-cpu tests",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #145865\n\nI'm guessing this isn't going to pass, but I want to see how long the CI takes.",
        "open",
        "2025-01-28T20:21:09Z",
        null,
        null,
        "ciflow/inductor, keep-going",
        "gh/davidberard98/335/base",
        "gh/davidberard98/335/head",
        1,
        1,
        1,
        1,
        5,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/145865"
    ],
    [
        145863,
        "Cleanup VS 2019 refs in pytorch",
        "Related to: https://github.com/pytorch/pytorch/issues/128835\r\nFollow up on PR: https://github.com/pytorch/pytorch/pull/145319",
        "open",
        "2025-01-28T19:26:12Z",
        null,
        null,
        "ciflow/binaries, ciflow/trunk, release notes: releng, test-config/default",
        "main",
        "cleanup_vs_2019",
        10,
        9,
        130,
        4,
        8,
        1,
        "Skylion007, huydhn, malfet, huydhn, huydhn, atalman",
        "APPROVED, COMMENTED, APPROVED, APPROVED, COMMENTED, APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145863"
    ],
    [
        145854,
        "[c10d][UCC] Support coalesing in `c10d::ProcessGroupUCC` through alltoallv",
        "# What \r\nAdd support for coalescing communication in `ProcessGroupUCC`, by implementing the methods `startCoalescing` and `endCoalescing`.\r\n\r\nWe need to impose several restrictions to the coalesced group:\r\n1) we can only coalesce `send` and `recv` ops\r\n2) we can only coalesce one `send` and one `recv` maximum per pair of ranks\r\n3) all ranks must participate in the `startCoalescing` and `endCoalescing` calls, even if the group is empty.\r\n4) we do not support tags for coalesced groups.\r\n\r\n# Why\r\n\r\nDespite the above restrictions, we cover a number of useful data patterns, such as ring p2p, allgather, broadcast, alltoall, etc. Those data patterns or other custom ones are conveniently written in terms of coalesced send/recv calls, which this patch makes possible.\r\n\r\nRecall that for a p2p bidirectional transfer, the send and recv need to be coalesced to enjoy full bidirectional bandwidth\r\n\r\n# How\r\n\r\nSince UCC does not natively support Coalescing, we implement it at the ProcessGroup level. The implementation relies on calling UCC's alltoallv, setting the count to `0` and displacement to `nullptr` for ranks that do not exchange data.\n\ncc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o",
        "open",
        "2025-01-28T17:16:51Z",
        null,
        null,
        "oncall: distributed, triaged, open source, release notes: distributed (c10d)",
        "main",
        "ucc_coalesced_a2av",
        3,
        151,
        21,
        2,
        2,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145854"
    ],
    [
        145853,
        "cmake: fix detection logic when using system XNNPACK",
        "This commit makes the following improvements:\r\n\r\n* The \"elseif\" branch now only runs when USE_XNNPACK is set.\r\n* Use \"REQUIRED\" to enforce the existence of XNNPACK libraries, and remove the erroneous if statement ('or' should be 'OR').\r\n* libmicrokernels-prod is built statically in XNNPACK [1], change in pytorch side accordingly.\r\n\r\n[1]: https://github.com/google/XNNPACK/blob/d7f398ee5e135ef4f7045802eea973cc6cb26c6c/CMakeLists.txt#L819\r\n\r\n",
        "open",
        "2025-01-28T17:05:50Z",
        null,
        null,
        "triaged, open source, topic: not user facing",
        "main",
        "fix-system-xnnpack",
        1,
        4,
        7,
        1,
        2,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145853"
    ],
    [
        145850,
        "Skip search for MKL on ARM cpus",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #145871\n* #145870\n* __->__ #145850\n\nIt will not find it anyway and makes a bit easier parsing thru CMake log on non-x86 systems",
        "open",
        "2025-01-28T16:40:35Z",
        null,
        null,
        "topic: not user facing",
        "gh/malfet/155/base",
        "gh/malfet/155/head",
        1,
        5,
        0,
        1,
        1,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145850"
    ],
    [
        145847,
        "Improve the guards on some cpp wrapper tests ",
        "Since they're the CPU CPP wrapper tests, they should only run if the CPU backend we're using is the CPP one.\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang @aakhundov",
        "open",
        "2025-01-28T16:22:39Z",
        null,
        null,
        "triaged, open source, topic: not user facing, module: inductor",
        "main",
        "charliew/test-skip",
        1,
        1,
        1,
        1,
        5,
        0,
        "desertfire",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145847"
    ],
    [
        145834,
        "[inductor] Add features to docstring_linter (see #142496)",
        "## Improvements to `docstring_linter`\r\n\r\n* Add a \"grandfather list\" of existing undocumented classes and functions (`--grandfather`, `--grandfather-tolerance`, `--no-grandfather`, `--write-grandfather`)\r\n* In classes, now just one of the class itself or its `__init__()` method needs to be documented (`--lint-init` turns the old behavior back on)\r\n* Now classes and functions defined local to other functions do not need to be documented (`--lint-local` turns the old behavior back on)\r\n* New `--report` flag produces a compact report of long, undocumented classes or function definitions: see attached example run over all pytorch: [pytorch-docs.json](https://github.com/user-attachments/files/18455981/pytorch-docs.json)\r\n\r\n## Help text\r\n\r\n```\r\n$ python tools/linter/adapters/docstring_linter.py --help\r\nusage: docstring_linter.py [-h] [-l] [-v] [--grandfather GRANDFATHER] [--grandfather-tolerance GRANDFATHER_TOLERANCE] [--lint-init]\r\n                           [--lint-local] [--lint-protected] [--max-class MAX_CLASS] [--max-def MAX_DEF]\r\n                           [--min-docstring MIN_DOCSTRING] [--no-grandfather] [--report] [--write-grandfather]\r\n                           [files ...]\r\n\r\n`docstring_linter` reports on long functions, methods or classes without docstrings\r\n\r\npositional arguments:\r\n  files                 A list of files or directories to lint\r\n\r\noptional arguments:\r\n  -h, --help            show this help message and exit\r\n  -l, --lintrunner      Run for lintrunner and print LintMessages which aren't edits\r\n  -v, --verbose         Print more debug info\r\n  --grandfather GRANDFATHER, -g GRANDFATHER\r\n                        Set the grandfather list\r\n  --grandfather-tolerance GRANDFATHER_TOLERANCE, -t GRANDFATHER_TOLERANCE\r\n                        Tolerance for grandfather sizes, in percent\r\n  --lint-init, -i       Lint __init__ and class separately\r\n  --lint-local, -o      Lint definitions inside other functions\r\n  --lint-protected, -p  Lint functions, methods and classes that start with _\r\n  --max-class MAX_CLASS, -c MAX_CLASS\r\n                        Maximum number of lines for an undocumented class\r\n  --max-def MAX_DEF, -d MAX_DEF\r\n                        Maximum number of lines for an undocumented function\r\n  --min-docstring MIN_DOCSTRING, -s MIN_DOCSTRING\r\n                        Minimum number of characters for a docstring\r\n  --no-grandfather, -n  Disable the grandfather list\r\n  --report, -r          Print a report on all classes and defs\r\n  --write-grandfather, -w\r\n                        Rewrite the grandfather list\r\n```\r\n\r\n---\r\n\r\nStack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #144622\n* #144621\n* __->__ #145834\n* #144620\n\r\n",
        "open",
        "2025-01-28T12:03:29Z",
        null,
        null,
        "module: lint, open source, better-engineering, topic: not user facing, suppress-api-compatibility-check, suppress-bc-linter",
        "gh/rec/128/base",
        "gh/rec/128/head",
        9,
        1199,
        191,
        5,
        2,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145834"
    ],
    [
        145833,
        "Implement KL for studentT",
        "Fixes #145729\r\n",
        "open",
        "2025-01-28T10:37:07Z",
        null,
        null,
        "triaged, open source",
        "main",
        "fix/student_t_kl",
        1,
        66,
        0,
        1,
        2,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145833"
    ],
    [
        145832,
        "[DO NOT MERGE] [TESTING] [ROCm] Triton cherry-picks for AMD backend perf optimisation",
        "Testing for rc/3.2.x PR\n\ncc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @hongxiayang @naromero77amd",
        "open",
        "2025-01-28T10:21:32Z",
        null,
        null,
        "module: rocm, open source, ciflow/trunk, topic: not user facing, ciflow/periodic, ciflow/inductor, ciflow/inductor-perf-compare, ciflow/rocm, ciflow/inductor-rocm, ciflow/inductor-periodic",
        "main",
        "rc32-cps",
        2,
        2,
        2,
        2,
        1,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/145832"
    ],
    [
        145822,
        "Remove unneeded CUDA logic from _create_build_env",
        "Because FindCUDAToolkit.cmake has that logic.\r\n",
        "open",
        "2025-01-28T03:34:42Z",
        null,
        null,
        "triaged, open source, topic: not user facing",
        "main",
        "cmake_find",
        1,
        0,
        10,
        1,
        1,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145822"
    ],
    [
        145819,
        "Replace distutils.version with  copied looseversion",
        "distutils was deprecated.\r\n",
        "open",
        "2025-01-28T03:17:56Z",
        null,
        null,
        "triaged, open source, topic: not user facing",
        "main",
        "looseversion",
        1,
        92,
        2,
        1,
        2,
        1,
        "albanD",
        "COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145819"
    ],
    [
        145812,
        "[cutlass backend] check against arch >= 100",
        "Summary:\nWant to add a guard against silent fallback to SM90.\n\nGenerateSM100 was just added 3 days ago. https://github.com/NVIDIA/cutlass/blame/main/python/cutlass_library/generator.py#L8896\n\nIt should show up in CUTLASS 3.8 (not pinned yet).\n\nTest Plan: ci\n\nDifferential Revision: D68748705\n\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang @aakhundov",
        "open",
        "2025-01-28T02:03:04Z",
        null,
        null,
        "fb-exported, ciflow/trunk, topic: not user facing, module: inductor, ciflow/inductor",
        "main",
        "export-D68748705",
        1,
        21,
        2,
        1,
        7,
        0,
        "chenyang78, ColinPeppler, ColinPeppler, Aidyn-A, henrylhtsang, Aidyn-A, henrylhtsang",
        "APPROVED, COMMENTED, APPROVED, CHANGES_REQUESTED, COMMENTED, COMMENTED, COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145812"
    ],
    [
        145811,
        "[AsyncMM] re-enable and adapt to cutlass 3.6.0 (#144011)",
        "Summary:\n\n\n\n\ncc H-Huang awgu kwen2501 wanchaol fegin fduwjj wz337 wconstab d4l3k c-p-i-o\n\nimported-using-ghimport\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D68734003\n\nPulled By: yifuwang\n\n\n\n\ncc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o",
        "open",
        "2025-01-28T01:59:16Z",
        null,
        null,
        "oncall: distributed, fb-exported, release notes: distributed (c10d)",
        "main",
        "export-D68734003",
        2,
        262,
        119,
        1,
        4,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145811"
    ],
    [
        145798,
        "[will-not-merge] tuning",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* (to be filled)\n\n\n\ncc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o",
        "open",
        "2025-01-27T23:43:34Z",
        null,
        null,
        "oncall: distributed, release notes: distributed (c10d)",
        "gh/yifuwang/192/base",
        "gh/yifuwang/192/head",
        3,
        49,
        59,
        1,
        1,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/145798"
    ],
    [
        145797,
        "[Async-TP] improve algo selection",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #145798\n* __->__ #145797\n* #145796\n* #145795\n* #145794\n\n",
        "open",
        "2025-01-27T23:43:30Z",
        null,
        null,
        "oncall: distributed",
        "gh/yifuwang/191/base",
        "gh/yifuwang/191/head",
        2,
        134,
        87,
        1,
        2,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/145797"
    ],
    [
        145796,
        "[Async-TP] _pipelined_multi_all_gather_and_consume reduce overhead",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #145798\n* #145797\n* __->__ #145796\n* #145795\n* #145794\n\n",
        "open",
        "2025-01-27T23:43:25Z",
        null,
        null,
        "oncall: distributed",
        "gh/yifuwang/190/base",
        "gh/yifuwang/190/head",
        1,
        15,
        18,
        1,
        2,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/145796"
    ],
    [
        145795,
        "[AsyncMM] preliminary tuning",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #145798\n* #145797\n* #145796\n* __->__ #145795\n* #145794\n\n",
        "open",
        "2025-01-27T23:43:21Z",
        null,
        null,
        "oncall: distributed, release notes: distributed (c10d)",
        "gh/yifuwang/189/base",
        "gh/yifuwang/189/head",
        1,
        57,
        4,
        1,
        1,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/145795"
    ],
    [
        145794,
        "[Async-TP] Port _fused_all_gather_matmul_native to cpp to reduce launching overhead",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\r\n* #145798\r\n* #145797\r\n* #145796\r\n* #145795\r\n* __->__ #145794\r\n\r\n`_fused_all_gather_matmul_native` schedules multiple tasks (e.g., kernel, copy engine transfers, and stream_write_value32_) onto the GPU. Previously, `fused_all_gather_matmul_native` was implemented in Python, and issuing most of these tasks incurred dispatcher overhead. When the problem size is small, the CPU overhead can exceed the GPU\u2019s execution time. While this may be acceptable in workloads where the CPU runs ahead of the GPU, it still isn\u2019t ideal.\r\n\r\nThis PR reduces CPU overhead by porting `_fused_all_gather_matmul_native` to C++. Specifically, it eliminates dispatcher overhead for:\r\n- `aten.split` (calling `aten.narrow` \u00d7 `world_size` times)\r\n- `symm_mem::stream_write_value32_` \u00d7 `world_size` times\r\n\r\n<img width=\"842\" alt=\"image\" src=\"https://github.com/user-attachments/assets/176ebc89-a2e1-4c07-b340-c2d4422def09\" />\r\n<img width=\"455\" alt=\"image\" src=\"https://github.com/user-attachments/assets/3768f0c1-876f-4c66-bd22-89aa80d0889c\" />",
        "open",
        "2025-01-27T23:43:16Z",
        null,
        null,
        "oncall: distributed, release notes: distributed (c10d)",
        "gh/yifuwang/188/base",
        "gh/yifuwang/188/head",
        4,
        135,
        46,
        1,
        1,
        1,
        "lw",
        "APPROVED",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/145794"
    ],
    [
        145779,
        "[CUDNN][CUDNN V8 API] Allow user-specified CUDNN V8 API benchmarking technique",
        "Useful for debugging apparent \"regressions\" when using cuDNN autotuning (\"benchmarking\")\n\ncc @csarofeen @ptrblck @xwang233",
        "open",
        "2025-01-27T21:33:12Z",
        null,
        null,
        "module: cudnn, triaged, open source, topic: not user facing",
        "main",
        "cudnnv8technique",
        1,
        73,
        6,
        3,
        1,
        0,
        "Skylion007, eqy",
        "COMMENTED, COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145779"
    ],
    [
        145778,
        "NJT support for cat() on the ragged dim",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\r\n* __->__ #145778\r\n\r\nRequested [here](https://github.com/pytorch/pytorch/issues/118107#issuecomment-2615705795).\r\n\r\nThere's still a fair amount of work left. TODO:\r\n* Fix the backwards pass (need NJT-specific derivative formula, possibly `narrow()` on the ragged dim)\r\n* Fix data-dependency errors in forward + torch.compile() due to `unbind()` usage",
        "open",
        "2025-01-27T21:28:00Z",
        null,
        null,
        "topic: not user facing",
        "gh/jbschlosser/226/base",
        "gh/jbschlosser/226/head",
        3,
        161,
        6,
        1,
        1,
        0,
        "Skylion007",
        "COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145778"
    ],
    [
        145776,
        "Update to NCCL 2.25.1 for 12.8",
        "https://github.com/pytorch/pytorch/issues/145570\r\n\r\nfollow up for https://github.com/pytorch/pytorch/pull/145567/files",
        "open",
        "2025-01-27T21:16:42Z",
        null,
        null,
        "triaged, open source, topic: not user facing",
        "main",
        "nccl-update-12.8",
        2,
        4,
        0,
        1,
        3,
        1,
        "eqy, Skylion007",
        "APPROVED, CHANGES_REQUESTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145776"
    ],
    [
        145774,
        "[POC] flat_apply HOP",
        "[no-ci]\r\n\r\nFixes #ISSUE_NUMBER\r\n\n\ncc @ezyang @SherlockNoMad @EikanWang @jgong5 @wenzhe-nrv",
        "open",
        "2025-01-27T20:46:12Z",
        null,
        null,
        "release notes: fx, fx",
        "main",
        "flat_apply",
        3,
        183,
        0,
        2,
        1,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/145774"
    ],
    [
        145772,
        "Implement serializable getattr support for tensor subclasses",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #145772\n\r\nbuiltins.getattr is not serializable, so we replace it with a custom op that has more refined schema. \r\n\r\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames\n\nDifferential Revision: [D68899421](https://our.internmc.facebook.com/intern/diff/D68899421)",
        "open",
        "2025-01-27T20:19:13Z",
        null,
        null,
        "ciflow/trunk, module: dynamo, ciflow/inductor, release notes: export",
        "gh/tugsbayasgalan/288/base",
        "gh/tugsbayasgalan/288/head",
        5,
        49,
        30,
        8,
        4,
        1,
        "bdhirsh, bdhirsh, tugsbayasgalan, bdhirsh",
        "COMMENTED, COMMENTED, COMMENTED, APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145772"
    ],
    [
        145770,
        "[torch][distributed] re-merge NCCLComm::split impl",
        "Summary:\nThese were originally forked between fbcode and oss. This has led to some drift, as bugfixes related to ncclCommSplit + non-blocking never made it to internal. Now we're hitting these bugs in monarch so it would be nice to fix.\n\nJust upstream the forked code and delete the fb-only version.\n\nTest Plan: Unit tests\n\nDifferential Revision: D68727854\n\n\n\n\ncc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o",
        "open",
        "2025-01-27T20:12:04Z",
        null,
        null,
        "oncall: distributed, fb-exported, release notes: distributed (c10d)",
        "main",
        "export-D68727854",
        1,
        10,
        1,
        1,
        2,
        0,
        "Skylion007, kwen2501, suo",
        "COMMENTED, COMMENTED, COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145770"
    ],
    [
        145751,
        "[OSS] Update FileSystem methods to properly handle a string argument",
        "Summary: When testing, I tried to pass in a string argument to the FileSystem class' methods, which is a valid input, but the cast() that casted the string to a path wasn't working as was likely expected and was leading all the methods to fail with a string arg. Instead of a cast, a proper constructor should be used.\n\nTest Plan: N6475361 methods don't throw an error with a string arg like they were previously\n\nDifferential Revision: D68713937\n\n\n\n\ncc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o @LucasLLC @MeetVadakkanchery @mhorowitz @pradeepfn @ekr0",
        "open",
        "2025-01-27T18:57:16Z",
        null,
        null,
        "oncall: distributed, fb-exported, topic: not user facing, module: distributed_checkpoint",
        "main",
        "export-D68713937",
        1,
        19,
        6,
        1,
        3,
        1,
        "Skylion007, mhorowitz, Skylion007, ankitageorge, Skylion007, Skylion007, Skylion007, Skylion007, ankitageorge, Skylion007, Skylion007, ankitageorge, ankitageorge",
        "COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, CHANGES_REQUESTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145751"
    ],
    [
        145750,
        "[dynamo] save/restore system random state more carefully",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #145750\n\r\nReattempt of https://github.com/pytorch/pytorch/pull/145435 since the state of the linked internal diff appears to be messed up.\r\n\r\nNote: I have verified that the previously failing internal tests now pass internally.\r\n\r\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames\n\nDifferential Revision: [D68918404](https://our.internmc.facebook.com/intern/diff/D68918404)",
        "open",
        "2025-01-27T18:55:10Z",
        null,
        null,
        "Merged, Reverted, ciflow/trunk, topic: bug fixes, topic: not user facing, module: dynamo, ciflow/inductor, ci-no-td",
        "gh/williamwen42/201/base",
        "gh/williamwen42/201/head",
        10,
        156,
        12,
        10,
        15,
        1,
        "StrongerXi, jansel",
        "APPROVED, CHANGES_REQUESTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145750"
    ],
    [
        145748,
        "Set USE_CUFILE=1 by default and add pypi package to binary build matrix",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #146739\n* __->__ #145748\n\n",
        "open",
        "2025-01-27T18:47:54Z",
        null,
        null,
        "ciflow/trunk, topic: not user facing, ciflow/binaries_wheel",
        "gh/mikaylagawarecki/311/base",
        "gh/mikaylagawarecki/311/head",
        8,
        80,
        25,
        18,
        1,
        0,
        "Skylion007, mikaylagawarecki, atalman",
        "COMMENTED, COMMENTED, COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145748"
    ],
    [
        145741,
        "[BE]: Update Cutlass submodule to 3.8 candidate for SM100+ support",
        "Update CUTLASS submodule to 3.8 candidate for preliminary Blackwell support, without this PyTorch will not compile various CUTLASS kernels properly for Blackwell.",
        "open",
        "2025-01-27T17:39:08Z",
        null,
        null,
        "open source, better-engineering, ciflow/trunk, release notes: cuda, topic: not user facing",
        "main",
        "skylion007/update-cutlass-3-8-0-prc-0",
        1,
        1,
        1,
        3,
        15,
        1,
        "albanD",
        "CHANGES_REQUESTED",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/145741"
    ],
    [
        145738,
        "[Docs] Make comm handle wait & is_completed docs more clear for multi-stream",
        "Fixes #145713\r\nLooks like a earlier fix on `wait()` (https://github.com/pytorch/pytorch/pull/143305) has been pushed but not reflected in the main branch, so feel free to revert my change on that part. \r\nMade coressponding clarifications on `is_completed`\r\ncc @awgu @wconstab ",
        "open",
        "2025-01-27T16:31:18Z",
        null,
        null,
        "triaged, open source, release notes: distributed (c10d)",
        "main",
        "dist_docs",
        1,
        4,
        3,
        2,
        4,
        1,
        "awgu, wconstab, wconstab, Edenzzzz, Edenzzzz, wconstab, wconstab, Edenzzzz, Edenzzzz, Edenzzzz, wconstab, Edenzzzz, kwen2501, Edenzzzz",
        "COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145738"
    ],
    [
        145719,
        "Fix support for nccl < 2.17",
        "Fix build failure with older (< 2.17) NCCL.\r\n\r\nRefactoring NCCL version related code: \r\n1. Fix failure against old NCCL versions since #138527 cc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o ;\r\n2. remove unused checks caused by unsupported NCCL version (since there's a static assert checking NCCL >= 2.7: #142023);\r\n3. move NCCL macros to `torch/csrc/cuda/nccl.h` from various places and uniform some style (`#if` to `#ifdef`), which could improve maintainability of the NCCL part I hope.\r\n\r\nResolves #141914\r\n\r\ncc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o",
        "open",
        "2025-01-27T06:09:47Z",
        null,
        null,
        "oncall: distributed, open source, ciflow/trunk, release notes: distributed (c10d), topic: not user facing",
        "main",
        "nccl-wraps",
        8,
        111,
        126,
        20,
        17,
        0,
        "wconstab, wconstab, c-p-i-o, c-p-i-o",
        "COMMENTED, APPROVED, COMMENTED, APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145719"
    ],
    [
        145717,
        "add input shape check for _local_scalar_dense",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\r\n* __->__ #145717\r\n\r\n\r\nFix https://github.com/pytorch/pytorch/issues/145066.\r\n",
        "open",
        "2025-01-27T04:03:36Z",
        null,
        null,
        "open source, ciflow/trunk, topic: not user facing",
        "gh/jiayisunx/55/base",
        "gh/jiayisunx/55/head",
        2,
        7,
        0,
        2,
        1,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145717"
    ],
    [
        145700,
        "[POC] [CPU][Inductor] Support INT8 SDPA based on CPP template",
        "This PR implements the Int8 SDPA CPU kernel based on CPP template following the RFC #144941.\r\n\r\nARs:\r\n\r\n- [ ] INT8 SDPA patterns:\r\n       - Done: FP32 with/wo mask, batch size >/= 1;\r\n       - Remain: BF16 with/wo mask, batch size >/= 1.\r\n- [ ] Add the `select_strategy` to generate the kernel with various parallel loop strategies.\r\n- [ ] Enable and validate on related models, and make sure the good accuracy/perf.\r\n- [ ] Refactor the codes, and make best use of the common parts.\r\n- [ ] Add necessary comments.\r\n\r\n\r\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang @aakhundov",
        "open",
        "2025-01-26T07:15:35Z",
        null,
        null,
        "open source, topic: not user facing, module: inductor, ciflow/inductor",
        "main",
        "int8_sdpa_template",
        6,
        2390,
        8,
        3,
        1,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/145700"
    ],
    [
        145694,
        "feat: improve indexing error messages",
        "Improves the error messages when certain tensor operations fail, for example\r\n\r\n`RuntimeError: index_select(): Expected dtype int32 or int64 for index` will now say\r\ne.g.\r\n`RuntimeError: index_select(): Expected dtype int32 or int64 for index, got float32`\r\n",
        "open",
        "2025-01-26T02:35:49Z",
        null,
        null,
        "triaged, open source, release notes: mps",
        "main",
        "mattb.feat.improve-indexing-error-msg",
        1,
        4,
        3,
        1,
        2,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145694"
    ],
    [
        145685,
        "[Easy] update pip sources for ROCm in nightly pull tool",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #145685\n\n\n\ncc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd",
        "open",
        "2025-01-25T18:44:59Z",
        null,
        null,
        "module: rocm, open source, topic: not user facing, ciflow/rocm",
        "gh/XuehaiPan/239/base",
        "gh/XuehaiPan/239/head",
        1,
        8,
        0,
        3,
        3,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145685"
    ],
    [
        145681,
        "Avoid data-dependent errors by runtime assert substitution.",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #142372\n* __->__ #145681\n\nThis PR adds a simplification method using runtime asserts, whenever we\nare about to raise a data-dependent error. We use the recorded runtime\nasserts as a source of knowledge for substituting the free symbols in\nthe given expression.\n\nThis is useful for avoiding data-dependent errors, specifically avoiding\nguarding on expressions with unbacked integers, that are deducible from\nthe past runtime asserts.\n\ncc @ezyang @SherlockNoMad @EikanWang @jgong5 @wenzhe-nrv @voznesenskym @penguinwu @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-01-25T16:08:37Z",
        null,
        null,
        "open source, release notes: fx, fx, module: dynamo, ciflow/inductor",
        "gh/ysiraichi/81/base",
        "gh/ysiraichi/81/head",
        2,
        88,
        0,
        1,
        6,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145681"
    ],
    [
        145680,
        "Add icdf to Gamma dist",
        "Fixes #145679\n",
        "open",
        "2025-01-25T15:30:52Z",
        null,
        null,
        "triaged, open source",
        "main",
        "fix/chi2",
        1,
        5,
        0,
        1,
        5,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145680"
    ],
    [
        145677,
        "[1/N] Improve typing in  torch/_C/__init__.pyi.in",
        "Fixes #ISSUE_NUMBER\r\n\n\ncc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o @mcarilli @ptrblck @leslie-fang-intel @jgong5",
        "open",
        "2025-01-25T10:39:28Z",
        null,
        null,
        "oncall: distributed, triaged, open source, module: amp (automated mixed precision), ciflow/trunk, topic: not user facing",
        "main",
        "C_init_py2",
        1,
        183,
        183,
        3,
        12,
        2,
        "guangyey, cyyever, guangyey, XuehaiPan, XuehaiPan, albanD, cyyever, cyyever, XuehaiPan, XuehaiPan, cyyever, cyyever, XuehaiPan, XuehaiPan, cyyever, cyyever, XuehaiPan, cyyever, XuehaiPan, cyyever, cyyever, Skylion007, albanD, cyyever",
        "COMMENTED, COMMENTED, APPROVED, COMMENTED, APPROVED, CHANGES_REQUESTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145677"
    ],
    [
        145674,
        "Convert Tensor lr to 0-dim as needed for the optimizer to normally work",
        "Fixes #145461\r\n",
        "open",
        "2025-01-25T06:52:07Z",
        null,
        null,
        "triaged, open source, release notes: optim",
        "main",
        "fix-tensor-lr-check",
        4,
        33,
        23,
        10,
        12,
        0,
        "janeyx99, janeyx99, Tony-Y, janeyx99, Tony-Y, Tony-Y, janeyx99, Tony-Y, Tony-Y, janeyx99, Tony-Y, janeyx99, Tony-Y",
        "COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145674"
    ],
    [
        145652,
        "[c10d] implement ReduceOp.unbox()",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\r\n* __->__ #145652\r\n* #144886\r\n\r\n```python\r\n>>> import torch\r\n>>> op = torch.classes.c10d.ReduceOp()\r\n>>> op\r\n<torch.ScriptObject object at 0x5b688b0>\r\n>>> torch.distributed.ReduceOp.unbox(op)\r\n<torch.distributed.distributed_c10d.ReduceOp object at 0x7fd9e3066ff0>\r\n>>> torch.distributed.ReduceOp.unbox(op).op\r\n<RedOpType.SUM: 0>\r\n```\r\n\r\n\r\ncc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o",
        "open",
        "2025-01-24T20:55:44Z",
        null,
        null,
        "oncall: distributed, release notes: distributed (c10d)",
        "gh/yifuwang/187/base",
        "gh/yifuwang/187/head",
        1,
        7,
        1,
        1,
        1,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/145652"
    ],
    [
        145651,
        "Add link to non_blocking/pinmem tutorial in `Tensor.to` docstrings",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #145651\n\n\n\ncc @svekars @brycebortree @sekyondaMeta @AlannaBurke",
        "open",
        "2025-01-24T20:48:22Z",
        null,
        null,
        "module: docs, topic: not user facing",
        "gh/vmoens/19/base",
        "gh/vmoens/19/head",
        1,
        11,
        6,
        1,
        1,
        0,
        "svekars",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145651"
    ],
    [
        145648,
        "Test distributions compilation",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #145648\n* #145647\n* #145646\n* #145645\n\n",
        "open",
        "2025-01-24T20:04:23Z",
        null,
        null,
        "topic: not user facing",
        "gh/vmoens/18/base",
        "gh/vmoens/18/head",
        1,
        20,
        0,
        2,
        2,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145648"
    ],
    [
        145647,
        "Fix distributions dynamo tracing (`__init__`, sample and log_prob)",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #145648\n* __->__ #145647\n* #145646\n* #145645\n\n",
        "open",
        "2025-01-24T20:04:15Z",
        null,
        null,
        "module: dynamo, ciflow/inductor",
        "gh/vmoens/17/base",
        "gh/vmoens/17/head",
        8,
        94,
        35,
        2,
        3,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145647"
    ],
    [
        145646,
        "refactor Distribution class for compile support",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #145648\n* #145647\n* __->__ #145646\n* #145645\n\n",
        "open",
        "2025-01-24T20:04:07Z",
        null,
        null,
        "",
        "gh/vmoens/16/base",
        "gh/vmoens/16/head",
        2,
        27,
        7,
        1,
        2,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145646"
    ],
    [
        145645,
        "Parametrize distributions tests",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #145648\n* #145647\n* #145646\n* __->__ #145645\n\n",
        "open",
        "2025-01-24T20:03:59Z",
        null,
        null,
        "topic: not user facing",
        "gh/vmoens/15/base",
        "gh/vmoens/15/head",
        1,
        878,
        745,
        1,
        1,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145645"
    ],
    [
        145636,
        "Simplify functional composition in _aot_autograd/dispatch_and_compile_graph.py",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #145636\n\n",
        "open",
        "2025-01-24T18:34:01Z",
        null,
        null,
        "open source, topic: not user facing, ciflow/inductor",
        "gh/rec/125/base",
        "gh/rec/125/head",
        1,
        115,
        60,
        10,
        3,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145636"
    ],
    [
        145628,
        "Add __all__ for torch.nn.init",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #145628\n\n",
        "open",
        "2025-01-24T17:17:45Z",
        null,
        null,
        "",
        "gh/mikaylagawarecki/310/base",
        "gh/mikaylagawarecki/310/head",
        2,
        31,
        3,
        1,
        2,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/145628"
    ],
    [
        145622,
        "[AOTI] Update test runner to use the new APIs",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #145622\n\nSummary: Switch to the newer aoti_compile_and_package APIs. Some tests still kept using legacy APIs, and will follow up with internal test refactoring.\n\ncc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @ColinPeppler\n\nDifferential Revision: [D69306100](https://our.internmc.facebook.com/intern/diff/D69306100)",
        "open",
        "2025-01-24T15:42:41Z",
        null,
        null,
        "oncall: distributed, ciflow/trunk, topic: not user facing, module: inductor, ciflow/inductor, keep-going",
        "gh/desertfire/531/base",
        "gh/desertfire/531/head",
        7,
        133,
        87,
        6,
        3,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145622"
    ],
    [
        145612,
        "[BE] Bump huggingface pin",
        null,
        "open",
        "2025-01-24T13:47:11Z",
        null,
        null,
        "topic: not user facing, ciflow/inductor, ciflow/inductor-periodic",
        "main",
        "desertfire/update_hf_pin",
        1,
        1,
        1,
        1,
        1,
        0,
        "huydhn",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145612"
    ],
    [
        145606,
        "[BE][CI] bump ruff to 0.9.5",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #145606\n* #144546\n* #144569\n* #145148\n* #146509\n\n",
        "open",
        "2025-01-24T12:12:33Z",
        null,
        null,
        "open source, ciflow/trunk, topic: not user facing",
        "gh/XuehaiPan/238/base",
        "gh/XuehaiPan/238/head",
        1,
        2,
        2,
        7,
        15,
        1,
        "malfet",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145606"
    ],
    [
        145605,
        "WIP error_prop sc",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #145605\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-01-24T11:44:25Z",
        null,
        null,
        "module: dynamo, ciflow/inductor",
        "gh/IvanKobzarev/98/base",
        "gh/IvanKobzarev/98/head",
        3,
        172,
        1,
        1,
        2,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145605"
    ],
    [
        145603,
        "[dynamo] refactor dynamo__custom_eval_frame to C++, refactor SKIP_CODE[_RECURSIVE]",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #146355\n* __->__ #145603\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-01-24T11:19:43Z",
        null,
        null,
        "topic: not user facing, module: dynamo, ciflow/inductor",
        "gh/williamwen42/200/base",
        "gh/williamwen42/200/head",
        9,
        376,
        325,
        9,
        2,
        1,
        "jansel, jansel, anijain2305",
        "CHANGES_REQUESTED, APPROVED, APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145603"
    ],
    [
        145600,
        "Add device support for chunk_cat, all_gather_copy_in, and split_with_\u2026",
        "\u2026sizes_copy in _fsdp_collectives.py\r\n\n\ncc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o",
        "open",
        "2025-01-24T09:29:41Z",
        null,
        null,
        "oncall: distributed, open source, release notes: distributed (fsdp)",
        "main",
        "fsdp_add_device",
        1,
        3,
        0,
        2,
        2,
        0,
        "awgu",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145600"
    ],
    [
        145595,
        "[micro_pipeline_tp] support pattern matching row-wise scaled_mm with sharded scale",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #145595\n* #145594\n\n\n\ncc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang @aakhundov",
        "open",
        "2025-01-24T07:46:58Z",
        null,
        null,
        "oncall: distributed, release notes: distributed (pipeline), module: inductor, ciflow/inductor",
        "gh/yifuwang/186/base",
        "gh/yifuwang/186/head",
        3,
        123,
        35,
        1,
        1,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/145595"
    ],
    [
        145594,
        "[micro_pipeline_tp] add logging for all-gather-matmul fusion",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* (to be filled)\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang @aakhundov",
        "open",
        "2025-01-24T07:46:54Z",
        null,
        null,
        "module: inductor, ciflow/inductor",
        "gh/yifuwang/185/base",
        "gh/yifuwang/185/head",
        1,
        53,
        9,
        1,
        2,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/145594"
    ],
    [
        145589,
        "Replace decorators in UTs  to cover additional devices",
        "This is follow-up of https://github.com/pytorch/pytorch/pull/128584.  Covering additional files for execution.\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang @aakhundov",
        "open",
        "2025-01-24T05:51:12Z",
        null,
        null,
        "triaged, open source, topic: not user facing, module: inductor",
        "main",
        "expand_ops_execution",
        19,
        314,
        313,
        1,
        2,
        1,
        "albanD",
        "COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145589"
    ],
    [
        145584,
        "layer_norm_kernel.cu eliminate the need for divisions for default vector size",
        "Eliminate the need for divisions in layernorm for default vector size.\r\n\r\nThe divisions performed in the online sum section can be replaced with immediate values as the vector size used is always 4. We special case this and leave the alternative in place in case other vector sizes are to be explored in the future.\r\n\r\nFor the combine step the division is always a power of 2 as long as the vector size used is a power of two so we can use shuffles to perform the division.\r\n\r\ncc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd",
        "open",
        "2025-01-24T03:39:02Z",
        null,
        null,
        "module: rocm, triaged, open source, release notes: cuda, ciflow/rocm",
        "main",
        "remove-divisions-from-layernorm",
        1,
        35,
        6,
        2,
        3,
        1,
        "jeffdaily, jeffdaily",
        "APPROVED, CHANGES_REQUESTED",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/145584"
    ],
    [
        145562,
        "[Not for land] hacking up mx",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #145562\n\r\n\r\nhttps://www.internalfb.com/intern/paste/P1717686991/\r\n",
        "open",
        "2025-01-24T00:08:37Z",
        null,
        null,
        "ciflow/inductor",
        "gh/drisspg/119/base",
        "gh/drisspg/119/head",
        6,
        77,
        84,
        3,
        3,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/145562"
    ],
    [
        145559,
        "[dynamo][builtin-skipfile-cleanup] Remove collections",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #145559\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-01-23T23:43:02Z",
        null,
        null,
        "ciflow/trunk, topic: not user facing, module: dynamo, ciflow/inductor, keep-going",
        "gh/anijain2305/634/base",
        "gh/anijain2305/634/head",
        1,
        0,
        2,
        17,
        1,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/145559"
    ],
    [
        145540,
        "[BE][hop] make it easier to use speculate_subgraph",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\r\n* __->__ #145540\r\n\r\n\r\nPreviously, it's tricky to construct the required inputs for speculate_subgraph as discussed in https://github.com/pytorch/pytorch/issues/144805 because we have to program against variable trackers. This PR turns speculate_subgraph into a hop and uses _make_inlined to get the subgraphs. Now HOP developers only need to program against normal tensors and uses the hop speculate_subgraph to trace the subgraph.\r\n\r\nUsing the flex attention hop as an example, now we can write the sub graph tracing code with following code:\r\n```python\r\n@_make_inlinable\r\ndef _create_scalar(query: torch.Tensor):\r\n    return query.new_empty([], dtype=torch.int32)\r\n\r\n@_make_inlinable\r\ndef _create_scalars(query: torch.Tensor):\r\n    return (\r\n        _create_scalar(query),\r\n        _create_scalar(query),\r\n        _create_scalar(query),\r\n        _create_scalar(query),\r\n    )\r\n\r\n# A normal python function that works on tensors and operators\r\n@_make_inlinable\r\ndef _fn(query: torch.Tensor, fn: Callable):\r\n    # since these return tensors are created in speculate_subgraph\r\n    # it will not affect the current graph.\r\n    score, *_ = torch.ops.higher_order.speculate_subgraph(\r\n        _create_scalar, (query,)\r\n    )\r\n    (b, h, m, n), *_ = torch.ops.higher_order.speculate_subgraph(\r\n        _create_scalars, (query,)\r\n    )\r\n    return torch.ops.higher_order.speculate_subgraph(\r\n        fn, (score, b, h, m, n)\r\n    )\r\n\r\n# This is the driving logic, essentially, it inlines into _fn and returns whatever _fn returns\r\n# in this case, higher_order.speculate_subgraph's output is \r\n# (tensor_var, tree_spec_var, UserdefinedObject(fx.Graph), UserdefinedObject(parent_proxy_to_child_proxy_map))\r\nwith TransformGetItemToIndex():\r\n    (\r\n        _body_output,\r\n        _body_tree_spec,\r\n        body_graph_var,\r\n        body_lifted_freevars_var,\r\n    ) = _make_inlined(tx, _fn)(query, fn).unpack_var_sequence(tx)\r\n```\r\nThe other benefit is that: we can avoid putting unnecessary tensor calls before the hop because they can be put  in speculate_subgraph, which also addresses the issue in https://github.com/pytorch/pytorch/issues/144803\r\n\r\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-01-23T21:50:20Z",
        null,
        null,
        "topic: not user facing, module: dynamo, ciflow/inductor",
        "gh/ydwu4/201/base",
        "gh/ydwu4/201/head",
        3,
        128,
        36,
        4,
        5,
        0,
        "ydwu4, ydwu4, ydwu4, xmfan, zou3519, ydwu4, zou3519, zou3519, zou3519, zou3519, zou3519",
        "COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145540"
    ],
    [
        145527,
        "fix intermediate debug information with cpp_wrapper",
        "Summary: before fix, code like:\r\n```cpp\r\n    aoti_torch_print_tensor_handle(buf0, \"after_launch - triton_poi_fused_randn_0 - buf0\");\r\n    aoti_torch_print_tensor_handle(buf1, \"after_launch - triton_poi_fused_randn_0 - buf1\");\r\n    printf(\"[  after_launch - triton_poi_fused_randn_0 - 0: %ld  ]\", 0); printf(\"\r\n\");\r\n    printf(\"[  after_launch - triton_poi_fused_randn_0 - 1228800L: %ld  ]\", 1228800L); printf(\"\r\n\");\r\n```\r\nwas generated, which is a syntax error.\r\n\r\nTest Plan:\r\nNew unit test.\r\n\r\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang @aakhundov",
        "open",
        "2025-01-23T20:53:07Z",
        null,
        null,
        "ciflow/trunk, topic: bug fixes, module: inductor, ciflow/inductor, release notes: inductor, merging",
        "main",
        "exclamaforte/cpp-wrapper-debug",
        2,
        21,
        1,
        1,
        19,
        0,
        "desertfire",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145527"
    ],
    [
        145526,
        "[MPS] Add bilineard2d_aa implementation",
        "It works, but still struggling with correctness issues...",
        "open",
        "2025-01-23T20:52:21Z",
        null,
        null,
        "topic: improvements, release notes: mps, ciflow/mps",
        "main",
        "malfet/mps-add-bilineard2d-aa",
        4,
        95,
        1,
        1,
        2,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/145526"
    ],
    [
        145523,
        "inductor.config.descriptive_names = False is not actually supported",
        "Summary:\r\nThis config is not supported (it throws an error when set), and doesn't really make sense imo.\r\n\r\n\r\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang @aakhundov",
        "open",
        "2025-01-23T20:44:23Z",
        null,
        null,
        "Merged, Reverted, ciflow/trunk, topic: deprecation, module: inductor, ciflow/inductor, release notes: inductor, ci-no-td",
        "main",
        "exclamaforte/remove-desc-names",
        1,
        2,
        3,
        1,
        13,
        0,
        "eellison",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145523"
    ],
    [
        145521,
        "General Changes for multi accelerators",
        "Intend to generailze the framework for multiple accelerators.\r\nMajor changes includes:\r\n> Add TEST_CUDA & TEST_HPU condition for generalization at common place.\r\n> Move \".cuda()\" to \".to(device_type)\" call\r\n\r\n\r\ncc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o",
        "open",
        "2025-01-23T20:17:22Z",
        null,
        null,
        "oncall: distributed, triaged, open source, release notes: distributed (fsdp)",
        "main",
        "common_fsdp",
        17,
        744,
        602,
        2,
        5,
        0,
        "wconstab, rahulsingh-intel, ankurneog, ankurneog, ankurneog, ankurneog, ankurneog, ankurneog",
        "COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145521"
    ],
    [
        145512,
        "Add missing autoreleasepool around runUniqueGraph to prevent leaks",
        "References were held onto longer than needed. Added autoreleasepool around the runUniqueGraph to allow the memory to be freed.\r\n\r\nFixes #145151\r\n\n\ncc @kulinseth @albanD @malfet @DenisVieriu97",
        "open",
        "2025-01-23T18:41:38Z",
        null,
        null,
        "module: memory usage, triaged, open source, module: mps, release notes: mps, ciflow/mps",
        "main",
        "dev/joona/unique_leak",
        1,
        7,
        4,
        4,
        3,
        1,
        "Skylion007, Skylion007, malfet",
        "COMMENTED, COMMENTED, CHANGES_REQUESTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145512"
    ],
    [
        145510,
        "Add istft option to align window for center = false",
        "Following up from https://github.com/pytorch/pytorch/pull/145324, also add the `align_to_window` parameter for the inverse short fourier transform op.\r\n\r\n_PENDING: stft round trip tests for center = false and align_window = true_\r\n\r\nPr chain:\r\n- [Advance past fc window for stft center #145437](https://github.com/pytorch/pytorch/pull/145437)\r\n- [Add stft option to align window for center = false #145324](https://github.com/pytorch/pytorch/pull/145324)\r\n- -> [Add istft option to align window for center = false](https://github.com/pytorch/pytorch/pull/145510)",
        "open",
        "2025-01-23T18:36:31Z",
        null,
        null,
        "release notes: onnx",
        "jz/stft-old-fc",
        "jz/istft",
        9,
        113,
        22,
        30,
        1,
        0,
        "Skylion007",
        "COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145510"
    ],
    [
        145504,
        "[DO NOT MERGE] Update workflow to use root user instead of jenkins user",
        "cc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd",
        "open",
        "2025-01-23T17:41:06Z",
        null,
        null,
        "module: rocm, open source, topic: not user facing, ciflow/periodic, ciflow/rocm, ci-no-td, ciflow/inductor-rocm",
        "main",
        "patch-5",
        3,
        3,
        2,
        11,
        1,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/145504"
    ],
    [
        145499,
        "Remove truncated normal initialization for 16-bit (and lower) tensors",
        "Fixes #145498\r\n\n\ncc @albanD @mruberry @jbschlosser @walterddr @mikaylagawarecki",
        "open",
        "2025-01-23T17:09:38Z",
        null,
        null,
        "module: nn, triaged, open source",
        "main",
        "fix-bf16-inits",
        1,
        2,
        0,
        1,
        3,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145499"
    ],
    [
        145492,
        "[BE]: Fix OrderedSet equality oversight",
        "Test to see if #145489 even causes behavior difference in test suite\r\n",
        "open",
        "2025-01-23T15:47:17Z",
        null,
        null,
        "open source, topic: bug fixes",
        "main",
        "skylion007/fix-orderedset-equality-2025-01-23",
        1,
        7,
        2,
        1,
        2,
        1,
        "Skylion007, eellison",
        "COMMENTED, COMMENTED",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/145492"
    ],
    [
        145487,
        "Remove unnecessary \"special linking\" for `BLAS_LIBRARIES`",
        "Remove the \"special linking\" that involves listing `BLAS_LIBRARIES` thrice if `TH_BINARY_BUILD` is set, as it should not be any different from listing it just once.\r\n\r\nThe code seems to date back to commit cfcf2af95f91a88ec61cbcac8b30a718e7332aa5. The original code already listed `BLAS_LIBRARIES` thrice, but it provided no explanation for doing that \u2014 and without `TH_BINARY_BUILD`, BLAS was not linked at all.  The current version seems to originate in d6a8d28d6529a4f0b80a8c046ca9c36ca6c8b347 \u2014 and it already provided an `ELSE` clause listing `BLAS_LIBRARIES` only once.  From this, I suspect that it is probably an unnecessary leftover.\n\ncc @malfet @seemethere",
        "open",
        "2025-01-23T15:13:03Z",
        null,
        null,
        "module: build, triaged, open source, topic: not user facing",
        "main",
        "blas-libs-special-linking",
        1,
        2,
        9,
        1,
        3,
        1,
        "malfet",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145487"
    ],
    [
        145486,
        "feat: add SVE dispatch for non-FBGEMM qembeddingbag",
        "Adds an accelerated kernel for `quantized::embedding_bag_byte` and integrates it with the dispatch mechanism.\r\n\r\nThe bulk of the SVE code has already been seen before and can be found here: https://github.com/pytorch/pytorch/pull/139753.\n\ncc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @malfet @snadampal @milpuz01",
        "open",
        "2025-01-23T15:06:46Z",
        null,
        null,
        "module: cpu, triaged, open source, module: arm, release notes: quantization",
        "main",
        "qembeddingbag",
        3,
        565,
        128,
        1,
        5,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145486"
    ],
    [
        145476,
        "Adapt Dynamo Tests to HPUs",
        "This PR is a continuation of https://github.com/pytorch/pytorch/pull/144387 .  Adapted two more files with the approach described below.\r\n\r\n#MOTIVATION\r\n\r\nWe recently integrated support for Intel Gaudi devices (identified as 'hpu') into the common_device_type framework via the pull request at https://github.com/pytorch/pytorch/pull/126970. This integration allows tests to be automatically instantiated for Gaudi devices upon loading the relevant library. Building on this development, the current pull request extends the utility of these hooks by adapting selected CUDA tests to operate on Gaudi devices. Additionally, we have confirmed that these modifications do not interfere with the existing tests on CUDA devices.\r\n\r\nOther accelerators can also extend the functionality by adding the device in the devices list. ( For eg: xpu )\r\n\r\n#CHANGES\r\n\r\nCreate a separate class for test functions running on CUDA devices\r\nExtend the functionality of these tests to include HPUs\r\nUse instantiate_device_type_tests with targeted attributes to generate device-specific test instances within the new classes\r\nApply skipIfHPU decorator to bypass tests that are not yet compatible with HPU devices\r\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-01-23T10:20:41Z",
        null,
        null,
        "triaged, open source, topic: not user facing, module: dynamo",
        "main",
        "dynamo_changes1",
        2,
        268,
        251,
        4,
        4,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145476"
    ],
    [
        145475,
        "[dynamo] added support to trace torch.cuda.is_current_stream_capturing",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #145475\n\nenabled tracing torch.cuda.is_current_stream_capturing\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @kadeng @chauhang @amjames",
        "open",
        "2025-01-23T10:06:58Z",
        null,
        null,
        "topic: not user facing, module: dynamo, ciflow/inductor, ciflow/rocm",
        "gh/chenyang78/1/base",
        "gh/chenyang78/1/head",
        2,
        10,
        0,
        2,
        2,
        0,
        "mlazos",
        "COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145475"
    ],
    [
        145474,
        "fix test_convolution error when use cudnn.flags",
        "Fixes #145473\r\n\n\ncc @csarofeen @ptrblck @xwang233 @eqy",
        "open",
        "2025-01-23T10:01:29Z",
        null,
        null,
        "module: cudnn, module: convolution, triaged, open source, topic: not user facing",
        "main",
        "cudnn_flags_use_fix",
        1,
        2,
        2,
        1,
        1,
        1,
        "eqy",
        "COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145474"
    ],
    [
        145459,
        "[AOTInductor] Align behavior between CPU and GPU",
        "Summary:\n(1) Make sure CPU and GPU doesn't have different implementation and behavior when calling from the same path and API. Only difference between CPU and GPU after this PR should ONLY be the running hardware.\n(2) This PR fixes the issue of memory access when it==constants_map.end()\n(3) This PR resolves T179437596\n\nTest Plan: buck2 run mode/dev sigmoid/inference/test:e2e_test_cpu\n\nDifferential Revision: D68540744\n\n\n",
        "open",
        "2025-01-23T04:33:17Z",
        null,
        null,
        "fb-exported, topic: not user facing, ciflow/inductor",
        "main",
        "export-D68540744",
        2,
        108,
        66,
        1,
        14,
        0,
        "desertfire, muchulee8, muchulee8, muchulee8",
        "COMMENTED, COMMENTED, COMMENTED, COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145459"
    ],
    [
        145450,
        "Replace is_same with is_same_v for concise syntax",
        "Replace `std::is_same<T, U>::value` with `std::is_same_v` for concise and consistent syntax with other code.\n\ncc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10",
        "open",
        "2025-01-23T02:28:06Z",
        null,
        null,
        "module: cpu, open source, ciflow/trunk, release notes: sparse",
        "main",
        "opt/aten/syntax",
        6,
        10,
        10,
        1,
        16,
        0,
        "Skylion007",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145450"
    ],
    [
        145437,
        "Advance past fc window for stft center",
        "Long overdue follow-up on https://github.com/pytorch/pytorch/pull/73432/files#diff-5f3d4caa0693a716fc46fd7f6339312f1b5f0bf89e3a3ff58e9dc13a9486b17aR719\r\n\r\nOnnx stft doesn't support centering, [and all of the existing tests are for center = False](https://github.com/pytorch/pytorch/blob/main/test/onnx/test_pytorch_onnx_onnxruntime.py#L8026). I will open a follow-up issue to address this, this is just a nice-to-have.\r\n\r\nPr chain:\r\n- -> [Advance past fc window for stft center #145437](https://github.com/pytorch/pytorch/pull/145437)\r\n- [Add stft option to align window for center = false #145324](https://github.com/pytorch/pytorch/pull/145324)\r\n- [Add istft option to align window for center = false](https://github.com/pytorch/pytorch/pull/145510)",
        "open",
        "2025-01-23T00:49:05Z",
        null,
        null,
        "Merged, Reverted, ciflow/trunk, topic: not user facing, ciflow/slow, ci-no-td",
        "main",
        "jz/stft-old-fc",
        9,
        65,
        20,
        5,
        13,
        2,
        "justinchuby, titaiwangms, jackzhxng, iseeyuan",
        "APPROVED, COMMENTED, COMMENTED, APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145437"
    ],
    [
        145435,
        "[dynamo] save/restore system random state more carefully",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #145435\n\r\nFixes https://github.com/pytorch/pytorch/issues/145329.\r\n\r\nWe we need to save/restore system `random` state in 2 places:\r\n- in `eval_frame.py`, we need to make sure that wrapper code between the user call to a `torch.compile`d function and the actual function call (intercepted by eval_frame.c) doesn't modify random state (https://github.com/pytorch/pytorch/blob/b2c89bc115123aea8e075e882ee121537ec92f89/torch/_dynamo/eval_frame.py#L532)\r\n- in `eval_frame.c`, we need to make sure that guard eval and calling convert_frame don't modify random state (https://github.com/pytorch/pytorch/blob/b2c89bc115123aea8e075e882ee121537ec92f89/torch/csrc/dynamo/eval_frame.c#L575)\r\n\r\nFollowup - perhaps more global state from `convert_frame.py:preserve_global_state` can be moved to `eval_frame.py/c`.\r\n\r\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames\r\n\r\nDifferential Revision: [D68532640](https://our.internmc.facebook.com/intern/diff/D68532640)",
        "open",
        "2025-01-23T00:41:37Z",
        null,
        null,
        "ciflow/trunk, topic: bug fixes, module: dynamo, ciflow/inductor, release notes: dynamo",
        "gh/williamwen42/199/base",
        "gh/williamwen42/199/head",
        9,
        200,
        22,
        8,
        9,
        1,
        "jansel, williamwen42, williamwen42, williamwen42, jansel",
        "CHANGES_REQUESTED, COMMENTED, COMMENTED, COMMENTED, APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145435"
    ],
    [
        145426,
        "Fix aot inductor intermediate debug printing",
        "Fixes https://github.com/pytorch/pytorch/issues/145425\r\n\r\nThe other way to fix this would be to change `_print_debugging_tensor_value_info` to handle constants.\r\n\r\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang @aakhundov",
        "open",
        "2025-01-22T23:55:41Z",
        null,
        null,
        "ciflow/trunk, topic: bug fixes, module: inductor, ciflow/inductor, release notes: inductor, merging, module: aotinductor",
        "main",
        "exclamaforte/aot-inductor-debug",
        2,
        23,
        1,
        1,
        7,
        1,
        "desertfire, desertfire",
        "COMMENTED, APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145426"
    ],
    [
        145424,
        "Tag storages with offset in file when  with FakeTensorMode",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #145424\n\n",
        "open",
        "2025-01-22T23:33:36Z",
        null,
        null,
        "",
        "gh/mikaylagawarecki/307/base",
        "gh/mikaylagawarecki/307/head",
        1,
        3,
        0,
        1,
        2,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/145424"
    ],
    [
        145406,
        "[export][be] Clean up local imports from export [2/n]",
        "Summary: as title\n\nTest Plan: CI\n\nDifferential Revision: D68450108\n",
        "open",
        "2025-01-22T21:39:02Z",
        null,
        null,
        "fb-exported, ciflow/trunk, release notes: export",
        "main",
        "export-D68450108",
        1,
        8,
        13,
        1,
        5,
        0,
        "tugsbayasgalan",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145406"
    ],
    [
        145404,
        "[distributions] Catch inf gradient in beta distribution",
        "Fixes #127387\r\n\r\nUnder the conditions in the issue, the calculations in [_beta_grad_beta_small](https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/Distributions.h#L397) are numerically unstable (due to the `betas = betas * (beta - casted_i);` blowing up, since in that code path `beta` is large), and the gradient can end up being `nan` when `x` is close to 1 (and hence is close to 0 in that function as it uses `1-x`).\r\nIt seems that sometimes rather than become `nan`, the series ends up being `inf`, which isn't currently caught. I was able to verify this through some debug/print statements. I struggled to recreate the issue directly with a size of 1, even with directly calling the backward function with `x` values close to 1.\r\n\r\nThis PR amends the `nan` check by also checking for `inf`, and adds a test based on the failing case from the linked issue.\r\n",
        "open",
        "2025-01-22T21:27:50Z",
        null,
        null,
        "triaged, open source, topic: not user facing",
        "main",
        "fix-dirichlet-grad-inf-cpu",
        2,
        9,
        1,
        1,
        3,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145404"
    ],
    [
        145381,
        "Use AOTI as inductor backend with precompile mode.",
        "Summary:\r\nDesign doc: https://docs.google.com/document/d/1Z15cBBPjoZ7gH00TSgCdgaYko7a7Br-ERd3_hA-g2IU/edit?usp=sharing\r\n\r\nIn this diff we are trying to introduce some stateful API to enable a global mode which will force inductor to use AOTI as a backend. Different from PR https://github.com/pytorch/pytorch/pull/141700, we didn't try to populate the package file into caching system, instead we bypass caching to simplify the implementation in the current form.\r\n\r\nSimilar to PR https://github.com/pytorch/pytorch/pull/141700, I did a quick benchmark to the loading time and it looks like the following:\r\n- Precompile\r\n```\r\nbuck run mode/opt scripts/zhxchen17:precompile\r\n```\r\n- Load using cache:\r\n```\r\ntime buck run mode/opt scripts/zhxchen17:precompile -- --loader cache\r\n```\r\nOutput:\r\n```\r\nreal    0m24.593s\r\nuser    0m59.342s\r\nsys     0m17.201s\r\n```\r\n- Load using load_fullgraph_package\r\n```\r\ntime buck run mode/opt scripts/zhxchen17:precompile -- --loader precompile\r\n```\r\nOutput:\r\n```\r\nreal    0m10.907s\r\nuser    0m9.210s\r\nsys     0m1.173s\r\n```\r\n\r\nTest Plan:\r\nbuck run mode/opt caffe2/test:test_export -- -r test_fullgraph_package_basic\r\n_function\r\n\r\nDifferential Revision: D68459341\r\n\r\n\r\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang @aakhundov",
        "open",
        "2025-01-22T16:27:55Z",
        null,
        null,
        "fb-exported, ciflow/trunk, topic: not user facing, module: inductor, ciflow/inductor",
        "main",
        "export-D68459341",
        6,
        274,
        10,
        2,
        8,
        1,
        "ezyang, ezyang, ezyang, jamesjwu, jamesjwu, jamesjwu, jamesjwu, zhxchen17, jansel, zhxchen17, zhxchen17, jansel, jansel, ezyang",
        "COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, CHANGES_REQUESTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145381"
    ],
    [
        145367,
        "[ARM] Fix broken tests in test_tensor_creation_ops on AArch64",
        "We have broken tests on Aarch64 which are not enabled upstream, this PR will fix and enable those tests.\r\n\r\n```\r\nAssertionError: Tensor-likes are not equal!\r\n\r\nMismatched elements: 2 / 3 (66.7%)\r\nGreatest absolute difference: 1 at index (1,)\r\nGreatest relative difference: 1.0842021724855044e-19 at index (1,)\r\n\r\nTo execute this test, run the following from the base repo dir:\r\n    python test/test_tensor_creation_ops.py TestTensorCreationCPU.test_float_to_int_conversion_nonfinite_cpu_int64\r\n\r\nThis message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0\r\n```",
        "open",
        "2025-01-22T11:29:20Z",
        null,
        null,
        "triaged, open source, topic: not user facing",
        "main",
        "tensor_creation",
        2,
        13,
        7,
        2,
        5,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145367"
    ],
    [
        145366,
        "removed check for ConvTranspose3D on MPS",
        "Fixes #130256\r\n\r\nI removed `TORCH_CHECK(input_t.dim() < 5, \"ConvTranspose 3D is not supported on MPS\");` as it is actually supported.",
        "open",
        "2025-01-22T10:20:05Z",
        null,
        null,
        "triaged, open source, release notes: mps",
        "main",
        "convtranspose_mps_remove_check",
        1,
        0,
        2,
        1,
        2,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145366"
    ],
    [
        145358,
        "Fix avg_pool crash with negative numbers",
        "Fixes #145077\r\n\n\ncc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10",
        "open",
        "2025-01-22T08:53:13Z",
        null,
        null,
        "module: cpu, triaged, open source, release notes: quantization",
        "main",
        "avg_pool_positive",
        5,
        56,
        7,
        4,
        2,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145358"
    ],
    [
        145353,
        "[dtensor][cp] experiment: call flex_attention on DTensor",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\r\n* __->__ #145353\r\n\r\n```\r\n  File \"/data/users/xilunwu/oss/pytorch/torch/_higher_order_ops/flex_attention.py\", line 459, in flex_attention_fake_impl\r\n    out = _permute_strides(out, query.stride())\r\n  File \"/data/users/xilunwu/oss/pytorch/torch/_higher_order_ops/flex_attention.py\", line 70, in _permute_strides\r\n    new_out = out.new_empty(out.shape).as_strided(out.shape, out_strides)\r\n  File \"/data/users/xilunwu/oss/pytorch/torch/_compile.py\", line 51, in inner\r\n    return disable_fn(*args, **kwargs)\r\n  File \"/data/users/xilunwu/oss/pytorch/torch/_dynamo/eval_frame.py\", line 745, in _fn\r\n    return fn(*args, **kwargs)\r\n  File \"/data/users/xilunwu/oss/pytorch/torch/distributed/tensor/_api.py\", line 348, in __torch_dispatch__\r\n    return DTensor._op_dispatcher.dispatch(\r\n  File \"/data/users/xilunwu/oss/pytorch/torch/distributed/tensor/_dispatch.py\", line 174, in dispatch\r\n    self.sharding_propagator.propagate(op_info)\r\n  File \"/data/users/xilunwu/oss/pytorch/torch/distributed/tensor/_sharding_prop.py\", line 207, in propagate\r\n    OutputSharding, self.propagate_op_sharding(op_info.schema)\r\n  File \"/data/users/xilunwu/oss/pytorch/torch/distributed/tensor/_sharding_prop.py\", line 47, in __call__\r\n    return self.cache(*args, **kwargs)\r\n  File \"/data/users/xilunwu/oss/pytorch/torch/distributed/tensor/_sharding_prop.py\", line 456, in propagate_op_sharding_non_cached\r\n    raise NotImplementedError(\r\ntorch._dynamo.exc.InternalTorchDynamoError: NotImplementedError: Operator aten.as_strided.default does not have a sharding strategy registered.\r\n```\r\n\r\n\r\ncc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o",
        "open",
        "2025-01-22T06:51:47Z",
        null,
        null,
        "oncall: distributed, ciflow/inductor",
        "gh/XilunWu/110/base",
        "gh/XilunWu/110/head",
        2,
        77,
        1,
        1,
        2,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/145353"
    ],
    [
        145331,
        "[WIP] [AOTInductor] Use AtenTensorHandle as the constant map's holder.",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #145331\n\nSummary:\nPreviously, all constants are held by RAIIAtenTensorHandle, which\nimplicitly indicates constants' lifetime is managed by the model itself.\nWe want to provide the flexibility to let users control the tensor's\nlifetime instead.\n\nThis change is the first PR, aims to introduce a holder to act as the original\nRAII holder managing the lifetime by the model and change the constant map to use AtenTensorHandle.\nAll behavior should be exactly the same as previous cases.\n\nTest Plan:\nExisting test cases. Not yet introducing new functionalities in this PR.\n\nReviewers:\n\nSubscribers:\n\nTasks:\n\nTags:\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @ColinPeppler @amjames @desertfire @chauhang @aakhundov\n\nDifferential Revision: [](https://our.internmc.facebook.com/intern/diff/)\n\nDifferential Revision: [D68472175](https://our.internmc.facebook.com/intern/diff/D68472175)",
        "open",
        "2025-01-22T00:52:35Z",
        null,
        null,
        "ciflow/trunk, topic: not user facing, module: inductor, ciflow/inductor",
        "gh/muchulee8/41/base",
        "gh/muchulee8/41/head",
        4,
        83,
        17,
        1,
        3,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145331"
    ],
    [
        145310,
        "[Utilization] post-test-process workflow",
        "# Overview\r\nAdd reusable workflow to trigger the post-test right after each test job is complete.\r\n\r\nCousion with pr to setup the runner permissions:\r\nhttps://github.com/pytorch-labs/pytorch-gha-infra/pull/595/files\r\n\r\n \r\n\r\n",
        "open",
        "2025-01-21T21:44:31Z",
        null,
        null,
        "topic: not user facing",
        "main",
        "addDispatchPr",
        3,
        64,
        0,
        22,
        3,
        0,
        "huydhn, huydhn, yangw-dev",
        "COMMENTED, COMMENTED, COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145310"
    ],
    [
        145265,
        "Fix SEGFAULT when None arg was passed in GraphContext.op(..)",
        "Fixes #145261\r\n",
        "open",
        "2025-01-21T11:32:33Z",
        null,
        null,
        "module: onnx, triaged, open source, onnx-triaged, release notes: onnx, topic: bug fixes",
        "main",
        "fix_optional_arg_onnx_export",
        1,
        1,
        1,
        1,
        2,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145265"
    ],
    [
        145260,
        "[ARM] Add test_ops and test_memory_profiler to aarch64 tests",
        "Fixes #142371\r\n",
        "open",
        "2025-01-21T10:59:02Z",
        null,
        null,
        "triaged, open source, topic: not user facing",
        "main",
        "test_ops",
        2,
        2,
        2,
        1,
        9,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145260"
    ],
    [
        145254,
        "[TEST] tmp storage with CONSTANTHANDLE",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #145254\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @ColinPeppler @amjames @desertfire @chauhang @aakhundov\n\nDifferential Revision: [D68430118](https://our.internmc.facebook.com/intern/diff/D68430118)",
        "open",
        "2025-01-21T08:02:18Z",
        null,
        null,
        "ciflow/trunk, module: inductor, ciflow/inductor",
        "gh/muchulee8/40/base",
        "gh/muchulee8/40/head",
        6,
        126,
        17,
        2,
        3,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145254"
    ],
    [
        145250,
        "[Inductor][CPU] Add a lowering pass for _weight_int4pack_mm_for_cpu",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #146756\n* __->__ #145250\n* #145245\n\r\n**Summary**\r\nIt's part of the task to enable max-autotune with GEMM template for WoQ INT4 GEMM on CPU. Early work: #133846\r\n\r\nThis PR adds a lowering pass for `torch.ops.aten_weight_int4pack_mm_for_cpu`. This op is used for WoQ int4 in Torchao. The lowering pass is a prerequisite for max-autotune, which is planed to be enabled for this op in subsequent PRs.\r\n\r\n**Test plan**\r\n```\r\npython test/inductor/test_mkldnn_pattern_matcher.py -k test_woq_int4\r\npython test/inductor/test_cpu_cpp_wrapper.py -k test_woq_int4\r\n```\r\n\r\ncc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @voznesenskym @penguinwu @EikanWang @Guobing-Chen @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang @aakhundov",
        "open",
        "2025-01-21T06:17:47Z",
        null,
        null,
        "module: cpu, open source, ciflow/trunk, release notes: quantization, topic: not user facing, intel, module: inductor, ciflow/inductor",
        "gh/Xia-Weiwen/29/base",
        "gh/Xia-Weiwen/29/head",
        6,
        140,
        2,
        9,
        1,
        0,
        "sanchitintel, Xia-Weiwen, Xia-Weiwen, Xia-Weiwen, sanchitintel",
        "COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/145250"
    ],
    [
        145248,
        "[Break XPU][Inductor UT] Set input tensors to corresponding device for test case in test_aot_indutor.py",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #146763\n* __->__ #145248\n* #146762\n\r\nFix #145247\r\n\r\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang @aakhundov",
        "open",
        "2025-01-21T06:09:54Z",
        null,
        null,
        "open source, ciflow/trunk, topic: not user facing, module: inductor, ciflow/inductor, ciflow/xpu",
        "gh/etaf/95/base",
        "gh/etaf/95/head",
        3,
        30,
        17,
        6,
        5,
        3,
        "malfet, malfet, desertfire, etaf, desertfire, etaf, desertfire, jansel",
        "APPROVED, CHANGES_REQUESTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, APPROVED, APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145248"
    ],
    [
        145245,
        "[Don't review][Quant][CPU] add a wrapper op for _weight_int4pack_mm_for_cpu with tensor args",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #146756\n* #145250\n* __->__ #145245\n\r\n**Summary**\r\nIt's part of the task to enable max-autotune with GEMM template for WoQ INT4 GEMM on CPU. Early work: #133846\r\n\r\nThis PR adds a wrapper op in `quantized_decomposed` lib for `torch.ops.aten_weight_int4pack_mm_for_cpu`, whose arguments are all tensors. It will be used in Inductor lowering with max-autotune where scalar arguments are difficult to handle.\r\n\r\n**Test plan**\r\n```\r\npython test/test_linalg.py -k test__int4_mm\r\n```\r\n\r\ncc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @voznesenskym @penguinwu @EikanWang @Guobing-Chen @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang @aakhundov",
        "open",
        "2025-01-21T05:08:51Z",
        null,
        null,
        "module: cpu, open source, ciflow/trunk, release notes: quantization, release notes: linalg_frontend, intel, module: inductor, ciflow/inductor",
        "gh/Xia-Weiwen/28/base",
        "gh/Xia-Weiwen/28/head",
        5,
        30,
        1,
        9,
        3,
        0,
        "leslie-fang-intel, jgong5, sanchitintel, sanchitintel, jgong5, leslie-fang-intel, Xia-Weiwen, Xia-Weiwen, sanchitintel, Xia-Weiwen",
        "APPROVED, APPROVED, COMMENTED, COMMENTED, APPROVED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/145245"
    ],
    [
        145244,
        "Improve typing by using bool and int",
        "Fixes #ISSUE_NUMBER\r\n",
        "open",
        "2025-01-21T04:52:30Z",
        null,
        null,
        "triaged, open source, topic: not user facing",
        "main",
        "buildin_type",
        6,
        734,
        742,
        4,
        5,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145244"
    ],
    [
        145243,
        "[inductor] Make serialized inductor patterns path configurable instead of using \u2026",
        "\u2026fixed path in the inductor module\r\n\r\nFixes [145242](https://github.com/pytorch/pytorch/issues/145242)\r\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang @aakhundov",
        "open",
        "2025-01-21T04:42:58Z",
        null,
        null,
        "triaged, open source, module: inductor, release notes: inductor",
        "main",
        "expose_inductor_patt_path",
        3,
        27,
        8,
        6,
        6,
        1,
        "aorenste, kareemshaik80, aorenste, kareemshaik80, aorenste, aorenste",
        "CHANGES_REQUESTED, COMMENTED, COMMENTED, COMMENTED, APPROVED, CHANGES_REQUESTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145243"
    ],
    [
        145241,
        "add grad_output shape check for adaptive_avg_pool2d_backward",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #145241\n\r\nFix https://github.com/pytorch/pytorch/issues/145070.\r\n",
        "open",
        "2025-01-21T04:33:24Z",
        null,
        null,
        "open source, ciflow/trunk, topic: not user facing",
        "gh/jiayisunx/54/base",
        "gh/jiayisunx/54/head",
        3,
        41,
        12,
        4,
        2,
        1,
        "malfet",
        "COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145241"
    ],
    [
        145239,
        "Turn Stream into protocol and improve typing in torch/_C/__init__.pyi.in",
        "Fixes #ISSUE_NUMBER\r\n\n\ncc @EikanWang @jgong5 @wenzhe-nrv @sanchitintel",
        "open",
        "2025-01-21T03:10:24Z",
        null,
        null,
        "oncall: jit, triaged, open source, topic: not user facing",
        "main",
        "C_init_py",
        1,
        81,
        79,
        6,
        6,
        0,
        "guangyey, cyyever, guangyey, cyyever, XuehaiPan",
        "COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/145239"
    ],
    [
        145228,
        "Expose the rendezvous keepalive arguments",
        "Enables support for this:\r\n\r\n```python\r\nfrom torch.distributed.launcher.api import LaunchConfig\r\n\r\nconfig = LaunchConfig(\r\n    ...,\r\n    rdzv_configs={\"keep_alive_interval\": 1122, \"heartbeat_timeout\": 321, \"keep_alive_max_attempt\" 5},\r\n)\r\n```\r\n\r\nThese arguments are currently hard-coded inside torchrun. The default values are not suitable for jobs with thousands of ranks.\r\n\r\nToday, `rdzv_configs` only allows the keys `join_timeout`, `last_call_timeout`, `close_timeout`\r\n\r\ncc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o",
        "open",
        "2025-01-20T22:58:17Z",
        null,
        null,
        "oncall: distributed, triaged, open source, release notes: distributed (torchelastic)",
        "main",
        "patch-1",
        1,
        17,
        3,
        3,
        1,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145228"
    ],
    [
        145224,
        "update sympy version 1.13.3 in setup.py (previously update only in requirement.txt)",
        "Previously, only update `sympy` version number in `requirement.txt`, but `setup.py` is unchanged. In PyPI, the wheel will relay on the dependency spec in `setup.py`, so only change in `setup.py` will be effective.",
        "open",
        "2025-01-20T20:18:42Z",
        null,
        null,
        "open source, ciflow/binaries, ciflow/trunk, topic: not user facing",
        "main",
        "main",
        1,
        1,
        1,
        1,
        13,
        0,
        "Skylion007",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145224"
    ],
    [
        145223,
        "Raise MutationError if there are side effects when returning generator",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #142513\n* __->__ #145223\n* #144420\n* #144424\n* #144423\n* #144422\n* #144421\n* #141055\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-01-20T18:05:17Z",
        null,
        null,
        "open source, module: dynamo, ciflow/inductor, release notes: dynamo",
        "gh/guilhermeleobas/91/base",
        "gh/guilhermeleobas/91/head",
        6,
        313,
        24,
        30,
        4,
        1,
        "Skylion007, guilhermeleobas, zou3519, zou3519, zou3519, StrongerXi, zou3519, guilhermeleobas, guilhermeleobas, zou3519, zou3519, zou3519, zou3519, guilhermeleobas",
        "COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, COMMENTED, APPROVED, COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145223"
    ],
    [
        145209,
        "Fix incorrect citation of authors in documentation",
        "This PR corrects the citation of Adafactor authors \"Noam Shazeer\" and \"Mitchell Stern\" in the documentation.\r\nThe current text incorrectly lists them as \"Shazeer, Noam, and Mitchell Stern,\" which seems to be a result of a data parsing issue of some reference manager(s) [as you can find many papers with the same issue](https://www.google.com/search?q=%22Shazeer%2C+Noam%2C+and+Mitchell+Stern%22). \r\nThe updated citation follows standard conventions for author names.",
        "open",
        "2025-01-20T09:19:08Z",
        null,
        null,
        "open source, release notes: optim",
        "main",
        "fix-adafactor-citation",
        1,
        7,
        7,
        1,
        2,
        1,
        "janeyx99",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145209"
    ],
    [
        145197,
        "Use std::string_view in get_fully_qualified_type_name",
        "The same as #139164 but open a new PR due to messy history there.",
        "open",
        "2025-01-20T03:28:55Z",
        null,
        null,
        "triaged, open source, topic: not user facing",
        "main",
        "stringview30",
        2,
        33,
        38,
        1,
        2,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145197"
    ],
    [
        145194,
        "Add transpose support for CppMicroGemmFP32Vec",
        "cc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @voznesenskym @penguinwu @EikanWang @Guobing-Chen @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang @aakhundov",
        "open",
        "2025-01-20T01:45:49Z",
        null,
        null,
        "module: cpu, open source, ciflow/trunk, topic: improvements, module: inductor, ciflow/inductor, release notes: inductor",
        "main",
        "trans_gemm",
        5,
        856,
        20,
        3,
        1,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/145194"
    ],
    [
        145180,
        "Added torch check to ensure indices are not empty",
        "Fixes #142459\r\n\r\ncc @mruberry @jbschlosser @walterddr @mikaylagawarecki",
        "open",
        "2025-01-19T23:49:19Z",
        null,
        null,
        "triaged, open source, topic: not user facing",
        "main",
        "max-pool3d",
        2,
        4,
        0,
        5,
        3,
        1,
        "Skylion007, mikaylagawarecki",
        "COMMENTED, COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145180"
    ],
    [
        145169,
        "Added weight to MSELoss Criterion",
        "- Changed Inheritance of MSELoss from _Loss to _WeightedLoss\r\n- Modified MSELoss to include weight parameter\r\n- Removed TODO\r\n- Added weight documentation to MSELoss Class\r\n\r\ntopic: enhancement\r\nrelease notes: nn\r\n\r\nI couldn't find this in any issues or under any existing PR Requests, I only found it by finding the TODO in the loss.py file. \r\n\r\nEdit - Accidental Markdown all caps removed\r\n",
        "open",
        "2025-01-19T09:40:11Z",
        null,
        null,
        "triaged, open source, release notes: nn, topic: improvements",
        "main",
        "add_weighted_MSELoss",
        1,
        6,
        5,
        1,
        3,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145169"
    ],
    [
        145167,
        "[BE]: Update NCCL submodule to 2.24.3",
        "Update NCCL to the latest version\n\nLast bump was in https://github.com/pytorch/pytorch/pull/124014\n\nSee upstream release notes here: https://docs.nvidia.com/deeplearning/nccl/release-notes/rel_2-24-3.html#rel_2-24-3\n\ncc @Skylion007\n",
        "open",
        "2025-01-19T04:12:34Z",
        null,
        null,
        "triaged, open source, topic: not user facing",
        "main",
        "tmm1/bump-nccl-dep",
        8,
        54,
        54,
        2,
        3,
        0,
        "Skylion007, tmm1",
        "COMMENTED, COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145167"
    ],
    [
        145153,
        "[BE]: Apply ruff PERF401 to torch",
        "Applies PERF401 optimizations to torch.\r\n\r\n\r\ncc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o @EikanWang @jgong5 @wenzhe-nrv @sanchitintel @ezyang @SherlockNoMad @voznesenskym @penguinwu @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang @aakhundov",
        "open",
        "2025-01-18T16:38:34Z",
        null,
        null,
        "oncall: distributed, oncall: jit, open source, better-engineering, ciflow/trunk, release notes: quantization, fx, module: inductor, module: dynamo, ciflow/inductor, release notes: AO frontend",
        "main",
        "skylion007/apply-ruff-PERF401-2025-01-18",
        21,
        80,
        91,
        5,
        9,
        1,
        "XuehaiPan, albanD, albanD",
        "APPROVED, APPROVED, APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145153"
    ],
    [
        145150,
        "[inductor] Simplify _inductor/utils.py slightly",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #145150\n* #144108\n\n\n\ncc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang @aakhundov",
        "open",
        "2025-01-18T15:51:22Z",
        null,
        null,
        "oncall: distributed, module: rocm, open source, better-engineering, topic: not user facing, module: inductor, ciflow/inductor",
        "gh/rec/124/base",
        "gh/rec/124/head",
        11,
        89,
        130,
        8,
        1,
        1,
        "eellison, eellison, rec, rec",
        "COMMENTED, COMMENTED, COMMENTED, COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145150"
    ],
    [
        145148,
        "[BE][PYFMT] bump `ruff format` target version to py39: add parentheses around long `with`-statements",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #145606\n* #144546\n* #144569\n* __->__ #145148\n* #146509\n\n\n\ncc @ezyang @SherlockNoMad @EikanWang @jgong5 @wenzhe-nrv @voznesenskym @penguinwu @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-01-18T14:34:48Z",
        null,
        null,
        "open source, ciflow/trunk, release notes: onnx, topic: not user facing, fx, module: dynamo, ciflow/inductor",
        "gh/XuehaiPan/237/base",
        "gh/XuehaiPan/237/head",
        14,
        137,
        66,
        11,
        9,
        1,
        "XuehaiPan, justinchuby, Skylion007, XuehaiPan, malfet",
        "COMMENTED, APPROVED, COMMENTED, COMMENTED, CHANGES_REQUESTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145148"
    ],
    [
        145147,
        "[BE][Easy] increase pip timeout for nightly tool: 15s -> 60s",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #145147\n\n",
        "open",
        "2025-01-18T12:30:38Z",
        null,
        null,
        "open source, topic: not user facing",
        "gh/XuehaiPan/236/base",
        "gh/XuehaiPan/236/head",
        1,
        7,
        1,
        2,
        2,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145147"
    ],
    [
        145146,
        "improve perf for layer_norm",
        "Fixes #145145\r\n\r\nPlease see more details in the issue.\r\n",
        "open",
        "2025-01-18T12:21:27Z",
        null,
        null,
        "triaged, open source, release notes: cuda",
        "main",
        "layernorm",
        1,
        258,
        3,
        1,
        1,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145146"
    ],
    [
        145136,
        "[inductor] [bug fix] Fix `conv` on processing uint ",
        "Fixes #144314\r\n\r\n\r\nut\r\n```\r\npytest -s -v test/inductor/test_torchinductor.py -k test_conv_errors_with_uint\r\n```\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang @aakhundov",
        "open",
        "2025-01-18T02:55:12Z",
        null,
        null,
        "open source, ciflow/trunk, topic: not user facing, module: inductor",
        "main",
        "fix_conv_uint",
        2,
        20,
        0,
        11,
        18,
        0,
        "jansel, shaoyuyoung, jansel, etaf, shaoyuyoung",
        "CHANGES_REQUESTED, COMMENTED, APPROVED, COMMENTED, COMMENTED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145136"
    ],
    [
        145130,
        "[cuBLAS][cuBLASLt] Unify `cuBLASLt` workspaces with `cuBLAS` workspaces",
        "As `cuBLAS` workspaces are already per-stream, there shouldn't be kernel execution overlap with `cuBLASLt` kernels.\r\n\r\nThis PR reuses `cuBLAS` workspaces for `cuBLASLt` for the following benefits:\r\n\r\n+ caching (`cuBLAS` workspaces were already cached, so now we get that for `cuBLASLt`)\r\n+ \"free\" workspace size bump for `cuBLASLt` `cuBLASLt` workspace sizes were previously smaller than those for `cuBLAS` by default which potentially hurts performance, and we encountered difficulty in increasing the size due to downstream OOMs , see also #120925\r\n+ fixes behavior broken behavior with the memtracker; https://github.com/pytorch/pytorch/pull/139442 attempted to handle peaky allocation behavior that broke memtracker equivalence tests but it didn't seem to fully work, here the cached/reused `cuBLAS` workspace seems to fix it\r\n+ one environment variable to rule them all: `CUBLAS_WORKSPACE_CONFIG` applies directly to `cuBLASLt` without a confusing `CUBLASLT_WORKSPACE_SIZE` that users would also need to consider\r\n\r\ncc @ptrblck @msaroufim @csarofeen @xwang233 @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-01-18T00:33:07Z",
        null,
        null,
        "module: cuda, triaged, module: cublas, open source, Merged, Reverted, ciflow/trunk, topic: not user facing, ciflow/periodic, module: dynamo, ciflow/inductor, matrix multiplication, ciflow/rocm, ci-no-td",
        "main",
        "unifiedcublasltworkspace",
        4,
        73,
        16,
        6,
        30,
        0,
        "ngimel, eqy, ngimel",
        "COMMENTED, COMMENTED, APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145130"
    ],
    [
        145128,
        "[executorch hash update] update the pinned executorch hash",
        "This PR is auto-generated nightly by [this action](https://github.com/pytorch/pytorch/blob/main/.github/workflows/nightly.yml).\nUpdate the pinned executorch hash.",
        "open",
        "2025-01-18T00:22:18Z",
        null,
        null,
        "open source, ciflow/trunk, topic: not user facing, ciflow/inductor",
        "main",
        "update-executorch-commit-hash/12838938822-1425-1",
        1,
        1,
        1,
        1,
        30,
        0,
        "pytorchbot",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145128"
    ],
    [
        145125,
        "Repro collective timeout and FR dump",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\r\n* __->__ #145125\r\n* #144834\r\n* #145099\r\n* #145011\r\n* #145010\r\n\r\nthe timeout is unfortunatley not reliable to repro.  I'm not yet sure\r\nwhat the root cause is, so for now I am just uploading my FR trace files\r\nto improve the analyzer script.\r\n\r\nUnfortunatley, these traces that I got on one instance were apparently corrupted, or at least fr_trace complained of an unpickling error\r\n[traces.tar.gz](https://github.com/user-attachments/files/18461972/traces.tar.gz)\r\n\r\ncc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @d4l3k @c-p-i-o",
        "open",
        "2025-01-18T00:05:59Z",
        null,
        null,
        "oncall: distributed, topic: not user facing",
        "gh/wconstab/391/base",
        "gh/wconstab/391/head",
        1,
        21,
        7,
        1,
        2,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145125"
    ],
    [
        145119,
        "WIP sccache simplified",
        "Trying to see if we can get rid of all of the wrapper code now",
        "open",
        "2025-01-17T22:50:05Z",
        null,
        null,
        "ciflow/binaries, ciflow/trunk, topic: not user facing, ciflow/inductor",
        "main",
        "wdvr/sccache_simplified",
        1,
        2,
        33,
        3,
        1,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/145119"
    ],
    [
        145117,
        "[EXPERIMENTAL][dynamo] optimize `DictGetItemGuardAccessor`",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #145117\n* #143313\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",
        "open",
        "2025-01-17T22:44:58Z",
        null,
        null,
        "module: dynamo, ciflow/inductor",
        "gh/StrongerXi/67/base",
        "gh/StrongerXi/67/head",
        1,
        38,
        5,
        1,
        2,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145117"
    ],
    [
        145116,
        "WIP remove -E workaround for nvcc",
        "follow up on https://github.com/pytorch/pytorch/pull/145012 to remove workaround https://github.com/pytorch/pytorch/pull/142813/files\r\n\r\nTesting to see if sccache now handles the nvcc caching correctly",
        "open",
        "2025-01-17T22:41:59Z",
        null,
        null,
        "ciflow/trunk, topic: not user facing",
        "main",
        "wdvr/sccache_nvcc",
        1,
        1,
        27,
        2,
        1,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/145116"
    ],
    [
        145104,
        "futher scheduler changes for invoke_quant: prologue low prec, (slightly) more aggressive fusion",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #145104\n* #139102\n\r\nRespect invoke_quant low precision options, also, be more aggressive in attepmting fusion.\r\n\r\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang @aakhundov",
        "open",
        "2025-01-17T20:04:38Z",
        null,
        null,
        "ciflow/trunk, topic: not user facing, module: inductor, ciflow/inductor",
        "gh/eellison/752/base",
        "gh/eellison/752/head",
        7,
        108,
        33,
        12,
        1,
        1,
        "shunting314, jansel, jansel, jansel",
        "APPROVED, CHANGES_REQUESTED, APPROVED, APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145104"
    ],
    [
        145098,
        "Use STL string_view header",
        "Fixes #ISSUE_NUMBER\r\n\n\ncc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o @EikanWang @jgong5 @wenzhe-nrv @sanchitintel",
        "open",
        "2025-01-17T19:24:03Z",
        null,
        null,
        "oncall: distributed, oncall: jit, release notes: cpp, topic: improvements",
        "main",
        "richard/string_view_header",
        21,
        40,
        32,
        1,
        1,
        1,
        "Skylion007",
        "APPROVED",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145098"
    ],
    [
        145090,
        "Test",
        "Fixes #ISSUE_NUMBER\r\n",
        "open",
        "2025-01-17T17:43:35Z",
        null,
        null,
        "topic: not user facing",
        "main",
        "try-speedup-docbuild",
        2,
        42,
        1,
        14,
        1,
        0,
        "",
        "",
        true,
        "https://api.github.com/repos/pytorch/pytorch/issues/145090"
    ],
    [
        145089,
        "[POC] Extend torch function support to ALL arguments, not just scalar type (but not insides of list)",
        "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #145089\n\nSigned-off-by: Edward Z. Yang <ezyang@meta.com>",
        "open",
        "2025-01-17T17:28:38Z",
        null,
        null,
        "release notes: fx, no-stale",
        "gh/ezyang/3068/base",
        "gh/ezyang/3068/head",
        3,
        28,
        20,
        3,
        4,
        0,
        "",
        "",
        false,
        "https://api.github.com/repos/pytorch/pytorch/issues/145089"
    ]
]